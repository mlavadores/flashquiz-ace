Question 1 of 529
A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain cloud.example.com for the resources stored within VPCs.
The company has the following DNS resolution requirements:
On-premises systems should be able to resolve and connect to cloud.example.com.
All VPCs should be able to resolve cloud.example.com.
There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.
Which architecture should the company use to meet these requirements with the HIGHEST performance?
A.
Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.
B.
Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder.
C.
Associate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver.
D.
Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.
AnswerDiscussion
Correct Answer: A
To achieve the highest performance hybrid DNS solution, the company should associate the Route 53 private hosted zone with all VPCs, ensuring that all VPCs can resolve cloud.example.com. Additionally, creating a Route 53 inbound resolver in the shared services VPC and attaching all VPCs to the transit gateway will allow on-premises systems to resolve the cloud.example.com domain by forwarding rules in the on-premises DNS server pointing to the inbound resolver. This ensures seamless DNS resolution for both on-premises and AWS resources.
Question 2 of 529
A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the ability to fail over to a different AWS Region.
Which solution will meet these requirements?
A.
Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.
B.
Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.
C.
Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.
D.
Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.
AnswerDiscussion
Correct Answer: C
To achieve failover capability for the weather data API, the company should deploy a new API Gateway API and Lambda functions in another AWS Region. The Route 53 DNS record should be changed to a failover record, which allows traffic to be routed to a secondary region if the primary region is unavailable. Enabling target health monitoring ensures that the failover mechanism can detect the health status of the API endpoints and initiate failover when needed. Converting the DynamoDB tables to global tables ensures that the data remains consistent and accessible across multiple regions, allowing the API to function smoothly during a failover event.
Question 3 of 529
A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.
The company recently acquired a new business unit and invited the new unit’s existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company’s policies.
Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?
A.
Remove the organization’s root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company’s standard AWS Config rules and deploy them throughout the organization, including the new account.
B.
Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.
C.
Convert the organization’s root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization’s root that allows AWS Config actions for principals only in the new account.
D.
Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization’s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.
AnswerDiscussion
Correct Answer: D
To ensure administrators can update AWS Config rules without introducing long-term maintenance, the best approach would be to create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization's root SCP to the Production OU. Once adjustments to AWS Config are complete, the new account can be moved to the Production OU. This method addresses necessary permissions without altering the deny list SCP framework at the root, thereby maintaining the existing security and management protocols across the organization.
Question 4 of 529
A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a stateful application. The application connects to a PostgreSQL database running on a separate server. The application’s user base is expected to grow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load Balancing.
Which solution will provide a consistent user experience that will allow the application and database tiers to scale?
A.
Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.
B.
Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled.
C.
Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.
D.
Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.
AnswerDiscussion
Correct Answer: C
The company needs a solution that can scale both the application and database tiers while providing a consistent user experience. Enabling Aurora Auto Scaling for Aurora Replicas allows the database to scale based on demand by adding or removing read replicas automatically. Using an Application Load Balancer (ALB) with the round robin routing algorithm ensures that incoming traffic is evenly distributed across multiple instances of the application layer, which helps maintain load balance and performance. Enabling sticky sessions ensures that each user's session is maintained with the same backend server, providing a consistent user experience. This combination effectively addresses the requirements of scaling and user session consistency.
Question 5 of 529
A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers.
The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company has already migrated the applications into a set of AWS Lambda functions.
Which solution will meet these requirements?
A.
Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header.
B.
Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the User-Agent header.
C.
Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the User-Agent. Associate the response data mapping with the HTTP API.
D.
Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header.
AnswerDiscussion
Correct Answer: A
To meet the requirements of migrating the service to AWS, using serverless technologies, and supporting older devices, the best solution involves using an Amazon CloudFront distribution along with a CloudFront function. CloudFront functions are optimized for lightweight tasks like inspecting HTTP headers and modifying requests or responses. This setup allows for efficient removal of problematic headers based on the User-Agent header in the requests, ensuring compatibility with older devices. This approach integrates well with the existing AWS Lambda functions and avoids the need for more complex and potentially costly alternatives like Lambda@Edge or non-serverless components such as an Application Load Balancer.
Question 6 of 529
A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3 bucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to access the files from its own AWS account (Account B).
Which combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)
A.
Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A.
B.
In Account A, set the S3 bucket policy to the following:
C.
In Account A, set the S3 bucket policy to the following:
D.
In Account B, set the permissions of User_DataProcessor to the following:
E.
In Account B, set the permissions of User_DataProcessor to the following:
AnswerDiscussion
Correct Answer: C, D
For User_DataProcessor in Account B to access the S3 bucket in Account A, two key steps are needed. Firstly, Account A needs to add a policy to the S3 bucket that explicitly allows the IAM user from Account B the necessary permissions to access the bucket. This is achieved by specifying the principal as the IAM user and granting the required actions, as shown in option C. Secondly, Account B must assign an IAM policy to User_DataProcessor that grants permission to perform the required actions (GetObject and ListBucket) on the S3 bucket in Account A, which is specified in option D. Together, these steps ensure that cross-account access is correctly configured.
Question 7 of 529
A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices that run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is variable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless architecture that minimizes operational complexity.
Which solution will meet these requirements MOST cost-effectively?
A.
Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle the expected peak load. Configure two separate Lambda integrations within Amazon API Gateway: one for production and one for testing.
B.
Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters.
C.
Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the EKS clusters.
D.
Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct traffic to the Elastic Beanstalk deployments.
AnswerDiscussion
Correct Answer: B
To design a cost-effective serverless architecture that minimizes operational complexity while refactoring a traditional web application as microservices, using Amazon Elastic Container Service (ECS) with the Fargate launch type is a suitable solution. ECS with Fargate allows automatic scaling of containers based on the load, which handles the variable workload effectively. By uploading the container images to Amazon Elastic Container Registry (ECR) and deploying tasks from these images, the operational management is streamlined. Configuring two auto-scaled ECS clusters ensures separate environments for production and testing. Additionally, using Application Load Balancers to direct traffic to the ECS clusters aids in efficiently distributing the load. This solution fully leverages AWS managed services, reducing the overall operational burden and cost.
Question 8 of 529
A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application’s data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.
The company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region. The company does not have a large enough budget for an active-active strategy.
What should a solutions architect recommend to meet these requirements?
A.
Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.
B.
Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application’s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.
C.
Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3.
D.
Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.
AnswerDiscussion
Correct Answer: B
To achieve an automatic failover to the backup region and maintain an RTO of less than 15 minutes within a limited budget, the solution should include mechanisms for monitoring the primary region and taking swift action if it becomes unhealthy. Configuring an AWS Lambda function in the backup region to promote the read replica and modify the Auto Scaling group values of instances ensures that resources can quickly be provisioned in the backup region when needed. Using Route 53 with a health check to monitor the web application and sending an SNS notification to trigger the Lambda function when the primary region is unhealthy allows traffic to be rerouted to the backup region promptly. This setup avoids the need for an expensive active-active strategy while providing the necessary failover capability.
Question 9 of 529
A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node cluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application to function, each piece of the infrastructure must be healthy and must be in an active state.
A solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least possible downtime.
Which combination of steps will meet these requirements? (Choose three.)
A.
Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances.
B.
Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited mode.
C.
Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in failure scenarios.
D.
Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones.
E.
Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances.
F.
Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster.
AnswerDiscussion
Correct Answer: A, D, F
To ensure automatic recovery from failure with minimal downtime, several steps can be taken. First, using an Elastic Load Balancer to distribute traffic across multiple EC2 instances and ensuring these instances are part of an Auto Scaling group with a minimum capacity of two instances can help maintain application availability if one instance fails. Then, modifying the DB instance to create a Multi-AZ deployment ensures that the database remains available by automatically failing over to a secondary availability zone in the event of an issue. Finally, creating a replication group for the ElastiCache for Redis cluster and enabling Multi-AZ on the cluster ensures that the in-memory data store is resilient and can fail over to another availability zone if necessary. This combination achieves a robust, highly available architecture capable of automatically recovering from failures with minimal downtime.
Question 10 of 529
A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.
After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.
While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.
Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)
A.
Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.
B.
Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.
C.
Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.
D.
Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.
E.
Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.
AnswerDiscussion
Correct Answer: A, E
To provide a custom error page instead of the standard ALB error page with the least operational overhead, two steps are necessary. First, create an Amazon S3 bucket to host a static webpage and upload the custom error pages to that S3 bucket. This allows for a highly-available location to store the error pages. Second, configure a CloudFront custom error page to handle the custom error responses effectively. This setup leverages existing services with minimal additional configuration, offering an efficient and scalable solution without the need for complex DNS changes or handling custom code for each error.
Question 11 of 529
A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the company can use to share a common network across multiple accounts.
The company’s infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to manage the network. Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to create AWS resources within subnets.
Which combination of actions should the solutions architect perform to meet these requirements? (Choose two.)
A.
Create a transit gateway in the infrastructure account.
B.
Enable resource sharing from the AWS Organizations management account.
C.
Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs in each individual account with the VPC in the infrastructure account.
D.
Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share.
E.
Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each prefix list to associate with the resource share.
AnswerDiscussion
Correct Answer: B, D
To share a common network across multiple AWS accounts managed by AWS Organizations, the infrastructure account should use AWS Resource Access Manager (RAM) to share subnets with other accounts. Enabling resource sharing from the AWS Organizations management account allows resources to be shared across all accounts within the organization. By creating a resource share in RAM within the infrastructure account and associating specific subnets with the resource share, the infrastructure team can manage the network while allowing individual accounts to create resources within the shared subnets.
Question 12 of 529
A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API calls. The third-party SaaS application also runs on AWS inside a VPC.
The company will consume the third-party SaaS application from inside a VPC. The company has internal security policies that mandate the use of private connectivity that does not traverse the internet. No resources that run in the company VPC are allowed to be accessed from outside the company’s VPC. All permissions must conform to the principles of least privilege.
Which solution meets these requirements?
A.
Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint.
B.
Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. Configure network ACLs to limit access across the VPN tunnels.
C.
Create a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed routes for the peering connection.
D.
Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service. Grant permissions for the endpoint service to the specific account of the third-party SaaS provider.
AnswerDiscussion
Correct Answer: A
To meet the company's requirements of private connectivity that does not traverse the internet, using AWS PrivateLink is the most appropriate solution. This solution involves creating an AWS PrivateLink interface VPC endpoint and connecting it to the endpoint service provided by the third-party SaaS application. This configuration allows the company to access the SaaS application securely and privately within the AWS network, adhering to internal security policies. Additionally, a security group can limit access to the endpoint, ensuring compliance with the principle of least privilege.
Question 13 of 529
A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to perform patching. Management requires a single report showing the patch status of all the servers and instances.
Which set of actions should a solutions architect take to meet these requirements?
A.
Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports.
B.
Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks to generate patch compliance reports.
C.
Use an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to generate patch compliance reports.
D.
Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports.
AnswerDiscussion
Correct Answer: A
AWS Systems Manager is designed to manage patches on both on-premises servers and EC2 instances. It also provides the capability to generate patch compliance reports. This makes it the most suitable solution for consolidating patch status information across varied server environments.
Question 14 of 529
A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on the application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied to a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2 instances.
Which set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?
A.
Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group to prevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK.
B.
Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance.
C.
Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data. Create an Amazon EventBridge rule to detect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge rule that uses the AWS CLI to run the user-data script to copy the log files and terminate the instance.
D.
Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (Amazon SNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send ABANDON to the Auto Scaling group to terminate the instance.
AnswerDiscussion
Correct Answer: B
To ensure log files are copied to the central S3 bucket from terminated EC2 instances, the best approach is to utilize AWS Systems Manager and Auto Scaling lifecycle hooks. By creating a Systems Manager document with a script to copy log files to S3 and employing a lifecycle hook, instances can perform necessary actions before termination. The EC2_INSTANCE_TERMINATING transition event triggers a Lambda function via EventBridge, which calls the Systems Manager API to run the document, ensuring log files are copied before allowing the instance termination to proceed with a CONTINUE signal. This method efficiently handles log preservation during scaling activities.
Question 15 of 529
A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The company’s applications and databases are running in Account B.
A solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53.
During deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53.
Which combination of steps should the solutions architect take to resolve this issue? (Choose two.)
A.
Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance’s private IP in the private hosted zone.
B.
Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.
C.
Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.
D.
Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts.
E.
Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.
AnswerDiscussion
Correct Answer: C, E
In a multi-account AWS setup, proper authorization and association steps are necessary to ensure DNS resolution across VPCs in different accounts. Firstly, creating an authorization to associate the private hosted zone in Account A with the new VPC in Account B is required. This enables the VPC in Account B to be authorized to use the hosted zone in Account A. Secondly, after the authorization, the new VPC in Account B needs to be associated with the private hosted zone in Account A. Deleting the association authorization in Account A afterwards is a best practice to prevent unintentional re-creation of the same association. This ensures that the db.example.com DNS name is resolvable from the EC2 instances in Account B, thus resolving the issue.
Question 16 of 529
A company used Amazon EC2 instances to deploy a web fleet to host a blog site. The EC2 instances are behind an Application Load Balancer (ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.
The company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user traffic. At peak times of day, users report buffering and timeout issues while attempting to reach the site or watch videos.
Which is the MOST cost-efficient and scalable deployment that will resolve the issues for users?
A.
Reconfigure Amazon EFS to enable maximum I/O.
B.
Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown.
C.
Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.
D.
Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB.
AnswerDiscussion
Correct Answer: C
To address the buffering and timeout issues due to increased user traffic and video content, the best approach is to configure an Amazon CloudFront distribution and migrate the videos from EFS to Amazon S3. CloudFront, being a CDN, caches content at edge locations close to users, reducing latency and improving load times. Amazon S3 provides scalable, high-performance storage suitable for large amounts of video content, thereby resolving performance issues more cost-effectively and efficiently compared to other options.
Question 17 of 529
A company with global offices has a single 1 Gbps AWS Direct Connect connection to a single AWS Region. The company’s on-premises network uses the connection to communicate with the company’s resources in the AWS Cloud. The connection has a single private virtual interface that connects to a single VPC.
A solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. The solution also must provide connectivity to other Regions through the same pair of Direct Connect connections as the company expands into other Regions.
Which solution meets these requirements?
A.
Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC.
B.
Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new private virtual interface on the new connection, and connect the new private virtual interface to the single VPC.
C.
Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new public virtual interface on the new connection, and connect the new public virtual interface to the single VPC.
D.
Provision a transit gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the transit gateway. Associate the transit gateway with the single VPC.
AnswerDiscussion
Correct Answer: A
To meet the requirements of implementing a redundant Direct Connect connection in the same region and enabling connectivity to other regions through the same pair of connections, the best solution is to provision a Direct Connect gateway. A Direct Connect gateway allows you to connect multiple VPCs and on-premises networks in different accounts and regions to a single Direct Connect connection. By deleting the existing private virtual interface from the current connection, creating the new connection, and establishing new private virtual interfaces on both connections that connect to the Direct Connect gateway, you can ensure redundancy and the ability to expand to other regions while maintaining connectivity through the same infrastructure. This approach provides both automatic failover and routing capabilities, making it a robust solution for the company's requirements.
Question 18 of 529
A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization.
The website contains static content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue. The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.
Which solution meets these requirements?
A.
Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.
B.
Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.
C.
Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.
D.
Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.
AnswerDiscussion
Correct Answer: C
The best solution is to host the web application in Amazon S3, which is ideal for static content and can scale to handle variable traffic efficiently. Uploaded videos should be stored in Amazon S3 as well. By using S3 event notifications to publish events to the SQS queue, the videos can then be processed by an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos. This solution leverages AWS managed services, reduces the need for managing EC2 instances and custom software, and meets the requirement to minimize operational overhead.
Question 19 of 529
A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current deployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the new function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to decrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.
How can this be accomplished?
A.
Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version.
B.
Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.
C.
Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version.
D.
Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint.
AnswerDiscussion
Correct Answer: B
Using AWS SAM (Serverless Application Model) along with AWS CodeDeploy allows for a more streamlined and efficient deployment process. AWS SAM provides the framework to build, test, and deploy serverless applications while leveraging AWS CodeDeploy to manage the rollout of new versions of the Lambda function. By gradually shifting traffic to the new version and using pre-traffic and post-traffic test functions to verify code, it reduces downtime and ensures functionality before fully committing to the new version. If errors are detected, the system can automatically roll back to the previous version using CloudWatch alarms. This method significantly decreases the time needed to deploy new versions and speeds up error detection and rollback, making the deployment process faster and more reliable.
Question 20 of 529
A company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible to the public.
The documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low. Availability and speed of retrieval are not concerns of the company.
Which solution will meet these requirements at the LOWEST cost?
A.
Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.
B.
Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class Configure the instance security groups to allow access only from private networks.
C.
Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instance security groups to allow access only from private networks.
D.
Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.
AnswerDiscussion
Correct Answer: A
The best solution for storing a large number of archived documents, ensuring they are accessed only through a VPN without being publicly available, is to use Amazon S3 with the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class. This option meets the requirement of low-cost storage as the data is infrequently accessed, and availability and speed are not concerns. S3 One Zone-IA provides a cost-effective solution for infrequently accessed data that does not require the high availability of standard storage classes. Configuring the bucket for access through an S3 interface endpoint ensures that the data remains accessible only within the VPC, maintaining security and privacy.
Question 21 of 529
A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to sign in to the company’s AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-premises environment and all the company’s AWS accounts.
The company’s security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a single location.
Which solution will meet these requirements?
A.
Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using attribute-based access controls (ABACs).
B.
Configure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using IAM Identity Center permission sets.
C.
In one of the company’s AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provider. Provision IAM users that are mapped to the federated users. Grant access that corresponds to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM users.
D.
In one of the company’s AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity provider. Provision IAM roles that grant access to the AWS account for the federated users that correspond to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM roles.
AnswerDiscussion
Correct Answer: A
The correct solution involves using AWS IAM Identity Center (AWS Single Sign-On) to connect to on-premises Active Directory via SAML 2.0. This configuration allows the company to use its existing user authentication service for AWS accounts. By enabling automatic provisioning using the System for Cross-domain Identity Management (SCIM) v2.0 protocol, it ensures user identities are managed in a single location. Granting access to AWS accounts using attribute-based access controls (ABACs) provides the required conditional access based on user groups and roles, aligning with the company's security policy.
Question 22 of 529
A software company has deployed an application that consumes a REST API by using Amazon API Gateway, AWS Lambda functions, and an Amazon DynamoDB table. The application is showing an increase in the number of errors during PUT requests. Most of the PUT calls come from a small number of clients that are authenticated with specific API keys.
A solutions architect has identified that a large number of the PUT requests originate from one client. The API is noncritical, and clients can tolerate retries of unsuccessful calls. However, the errors are displayed to customers and are causing damage to the API’s reputation.
What should the solutions architect recommend to improve the customer experience?
A.
Implement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and handled with descriptive error messages.
B.
Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.
C.
Turn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is appropriate for the workload.
D.
Implement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in traffic.
AnswerDiscussion
Correct Answer: B
To manage the high volume of PUT requests coming from a single client and causing errors, the best solution is to implement API throttling through a usage plan at the API Gateway level. This approach will effectively limit the number of requests a client can make, reducing the occurrence of errors. Ensuring that the client application properly handles code 429 replies without error will provide a better experience for users and help maintain the API's reputation.
Question 23 of 529
A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file system also runs on several EC2 instances that store 200 TB of data. The application reads and modifies the data on the shared file system and generates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete. The compute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage instances are all in the same AWS Region.
A solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access to the needed data for the duration of the 72-hour run.
Which solution will provide the LARGEST overall cost reduction while meeting these requirements?
A.
Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.
B.
Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a user data script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for the duration of the job. Detach the EBS volume when the job is complete
C.
Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.
D.
Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the file gateway as the shared storage for the job. Delete the file gateway when the job is complete.
AnswerDiscussion
Correct Answer: A
To meet the requirement of reducing costs while maintaining high performance access for the duration of the 72-hour run, migrating the data to an Amazon S3 bucket using the S3 Intelligent-Tiering storage class is an optimal solution. This class automatically moves infrequently accessed data to lower-cost storage tiers, thereby reducing costs significantly. Using Amazon FSx for Lustre to create a new file system with the data from Amazon S3 with lazy loading allows data to be loaded on-demand, which means only the data that is actually needed during the job will be accessed and loaded, further optimizing costs. Deleting the file system after the job ensures that costs are only incurred for the duration of the job, not continuously. This approach harnesses both cost-efficiency and high performance, making it the best option.
Question 24 of 529
A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is highly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible. The service must use fixed address assignments so other companies can add the addresses to their allow lists.
Assuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?
A.
Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static TCP port. Register EC2 instances with the NLB. Create a new name server record set named my.service.com, and assign the Elastic IP addresses of the EC2 instances to the record set. Provide the Elastic IP addresses of the EC2 instances to the other companies to add to their allow lists.
B.
Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create a Network Load Balancer (NLB) and expose the TCP port. Create a target group and assign the ECS cluster name to the NLCreate a new A record set named my.service.com, and assign the public IP addresses of the ECS cluster to the record set. Provide the public IP addresses of the ECS cluster to the other companies to add to their allow lists.
C.
Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.
D.
Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster. Create an Application Load Balancer (ALB) and expose the static TCP port. Create a target group and assign the ECS service definition name to the ALB. Create a new CNAME record set and associate the public IP addresses to the record set. Provide the Elastic IP addresses of the Amazon EC2 instances to the other companies to add to their allow lists.
AnswerDiscussion
Correct Answer: C
To ensure high availability and redundancy across multiple Availability Zones, the service should be distributed using a Network Load Balancer (NLB), which supports TCP traffic. By assigning Elastic IP addresses to the NLB for each Availability Zone, the service gains fixed address assignments that can be added to allow lists by other companies. Additionally, by registering Amazon EC2 instances with the NLB and creating an A (alias) record pointing to the NLB's DNS name, the service becomes accessible via the specified my.service.com domain.
Question 25 of 529
A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the company’s data center.
The system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes and 2 hours to finish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in less than 5 minutes and have no SLA. The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to meet SLAs. However, user jobs can be delayed.
A solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-term commitments. The solution must maintain high availability and must not affect the SLAs.
Which solution will meet these requirements MOST cost-effectively?
A.
Split the 12 instances across two Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run four instances in each Availability Zone as Spot Instances.
B.
Split the 12 instances across three Availability Zones in the chosen AWS Region. In one of the Availability Zones, run all four instances as On-Demand Instances with Capacity Reservations. Run the remaining instances as Spot Instances.
C.
Split the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with a Savings Plan. Run two instances in each Availability Zone as Spot Instances.
D.
Split the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run one instance in each Availability Zone as a Spot Instance.
AnswerDiscussion
Correct Answer: D
The correct solution should ensure high availability, meet the 65% SLA for scheduled jobs, and be cost-effective. Splitting the 12 instances across three Availability Zones enhances redundancy. Running three instances in each AZ as On-Demand Instances with Capacity Reservations ensures that there is always enough capacity to meet the SLAs even during outages. The remaining required capacity can be handled by one Spot Instance per AZ, offering cost savings without affecting the overall availability. This setup provides a balance of guaranteed availability for scheduled jobs and cost savings for user jobs.
Question 26 of 529
A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security:
The database must use strong, randomly generated passwords stored in a secure AWS managed service.
The application resources must be deployed through AWS CloudFormation.
The application must rotate credentials for the database every 90 days.
A solutions architect will generate a CloudFormation template to deploy the application.
Which resources specified in the CloudFormation template will meet the security engineer’s requirements with the LEAST amount of operational overhead?
A.
Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days.
B.
Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days.
C.
Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days.
D.
Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days.
AnswerDiscussion
Correct Answer: A
AWS Secrets Manager is designed to store and manage secrets, such as database credentials, and includes built-in support for automated rotation of secrets. By generating the database password as a secret in AWS Secrets Manager and specifying a Secrets Manager RotationSchedule resource, the requirement for automatic password rotation every 90 days can be easily fulfilled. This approach leverages native AWS capabilities to minimize operational overhead, ensuring secure and efficient credential management. Other options, such as using AWS Systems Manager Parameter Store, do not have integrated secret rotation features and would require additional custom logic, increasing complexity.
Question 27 of 529
A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data accessible publicly through a simple API over HTTPS. The solution must scale automatically in response to demand.
Which solutions meet these requirements? (Choose two.)
A.
Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway’s AWS integration type.
B.
Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to Dynamo DB by using API Gateway’s AWS integration type.
C.
Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.
D.
Create an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables.
E.
Create a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions.
AnswerDiscussion
Correct Answer: A, C
To create a simple API over HTTPS accessible publicly and capable of scaling automatically, you can use Amazon API Gateway. Option A involves creating a REST API with direct integrations to DynamoDB using API Gateway's AWS integration type, which allows efficient and straightforward access to DynamoDB data. Option C involves using an HTTP API integrated with AWS Lambda functions that access DynamoDB tables, leveraging the flexibility and automatic scaling capabilities of Lambda. Using both of these options together ensures a robust and scalable serverless architecture.
Question 28 of 529
A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will redirect online visitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are managed by Amazon Route 53.
A solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.
Which combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose three.)
A.
Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.
B.
Create an Application Load Balancer that includes HTTP and HTTPS listeners.
C.
Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.
D.
Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.
E.
Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.
F.
Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.
AnswerDiscussion
Correct Answer: C, D, F
To meet the requirements with the least amount of operational effort, the solutions architect should implement the following steps: Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL. This reduces the reliance on traditional web servers. Use an Amazon API Gateway API with a custom domain to publish the AWS Lambda function, which will facilitate the redirection of HTTP and HTTPS requests. API Gateway manages the HTTPS listener for you, further reducing operational complexity. Create an SSL certificate by using AWS Certificate Manager (ACM) and include the domains as Subject Alternative Names to ensure secure HTTPS requests can be properly handled.
Question 29 of 529
A company that has multiple AWS accounts is using AWS Organizations. The company’s AWS accounts host VPCs, Amazon EC2 instances, and containers.
The company’s compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2 instances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-related resources with a key of “costCenter” and a value or “compliance”.
The company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the compliance team’s AWS account. The cost calculation must be as accurate as possible.
What should a solutions architect do to meet these requirements?
A.
In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.
B.
In the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Schedule a monthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter tagged resources.
C.
In the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly AWS Cost and Usage Report. Use the tag breakdown in the report to calculate the total cost for the costCenter tagged resources.
D.
Create a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the costCenter tagged resources in the compliance team’s AWS account.
AnswerDiscussion
Correct Answer: A
To accurately identify and track the costs associated with the security tools that run on EC2 instances across multiple AWS accounts, the management account in AWS Organizations must activate the costCenter user-defined tag. By configuring monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account, the company can leverage the tag breakdown feature in these reports to obtain the total cost for resources tagged with costCenter. This approach ensures that the compliance team’s AWS account is accurately charged for the use of the security tools, providing an organization-wide view of cost allocation that is both comprehensive and accurate.
Question 30 of 529
A company has 50 AWS accounts that are members of an organization in AWS Organizations. Each account contains multiple VPCs. The company wants to use AWS Transit Gateway to establish connectivity between the VPCs in each member account. Each time a new member account is created, the company wants to automate the process of creating a new VPC and a transit gateway attachment.
Which combination of steps will meet these requirements? (Choose two.)
A.
From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.
B.
From the management account, share the transit gateway with member accounts by using an AWS Organizations SCP.
C.
Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID.
D.
Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a peering transit gateway attachment in a member account. Share the attachment with the transit gateway in the management account by using a transit gateway service-linked role.
E.
From the management account, share the transit gateway with member accounts by using AWS Service Catalog.
AnswerDiscussion
Correct Answer: A, C
To meet the requirements, first, from the management account, the transit gateway should be shared with member accounts using AWS Resource Access Manager to enable resource sharing across different accounts. Then, an AWS CloudFormation stack set should be launched from the management account to automate the creation of a new VPC and a VPC transit gateway attachment in each member account. This stack set should associate the attachment with the transit gateway in the management account using the transit gateway ID to ensure connectivity.
Question 31 of 529
An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS Organizations account structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by procurement managers. The procurement team’s policy indicates that developers should be able to obtain third-party software from an approved list only and use Private Marketplace in AWS Marketplace to achieve this requirement. The procurement team wants administration of Private Marketplace to be restricted to a role named procurement-manager-role, which could be assumed by procurement managers. Other IAM users, groups, roles, and account administrators in the company should be denied Private Marketplace administrative access.
What is the MOST efficient way to design an architecture to meet these requirements?
A.
Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the PowerUserAccess managed policy to the role. Apply an inline policy to all IAM users and roles in every AWS account to deny permissions on the AWSPrivateMarketplaceAdminFullAccess managed policy.
B.
Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the AdministratorAccess managed policy to the role. Define a permissions boundary with the AWSPrivateMarketplaceAdminFullAccess managed policy and attach it to all the developer roles.
C.
Create an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization.
D.
Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developers. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an SCP in Organizations to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Apply the SCP to all the shared services accounts in the organization.
AnswerDiscussion
Correct Answer: C
To meet the requirements efficiently, create an IAM role named procurement-manager-role in all shared services accounts and assign the AWSPrivateMarketplaceAdminFullAccess policy to it. Then, implement organization root-level Service Control Policies (SCPs) to restrict administrative access to Private Marketplace. The first SCP should deny everyone except the procurement-manager-role permission to administer Private Marketplace. The second SCP should prevent the creation of an IAM role named procurement-manager-role across the organization. This ensures only procurement managers have the required administrative permissions while effectively blocking other users.
Question 32 of 529
A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon DynamoDB. The developers account resides in a dedicated organizational unit (OU). The solutions architect has implemented the following SCP on the developers account:
When this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy.
What should the solutions architect do to eliminate the developers’ ability to use services outside the scope of this policy?
A.
Create an explicit deny statement for each AWS service that should be constrained.
B.
Remove the FullAWSAccess SCP from the developers account’s OU.
C.
Modify the FullAWSAccess SCP to explicitly deny all services.
D.
Add an explicit deny statement using a wildcard to the end of the SCP.
AnswerDiscussion
Correct Answer: B
The proper way to ensure that developers cannot use services outside of Amazon EC2, Amazon S3, and Amazon DynamoDB is to remove the FullAWSAccess Service Control Policy (SCP) from the developers' account's Organizational Unit (OU). The FullAWSAccess SCP, if attached, allows all services by default. Removing this policy will effectively block access to any services that are not explicitly allowed by other SCPs applied to the developers' account or the OU it resides in.
Question 33 of 529
A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC. Mobile clients connect to the API by using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing policy with the IP addresses of all the EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to traffic. The app has not been able to keep up with the traffic.
A solutions architect needs to implement a solution so that the app can handle the new and varying load.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Separate the API into individual AWS Lambda functions. Configure an Amazon API Gateway REST API with Lambda integration for the backend. Update the Route 53 record to point to the API Gateway API.
B.
Containerize the API logic. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Run the containers in the cluster by using Amazon EC2. Create a Kubernetes ingress. Update the Route 53 record to point to the Kubernetes ingress.
C.
Create an Auto Scaling group. Place all the EC2 instances in the Auto Scaling group. Configure the Auto Scaling group to perform scaling actions that are based on CPU utilization. Create an AWS Lambda function that reacts to Auto Scaling group changes and updates the Route 53 record.
D.
Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances as targets for the ALB. Update the Route 53 record to point to the ALB.
AnswerDiscussion
Correct Answer: D
To handle the new and varying load on the REST-based API with the least operational overhead, it is most effective to create an Application Load Balancer (ALB) in front of the EC2 instances. This approach allows effective distribution of incoming traffic across multiple instances, ensuring that no single instance gets overwhelmed. Additionally, moving the EC2 instances to private subnets enhances security while the ALB takes care of managing the traffic. This solution requires minimal changes to the current setup and does not involve significant developmental refactoring or the need to manage container orchestration frameworks, making it the option with the least operational overhead.
Question 34 of 529
A company has created an OU in AWS Organizations for each of its engineering teams. Each OU owns multiple AWS accounts. The organization has hundreds of AWS accounts.
A solutions architect must design a solution so that each OU can view a breakdown of usage costs across its AWS accounts.
Which solution meets these requirements?
A.
Create an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access Manager. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.
B.
Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.
C.
Create an AWS Cost and Usage Report (CUR) in each AWS Organizations member account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.
D.
Create an AWS Cost and Usage Report (CUR) by using AWS Systems Manager. Allow each team to visualize the CUR through Systems Manager OpsCenter dashboards.
AnswerDiscussion
Correct Answer: B
The most effective solution to allow each OU to view a breakdown of usage costs across its AWS accounts is to create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. This approach ensures that all cost data is consolidated in one place, making it easier to manage and visualize. The CUR can then be visualized through an Amazon QuickSight dashboard, which provides a user-friendly interface for the teams to access and analyze their cost data.
Question 35 of 529
A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily.
The company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company already has established an AWS Direct Connect connection between the on-premises network and AWS.
Which data migration strategy should the company use?
A.
Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway.
B.
Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.
C.
Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).
D.
Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).
AnswerDiscussion
Correct Answer: B
The company needs a data migration strategy to have the data available on a file system in the cloud. AWS DataSync is designed to efficiently transfer large amounts of data between on-premises storage and AWS storage services. Amazon FSx for Windows File Server provides fully managed Windows file servers in the cloud, which is suitable for the company's Windows-based workload. Therefore, using AWS DataSync to schedule daily tasks to replicate data between the on-premises Windows file server and Amazon FSx is the best solution. This approach ensures seamless integration and availability of the data in the cloud.
Question 36 of 529
A company’s solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3 bucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a second Region.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Configure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set by using a weighted routing policy for each S3 bucket. Configure the application to reference the objects by using the Route 53 DNS name.
B.
Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda function each time an object is written to the S3 bucket in us-east-1. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.
C.
Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.
D.
Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region.
AnswerDiscussion
Correct Answer: C
The best solution for achieving resiliency across multiple AWS Regions with the least operational overhead is to configure S3 Cross-Region Replication (CRR) to replicate objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. This ensures automatic and continuous replication of objects between the two buckets. Additionally, setting up an Amazon CloudFront distribution with an origin group that includes both S3 buckets allows CloudFront to handle the failover automatically, without requiring manual updates or complex configurations in the application code. This setup minimizes operational overhead while achieving the desired resiliency.
Question 37 of 529
A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in traffic that resulted in downtime and a significant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a dependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily users.
Which steps should the solutions architect take to design an appropriate solution?
A.
Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance. The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones. Use an Amazon Route 53 alias record to route traffic from the company’s domain to the NLB.
B.
Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company’s domain to the ALB.
C.
Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Region. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica. Use Amazon Route 53 with a geoproximity routing policy to route traffic between the two Regions.
D.
Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot instances spanning three Availability Zones. The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy. Use an Amazon Route 53 alias record to route traffic from the company’s domain to the ALB.
AnswerDiscussion
Correct Answer: B
To design a scalable and highly available solution for migrating a .NET application with a MySQL database to AWS, the solution should utilize AWS CloudFormation to launch a stack. This stack would feature an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group that spans three Availability Zones, ensuring scalability and fault tolerance. For the database, a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster will provide high availability and automatic failover. Using an Amazon Route 53 alias record will efficiently route traffic from the company’s domain to the ALB, ensuring proper DNS resolution and load distribution. This approach meets the demands for scalability, high availability, and proper routing while leveraging AWS services designed for these purposes.
Question 38 of 529
A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts.
A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations.
What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?
A.
Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.
B.
Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.
C.
Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.
D.
Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection.
AnswerDiscussion
Correct Answer: C
To deploy an Amazon SNS topic through AWS CloudFormation StackSets in all AWS Organizations member accounts, the stack set must be created in the management account. This central management simplifies operations and ensures consistent deployment across all member accounts. Service-managed permissions leverage AWS CloudFormation to manage necessary roles, streamlining permissions. Setting deployment options to deploy to the organization ensures deployment to all member accounts. Enabling automatic deployment ensures that any new accounts added to the organization automatically receive the stack set, maintaining consistency across the organization.
Question 39 of 529
A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux and Windows. The company has a large on-premises infrastructure that consists of physical machines and VMs that host numerous applications.
The company must capture details about the system configuration, system performance, running processes, and network connections of its on-premises workloads. The company also must divide the on-premises applications into groups for AWS migrations. The company needs recommendations for Amazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effective manner.
Which combination of steps should a solutions architect take to meet these requirements? (Choose three.)
A.
Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs.
B.
Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs.
C.
Group servers into applications for migration by using AWS Systems Manager Application Manager.
D.
Group servers into applications for migration by using AWS Migration Hub.
E.
Generate recommended instance types and associated costs by using AWS Migration Hub.
F.
Import data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization.
AnswerDiscussion
Correct Answer: A, D, E
The requirements necessitate capturing details about system configurations, performance, running processes, and network connections of on-premises workloads. For this, using the AWS Application Discovery Agent is appropriate as it collects comprehensive data from physical machines and VMs. Grouping servers into applications for migration can be efficiently handled by AWS Migration Hub, which provides a centralized location and tools for organizing and tracking migrations. Lastly, generating recommended instance types and associated costs should also be done through AWS Migration Hub, which has the capability to analyze the collected data and recommend suitable EC2 instances for cost-effective migration.
Question 40 of 529
A company is hosting an image-processing service on AWS in a VPC. The VPC extends across two Availability Zones. Each Availability Zone contains one public subnet and one private subnet.
The service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service. The service needs to communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The EC2 instances retrieve approximately 1 ТВ of data from an S3 bucket each day.
The company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without compromising the service’s security posture or increasing the time spent on ongoing operations.
Which solution will meet these requirements?
A.
Replace the NAT gateways with NAT instances. In the VPC route table, create a route from the private subnets to the NAT instances.
B.
Move the EC2 instances to the public subnets. Remove the NAT gateways.
C.
Set up an S3 gateway VPC endpoint in the VPAttach an endpoint policy to the endpoint to allow the required actions on the S3 bucket.
D.
Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances. Host the images on the EFS volume.
AnswerDiscussion
Correct Answer: C
An S3 gateway VPC endpoint allows the EC2 instances in the private subnets to communicate directly with Amazon S3 without needing to go through the NAT gateways or the Internet. This reduces the cost associated with NAT gateways while still maintaining a high level of security since the data does not leave the AWS network. Additionally, setting up an endpoint policy ensures that only the required actions on the S3 bucket are allowed, maintaining the security posture and not increasing operational overhead. This setup is also beneficial for big data transfers such as the 1 TB of data retrieved daily from S3.
Question 41 of 529
A company recently deployed an application on AWS. The application uses Amazon DynamoDB. The company measured the application load and configured the RCUs and WCUs on the DynamoDB table to match the expected peak load. The peak load occurs once a week for a 4-hour period and is double the average load. The application load is close to the average load for the rest of the week. The access pattern includes many more writes to the table than reads of the table.
A solutions architect needs to implement a solution to minimize the cost of the table.
Which solution will meet these requirements?
A.
Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load.
B.
Configure on-demand capacity mode for the table.
C.
Configure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table.
D.
Configure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table.
AnswerDiscussion
Correct Answer: A
The most appropriate solution to minimize the cost of the DynamoDB table is to use AWS Application Auto Scaling to increase capacity during the peak period and purchase reserved RCUs and WCUs to match the average load. This approach utilizes reserved capacity for the consistent average load, which is more cost-effective, and leverages auto-scaling to handle the known peak periods, ensuring the table can accommodate higher traffic without incurring the higher costs associated with on-demand capacity.
Question 42 of 529
A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with the number of files rapidly declining after business hours.
What is the MOST cost-effective migration recommendation?
A.
Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket.
B.
Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete.
C.
Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS.
D.
Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.
AnswerDiscussion
Correct Answer: D
Creating a queue using Amazon SQS and configuring the existing web server to publish to the new queue ensures reliable message queuing with high availability and low cost. Utilizing Amazon EC2 instances in an EC2 Auto Scaling group allows for scalable processing of requests from the queue. The EC2 instances can be automatically scaled based on the SQS queue length, providing flexibility to handle higher loads during peak hours and reducing costs during off-peak hours. Storing the processed files in an Amazon S3 bucket further minimizes storage costs while ensuring durability and availability.
Question 43 of 529
A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes from an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the company deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.
The company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.
Which solution will meet these requirements MOST cost-effectively?
A.
Replace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.
B.
Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy.
C.
Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Add cold storage nodes to the cluster Transition the indexes from UltraWarm to cold storage. Delete the input data from the S3 bucket after 1 month by using an S3 Lifecycle policy.
D.
Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.
AnswerDiscussion
Correct Answer: B
To meet the requirements most cost-effectively, the OpenSearch Service cluster should reduce the number of data nodes to 2 and utilize UltraWarm nodes to handle the expected capacity. UltraWarm nodes are designed for large volumes of read-only data and are cost-effective for this use case. Configuring indexes to transition to UltraWarm immediately upon ingestion optimizes storage costs. After 1 month, the input data should be transitioned from S3 Standard to S3 Glacier Deep Archive using an S3 Lifecycle policy because S3 Glacier Deep Archive is the most cost-effective long-term storage solution for compliance purposes.
Question 44 of 529
A company has 10 accounts that are part of an organization in AWS Organizations. AWS Config is configured in each account. All accounts belong to either the Prod OU or the NonProd OU.
The company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic when an Amazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source. The company’s security team is subscribed to the SNS topic.
For all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source.
Which solution will meet this requirement with the LEAST operational overhead?
A.
Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic. Deploy the updated rule to the NonProd OU.
B.
Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU.
C.
Configure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. Apply the SCP to the NonProd OU.
D.
Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Apply the SCP to the NonProd OU.
AnswerDiscussion
Correct Answer: D
The correct solution is to configure an SCP (Service Control Policy) to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Applying this SCP to the NonProd OU will prevent the creation of any security group inbound rule that includes 0.0.0.0/0 as the source IP. This approach meets the requirement with the least operational overhead as it proactively blocks the undesirable action at the policy enforcement level, avoiding the need for reactive measures or additional steps to remove such rules after creation.
Question 45 of 529
A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud. The company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an Application Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a serverless architecture.
Which solution will meet these requirements with the LEAST operational overhead?
A.
For each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs.
B.
Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint.
C.
Deploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB endpoint.
D.
Containerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS Fargate. Create an Amazon API Gateway REST API, and set Fargate as the target. Update the Git servers to call the API Gateway endpoint.
AnswerDiscussion
Correct Answer: B
The solution with the least operational overhead is to create an Amazon API Gateway HTTP API and implement each webhook logic in a separate AWS Lambda function. This approach leverages the serverless capabilities of both API Gateway and Lambda, which automatically manage the underlying infrastructure, scaling, and maintenance. By using API Gateway as the single point of entry, you eliminate the need to manage multiple individual Lambda function URLs and ensure that updates can be centralized through the API Gateway configuration. This greatly reduces the complexity and operational effort compared to other options, such as containerizing the logic or using App Runner, which would require more complex setups and additional services.
Question 46 of 529
A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company’s data center. As part of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes. The company then wants to query and analyze the data.
Which solution will meet these requirements?
A.
Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select.
B.
Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight.
C.
Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console.
D.
Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.
AnswerDiscussion
Correct Answer: D
The most suitable solution involves deploying the AWS Application Discovery Agent to each on-premises server. This agent is specifically designed to collect detailed server metrics including CPU details, RAM usage, operating system information, and running processes. Once data collection is complete, configuring Data Exploration in AWS Migration Hub allows for efficient analysis and querying. Amazon Athena can then be used to run predefined queries against the data stored in Amazon S3, providing a powerful and flexible way to analyze the server metrics. This approach ensures comprehensive data collection and robust query capabilities, meeting the company’s requirements effectively.
Question 47 of 529
A company is building a serverless application that runs on an AWS Lambda function that is attached to a VPC. The company needs to integrate the application with a new service from an external provider. The external provider supports only requests that come from public IPv4 addresses that are in an allow list.
The company must provide a single public IP address to the external provider before the application can start using the new service.
Which solution will give the application the ability to access the new service?
A.
Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.
B.
Deploy an egress-only internet gateway. Associate an Elastic IP address with the egress-only internet gateway. Configure the elastic network interface on the Lambda function to use the egress-only internet gateway.
C.
Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the Lambda function to use the internet gateway.
D.
Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the default route in the public VPC route table to use the internet gateway.
AnswerDiscussion
Correct Answer: A
To enable an AWS Lambda function in a VPC to access an external service that requires requests to come from a single public IPv4 address, you should deploy a NAT gateway. By associating an Elastic IP address with the NAT gateway, the outbound traffic from the Lambda function routed through the NAT gateway will appear to come from this single public IP address. This setup allows the external provider to whitelist the specific public IP address, enabling the Lambda function to access the new service. The NAT gateway will translate the private IP address of the Lambda function to the Elastic IP address, ensuring that all traffic appears to come from a single public IP address.
Question 48 of 529
A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.
During testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The solutions architect must improve the application’s performance.
Which actions should the solutions architect take to meet these requirements? (Choose two.)
A.
Use the cluster endpoint of the Aurora database.
B.
Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.
C.
Use the Lambda Provisioned Concurrency feature.
D.
Move the code for opening the database connection in the Lambda function outside of the event handler.
E.
Change the API Gateway endpoint to an edge-optimized endpoint.
AnswerDiscussion
Correct Answer: B, D
To improve the performance of the application under high load when it opens a large number of database connections, the solutions architect should use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database. RDS Proxy helps manage and efficiently pool database connections, reducing the overhead of opening and closing connections, thereby improving performance. Additionally, moving the code for opening the database connection in the Lambda function outside of the event handler can help by reusing the database connection across multiple requests. This avoids the time-consuming process of establishing a new connection for each request and reduces resource usage.
Question 49 of 529
A company is planning to host a web application on AWS and wants to load balance the traffic across a group of Amazon EC2 instances. One of the security requirements is to enable end-to-end encryption in transit between the client and the web server.
Which solution will meet this requirement?
A.
Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Export the SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances.
B.
Associate the EC2 instances with a target group. Provision an SSL certificate using AWS Certificate Manager (ACM). Create an Amazon CloudFront distribution and configure it to use the SSL certificate. Set CloudFront to use the target group as the origin server.
C.
Place the EC2 instances behind an Application Load Balancer (ALB) Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Provision a third-party SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances.
D.
Place the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each EC2 instance. Configure the NLB to listen on port 443 and to forward traffic to port 443 on the instances.
AnswerDiscussion
Correct Answer: D
To achieve end-to-end encryption in transit between the client and the web server, using a Network Load Balancer (NLB) with TCP listener on port 443 is the appropriate solution. This approach ensures that encrypted traffic is forwarded to the EC2 instances without decrypting at the NLB, maintaining encryption from the client all the way to the web server. Therefore, the NLB option meets the requirement for end-to-end encryption effectively.
Question 50 of 529
A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js applications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into reports. When the aggregation jobs run, some of the load jobs fail to run correctly.
The company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the company’s customers.
What should a solutions architect do to meet these requirements?
A.
Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind a Network Load Balancer (NLB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the NLB.
B.
Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Move the aggregation jobs to run against the Aurora MySQL database. Set up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group. When the databases are synced, point the collector DNS record to the ALDisable the AWS DMS sync task after the cutover from on premises to AWS.
C.
Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS.
D.
Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as an Amazon Kinesis data stream. Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the Kinesis data stream.
AnswerDiscussion
Correct Answer: C
To migrate the data analytics environment from on-premises to AWS while addressing the loading issues and ensuring no interruptions or changes for customers, setting up an Amazon Aurora MySQL database and using AWS Database Migration Service (AWS DMS) for continuous data replication from the on-premises database is crucial. Creating an Aurora Replica allows the aggregation jobs to run against it, thereby offloading read operations from the primary database and mitigating the load issues. Utilizing AWS Lambda functions behind an Application Load Balancer (ALB) for collection endpoints, combined with Amazon RDS Proxy to handle database connections, provides scalability and reliability. Once the databases are synced, pointing the collector DNS record to the ALB and disabling the AWS DMS sync task ensures a smooth transition without affecting the end users.
Question 51 of 529
A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption with S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keys that the company’s security team manages. The S3 bucket does not have versioning enabled.
Which solution will meet these requirements?
A.
In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests.
B.
In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.
C.
In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests.
D.
In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket.
AnswerDiscussion
Correct Answer: B
To meet the requirement of encrypting all current and future objects in the S3 bucket with keys that the company’s security team manages, the appropriate solution is to use server-side encryption with AWS KMS managed encryption keys (SSE-KMS). This allows the company to specify a customer managed key that they control through AWS KMS. The process involves changing the default encryption in the S3 bucket properties to SSE-KMS, setting an S3 bucket policy to deny unencrypted PutObject requests, and using the AWS CLI to re-upload all existing objects to ensure they conform to the new encryption standard. This approach ensures that both new and existing objects are encrypted with the keys managed by the company’s security team.
Question 52 of 529
A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).
The company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an origin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.
A solutions architect must configure the application so that itis highly available and fault tolerant.
Which solution meets these requirements?
A.
Provision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add both of the CloudFront distributions as values. Create Route 53 health checks.
B.
Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALCreate an origin group for the two origins. Configure one origin as primary and one origin as secondary.
C.
Provision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the ALB. Set up the failover routing algorithm on the ALB.
D.
Provision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new application setup as an origin. Create an AWS Global Accelerator accelerator. Add both of the CloudFront distributions as endpoints.
AnswerDiscussion
Correct Answer: B
To ensure the application is highly available and fault-tolerant, the solution involves provisioning an additional ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. This setup ensures redundancy and failover capability. By updating the CloudFront distribution to include a second origin for the new ALB and creating an origin group with both origins, CloudFront can automatically route traffic to the healthy origin if the primary one fails. This approach leverages CloudFront's global reach to improve availability and fault tolerance without depending on DNS-level changes, which could introduce latency.
Question 53 of 529
A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a transit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured between all of the company’s global offices and the transit account. The company has AWS Config enabled on all of its accounts.
The company’s networking team needs to centrally manage a list of internal IP address ranges that belong to the global offices. Developers will reference this list to gain access to their applications securely.
Which solution meets these requirements with the LEAST amount of operational overhead?
A.
Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can be invoked when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update all relevant security group rules with the updated IP address ranges.
B.
Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each of the accounts to ensure compliance with the list of IP address ranges. Configure the rule to automatically remediate any noncompliant security group that is detected.
C.
In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts.
D.
In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts to reference the transit account’s security group by using a nested security group reference of “/sg-1a2b3c4d”.
AnswerDiscussion
Correct Answer: C
The best solution is to create a VPC prefix list with all of the internal IP address ranges in the transit account and share this prefix list with all other accounts using AWS Resource Access Manager. This method allows for central management of the IP address ranges and reduces operational overhead. By using the prefix list to configure security group rules in other accounts, this solution simplifies the updating process and ensures consistent security configurations across the organization.
Question 54 of 529
A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and uses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API method.
The company wants to create a CSV report every 2 weeks to show each API Lambda function’s recommended configured memory, recommended cost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.
Which solution will meet these requirements with the LEAST development time?
A.
Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.
B.
Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.
C.
Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks.
D.
Purchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks.
AnswerDiscussion
Correct Answer: B
Opting in to AWS Compute Optimizer and creating a Lambda function that calls the ExportLambdaFunctionRecommendations operation is the most efficient solution to minimize development time. AWS Compute Optimizer already performs the necessary analysis and recommendations for AWS Lambda functions, including cost and memory optimization. By leveraging the ExportLambdaFunctionRecommendations API, the company can automate the extraction of these recommendations directly into a CSV file that is then stored in an S3 bucket. Scheduling this Lambda function to run every two weeks using Amazon EventBridge ensures automation with minimal code and development effort.
Question 55 of 529
A company’s factory and automation applications are running in a single VPC. More than 20 applications run on a combination of Amazon EC2, Amazon Elastic Container Service (Amazon ECS), and Amazon RDS.
The company has software engineers spread across three teams. One of the three teams owns each application, and each time is responsible for the cost and performance of all of its applications. Team resources have tags that represent their application and team. The teams use IAM access for daily activities.
The company needs to determine which costs on the monthly AWS bill are attributable to each application or team. The company also must be able to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must recommend an AWS Billing and Cost Management solution that provides these cost reports.
Which combination of actions will meet these requirements? (Choose three.)
A.
Activate the user-define cost allocation tags that represent the application and the team.
B.
Activate the AWS generated cost allocation tags that represent the application and the team.
C.
Create a cost category for each application in Billing and Cost Management.
D.
Activate IAM access to Billing and Cost Management.
E.
Create a cost budget.
F.
Enable Cost Explorer.
AnswerDiscussion
Correct Answer: A, D, F
To accurately determine which costs on the monthly AWS bill are attributable to each application or team and to create detailed reports comparing costs for the last 12 months and forecasting costs for the next 12 months, the following actions are necessary: First, activate the user-defined cost allocation tags that represent the application and the team, ensuring costs can be attributed appropriately. Second, enable Cost Explorer, which allows for analyzing past spending and forecasting future costs. Lastly, it's crucial to activate IAM access to Billing and Cost Management, so that team members who are responsible for their application's costs and performance can access necessary billing information and reports.
Question 56 of 529
An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client’s allow list.
The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets.
How should a solutions architect ensure that the web application can continue to call the third-party API after the migration?
A.
Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.
B.
Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.
C.
Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB.
D.
Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint.
AnswerDiscussion
Correct Answer: B
To ensure that the migrated web application can continue to call the third-party API, the solution must allow the third party to whitelist a single public CIDR block through which requests will appear to come from. By registering a block of customer-owned public IP addresses in the AWS account and creating Elastic IP addresses from that block to assign to the NAT gateways in the VPC, all outgoing traffic from the EC2 instances to the third-party API will be routed through the NAT gateways. This configuration ensures that the source IP seen by the third-party API remains consistent and allows using the customer-owned CIDR block for the allow list.
Question 57 of 529
A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An administrator created the following SCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:
Developers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the administrator address this problem?
A.
Add s3:CreateBucket with “Allow” effect to the SCP.
B.
Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.
C.
Instruct the developers to add Amazon S3 permissions to their IAM entities.
D.
Remove the SCP from account 1111-1111-1111.
AnswerDiscussion
Correct Answer: C
The problem described does not originate from the Service Control Policy (SCP) itself based on the SCP content provided. The SCP is configured to allow all actions except for actions related to AWS CloudTrail, which are explicitly denied. Therefore, the inability for developers to create Amazon S3 buckets is not due to this SCP, as it does not restrict S3 actions. SCPs define guardrails and set limits on actions and do not grant permissions directly. The likely issue is that the developers' IAM entities lack the necessary permissions to create S3 buckets. Therefore, the correct way to address this problem is to instruct the developers to add Amazon S3 permissions to their IAM entities, as IAM permissions are required to perform actions within the AWS accounts.
Question 58 of 529
A company has a monolithic application that is critical to the company’s business. The company hosts the application on an Amazon EC2 instance that runs Amazon Linux 2. The company’s application team receives a directive from the legal department to back up the data from the instance’s encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the administrative SSH key pair for the instance. The application must continue to serve the users.
Which solution will meet these requirements?
A.
Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3.
B.
Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.
C.
Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.
D.
Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.
AnswerDiscussion
Correct Answer: A
To back up the data from an EC2 instance's EBS volume to an S3 bucket without interruption to the running application and without requiring SSH access, attach an IAM role to the instance with permissions to write to S3. Then, use the AWS Systems Manager Session Manager to access the instance and run commands to copy the data to S3. This method enables remote access without needing administrative SSH keys and maintains the continuous operation of the application.
Question 59 of 529
A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucket in a new AWS account. The solutions architect must implement a solution that uses the AWS CLI.
Which combination of steps will successfully copy the data? (Choose three.)
A.
Create a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket. Attach the bucket policy to the destination bucket.
B.
Create a bucket policy to allow a user in the destination account to list the source bucket’s contents and read the source bucket’s objects. Attach the bucket policy to the source bucket.
C.
Create an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user.
D.
Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set objectACLs in the destination bucket. Attach the policy to the user.
E.
Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data.
F.
Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data.
AnswerDiscussion
Correct Answer: B, D, F
To copy data from a source S3 bucket in one AWS account to a destination S3 bucket in a new AWS account using the AWS CLI, you need to follow these steps: Create a bucket policy to allow a user in the destination account to list the source bucket’s contents and read its objects. Attach this policy to the source bucket to grant necessary permissions. Then, create an IAM policy in the destination account allowing a user to list contents and get objects from the source bucket, as well as to list contents, put objects, and set object ACLs in the destination bucket. Attach this policy to the user. Finally, run the 'aws s3 sync' command as a user in the destination account. By executing the sync command as a user in the destination account, you ensure that the copied objects will have appropriate permissions for users in the destination account. Therefore, the correct steps are B, D, and F.
Question 60 of 529
A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.
Which solution will meet these requirements?
A.
Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.
B.
Deploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load.
C.
Create a version for every new deployed Lambda function. Use the AWS CLI update-function-configuration command with the routing-config parameter to distribute the load.
D.
Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load.
AnswerDiscussion
Correct Answer: A
To support a canary release for an AWS Lambda function, creating an alias for every new deployed version of the Lambda function is effective. Using the AWS CLI update-alias command with the routing-config parameter allows you to distribute traffic between different versions of the function. This approach allows you to gradually shift traffic to the new version, monitor its performance, and quickly roll back if any issues arise. It provides a simpler and more effective solution compared to deploying to a new CloudFormation stack or using CodeDeploy, especially given that specific CodeDeploy configurations were not mentioned as options.
Question 61 of 529
A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties. The company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the files are uploaded, they are moved to the data lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route 53.
What should a solutions architect do to improve the reliability and scalability of the SFTP solution?
A.
Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB.
B.
Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.
C.
Migrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the file gateway endpoint.
D.
Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB.
AnswerDiscussion
Correct Answer: B
Migrating the SFTP server to AWS Transfer for SFTP would significantly improve the reliability and scalability of the SFTP solution. AWS Transfer for SFTP is a fully managed service provided by AWS, which means that AWS handles the infrastructure management, including high availability and automatic scaling. This solution allows the company to transfer files directly into and out of Amazon S3 using the SFTP protocol, thereby integrating seamlessly with the existing data lake setup on S3. Updating the DNS record in Route 53 to point to the server endpoint hostname ensures that the service remains reachable using the existing DNS name.
Question 62 of 529
A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions architect must preserve the software and configuration settings during the migration.
What should the solutions architect do to meet these requirements?
A.
Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the VMware data store. Use VM Import/Export to move the VMs to Amazon EC2.
B.
Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command.
C.
Configure AWS Storage Gateway for files service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared folder. Sign in to the AWS Management Console and create an AMI from the backup copy. Launch an EC2 instance that is based on the AMI.
D.
Create a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on the on-premises VM. Register the VM with Systems Manager to be a managed instance. Use AWS Backup to create a snapshot of the VM and create an AMI. Launch an EC2 instance that is based on the AMI.
AnswerDiscussion
Correct Answer: B
To migrate an application from VMware Infrastructure in an on-premises data center to Amazon EC2 while preserving software and configuration settings, use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Store the image in an Amazon S3 bucket in the destination AWS Region. Create and apply an IAM role for VM Import and use the AWS CLI to run the EC2 import command. This method ensures the software and configuration settings are maintained during the migration process.
Question 63 of 529
A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed image in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and runs by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.
The application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing frequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application’s architecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)
A.
Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).
B.
Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.
C.
Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of the Lambda function.
D.
Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.
E.
Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB instance. Adjust the Lambda function to mount the EFS file share.
AnswerDiscussion
Correct Answer: A, B
The current application is experiencing timeout errors due to the increased size of the images being processed. To address this issue, it is advisable to containerize the application code and run it on a service that can handle the higher resource requirements. First, by modifying the application deployment to build a Docker image and publishing it to Amazon Elastic Container Registry (ECR), it ensures that the application is packaged efficiently. Then, creating a new Amazon Elastic Container Service (ECS) task definition with a compatibility type of AWS Fargate allows the containerized application to be run without managing the underlying infrastructure. Fargate automatically allocates the necessary resources, mitigating timeout errors, and adhering to the company's requirement of not managing the underlying infrastructure.
Question 64 of 529
A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization. The company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company’s production OU.
Which solution will meet this requirement?
A.
Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU.
B.
Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.
C.
Use AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU.
D.
Create a custom SCP in AWS Control Tower. Apply the SCP to the production OU.
AnswerDiscussion
Correct Answer: B
To meet the requirement of detecting Amazon RDS DB instances that are not encrypted at rest in the company’s production OU, enabling the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower is the suitable solution. These strongly recommended guardrails offer checks for best practices and additional security measures that are not automatically enforced but can be used to enforce policies such as encryption at rest for RDS instances. This will ensure that all RDS instances in the production OU are checked for encryption compliance, thus fulfilling the company's requirement.
Question 65 of 529
A startup company hosts a fleet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company’s engineers rely heavily on SSH access to the instances for troubleshooting.
The company’s existing architecture includes the following:
• A VPC with private and public subnets, and a NAT gateway.
• Site-to-Site VPN for connectivity with the on-premises environment.
• EC2 security groups with direct SSH access from the on-premises environment.
The company needs to increase security controls around SSH access and provide auditing of commands run by the engineers.
Which strategy should a solutions architect use?
A.
Install and configure EC2 Instance Connect on the fleet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI.
B.
Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer’s devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs.
C.
Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer’s devices. Enable AWS Config for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules.
D.
Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager.
AnswerDiscussion
Correct Answer: D
Creating an IAM role with the AmazonSSMManagedInstanceCore managed policy and using AWS Systems Manager Session Manager provides enhanced security and audit capabilities without the need for SSH. By attaching the IAM role to the EC2 instances, SSH access can be restricted by removing inbound TCP on port 22. Engineers can then use Systems Manager Session Manager to start sessions, which are logged for auditing purposes. This approach increases security by removing the dependency on IP-based security group rules and ensures that all activities are tracked and auditable.
Question 66 of 529
A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed, developers use their company email address to request an account. The company wants to ensure that developers are not launching costly services or running services unnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.
Which combination of steps will meet these requirements? (Choose three.)
A.
Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.
B.
Use AWS Budgets to create a fixed monthly budget for each developer’s account as part of the account creation process.
C.
Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.
D.
Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts.
E.
Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all services.
F.
Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services.
AnswerDiscussion
Correct Answer: B, C, F
To meet the requirements, first, create a fixed monthly budget for each developer’s account using AWS Budgets. Second, create a Service Control Policy (SCP) to deny access to costly services and components, which can be applied to the developer accounts to ensure they do not launch expensive resources. Finally, set up an AWS Budgets alert action to send an SNS notification when the budgeted amount is reached. This notification can invoke a Lambda function to terminate all services, effectively managing costs and ensuring that developers stay within their monthly budgets.
Question 67 of 529
A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the applications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions by using a deployment package. The company has configured automated backups for Aurora.
The company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application processes critical data, so the company must minimize downtime.
Which solution will meet these requirements?
A.
Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the automated Aurora DB cluster snapshot with the Target account.
B.
Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster.
C.
Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant the Target account permission to clone the Aurora DB cluster.
D.
Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the Target account.
AnswerDiscussion
Correct Answer: A
To migrate the Lambda functions and the Aurora database to a new AWS account while minimizing downtime, the most practical solution involves creating new Lambda functions in the Target account using the deployment package and sharing the automated Aurora DB cluster snapshot with the Target account. This method leverages existing functionalities to ensure minimal disruption. Creating new Lambda functions in the Target account ensures that the Lambda functions are correctly set up without relying on potentially unsupported sharing methods. Sharing the automated Aurora DB cluster snapshot allows the Target account to restore the database and continue operations with the critical data intact.
Question 68 of 529
A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an Amazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess a file that the script has already processed.
The company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the file processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term management overhead.
Which solution will meet these requirements MOST cost-effectively?
A.
Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects.
B.
Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a minimum size of one instance. Update the data processing script to poll the SQS queue. Process the S3 objects that the SQS message identifies.
C.
Migrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to poll the S3 bucket for new objects and to process the resulting objects.
D.
Migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 event notification to invoke the Lambda function.
AnswerDiscussion
Correct Answer: A
Migrating the data processing script to an AWS Lambda function and using an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects is the most cost-effective solution. Lambda is serverless, which means there is no need to manage any infrastructure. It automatically scales based on the number of requests, ensuring high availability and scaling capabilities. The pay-per-use pricing model of Lambda will likely be more cost-effective for this workload compared to running an EC2 instance or managing ECS containers, especially considering that the script runs for only 5 minutes every 10 minutes. This approach also minimizes long-term management overhead.
Question 69 of 529
A financial services company in North America plans to release a new online web application to its customers on AWS. The company will launch the application in the us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet user traffic. The company also wants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-passive failover.
Which solution will meet these requirements?
A.
Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB.
B.
Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPC. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC. Place the Auto Scaling group behind the ALSet up the same configuration in the us-west-1 VPC. Create an Amazon Route 53 hosted zone. Create separate records for each ALEnable health checks to ensure high availability between Regions.
C.
Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPCreate an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPPlace the Auto Scaling group behind the ALB. Set up the same configuration in the us-west-1 VPCreate an Amazon Route 53 hosted zone. Create separate records for each ALB. Enable health checks and configure a failover routing policy for each record.
D.
Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB. Create an Amazon Route 53 hosted zone. Create a record for the ALB.
AnswerDiscussion
Correct Answer: C
To achieve high availability and dynamic scaling on AWS, the application must span multiple Availability Zones within us-east-1. Additionally, a disaster recovery site with active-passive failover must be configured in us-west-1. The correct approach involves setting up a VPC and placing an Application Load Balancer (ALB) and Auto Scaling group within each region. The ALB ensures traffic distribution across multiple Availability Zones, while the Auto Scaling group manages instance scaling. In both regions, the resources are independently configured to ensure resiliency. Implementing Amazon Route 53 with health checks and a failover routing policy between the ALBs in us-east-1 and us-west-1 guarantees that traffic will be directed to the secondary region in case of a primary region failure, fulfilling both high availability and disaster recovery requirements.
Question 70 of 529
A company has an environment that has a single AWS account. A solutions architect is reviewing the environment to recommend what the company could improve specifically in terms of access to the AWS Management Console. The company’s IT support workers currently access the console for administrative tasks, authenticating with named IAM users that have been mapped to their job role.
The IT support workers no longer want to maintain both their Active Directory and IAM user accounts. They want to be able to access the console by using their existing Active Directory credentials. The solutions architect is using AWS IAM Identity Center (AWS Single Sign-On) to implement this functionality.
Which solution will meet these requirements MOST cost-effectively?
A.
Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company’s on-premises Active Directory. Configure IAM Identity Center and set the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.
B.
Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure an AD Connector to connect to the company’s on-premises Active Directory. Configure IAM Identity Center and select the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company’s Active Directory.
C.
Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company’s on-premises Active Directory. Configure IAM Identity Center and select the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.
D.
Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure an AD Connector to connect to the company’s on-premises Active Directory. Configure IAM Identity Center and set the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company’s Active Directory.
AnswerDiscussion
Correct Answer: D
To enable IT support workers to access the AWS Management Console using their existing Active Directory credentials without maintaining separate IAM user accounts, the solution involves leveraging the company's existing on-premises Active Directory infrastructure. Creating an organization in AWS Organizations and enabling all features is necessary to manage accounts with IAM Identity Center (AWS SSO). Using an AD Connector to connect to the on-premises Active Directory and configuring IAM Identity Center to set the AD Connector as the identity source is the most cost-effective solution. This approach avoids the need for a new AWS Managed Microsoft AD directory, reducing costs and simplifying the setup while meeting all specified requirements.
Question 71 of 529
A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-east-1 Region. The files range in size from 1 GB to 10 GB.
Users who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload for these users. A solutions architect must improve the app’s performance for these uploads.
Which solutions will meet these requirements? (Choose two.)
A.
Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.
B.
Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3 bucket.
C.
Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region.
D.
Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.
E.
Modify the app to add random prefixes to the files before uploading.
AnswerDiscussion
Correct Answer: A, D
To improve the upload performance for users in Australia, enabling S3 Transfer Acceleration on the S3 bucket will provide a fast and secure way to transfer large files over the internet by utilizing Amazon CloudFront’s globally distributed edge locations. This can significantly reduce latency and improve transfer speeds. Additionally, configuring the app to break the videos into chunks and using multipart uploads will allow the large files to be uploaded in parallel, which increases upload speed and mitigates the risk of failure for large files due to reduced size of individual parts being uploaded.
Question 72 of 529
An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After a failover test, the application lost the connections to the database and could not re-establish the connections. After a restart of the application, the application re-established the connections.
A solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.
Which solution will meet these requirements?
A.
Create an Amazon Aurora MySQL Serverless v1 DB instance. Migrate the RDS DB instance to the Aurora Serverless v1 DB instance. Update the connection settings in the application to point to the Aurora reader endpoint.
B.
Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.
C.
Create a two-node Amazon Aurora MySQL DB cluster. Migrate the RDS DB instance to the Aurora DB cluster. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.
D.
Create an Amazon S3 bucket. Export the database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Configure Amazon Athena to use the S3 bucket as a data store. Install the latest Open Database Connectivity (ODBC) driver for the application. Update the connection settings in the application to point to the Athena endpoint
AnswerDiscussion
Correct Answer: B
The application requires a solution that enables it to re-establish database connections automatically after a failover without needing a restart. RDS Proxy is designed to help with this by pooling and sharing database connections, which makes applications more resilient to database failures. It can automatically detect failovers and reroute connections to standby instances, maintaining application uptime and performance. Creating an RDS proxy, configuring the existing RDS endpoint as a target, and updating the application's connection settings to point to the RDS proxy endpoint effectively addresses the problem without requiring a full migration or more complex solutions.
Question 73 of 529
A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution and send data. Each device needs to be able to send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Set up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to Amazon MQ.
B.
Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in an Auto Scaling group. Set the Auto Scaling group as the target for the NLConnect each device to the NLB.
C.
Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.
D.
Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB. Configure a mutual TLS certificate authorizer on the HTTP API. Run an MQTT broker on an Amazon EC2 instance that the NLB targets. Connect each device to the NLB.
AnswerDiscussion
Correct Answer: C
The best solution to meet the requirements with the least operational overhead is to set up AWS IoT Core, create a corresponding AWS IoT thing for each device, and provision a certificate. AWS IoT Core provides a fully managed service that enables secure, bi-directional communication between internet-connected devices and the AWS Cloud. It supports the MQTT protocol and includes built-in device authentication and access control. This effectively minimizes operational complexity compared to other options, making it the most suitable choice for the scenario described.
Question 74 of 529
A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved resources and that engineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to enforce the new restriction on the IAM role that the engineers use for access.
What should the solutions architect do to create the solution?
A.
Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket. Update the IAM policy for the engineers’ IAM role to only allow access to Amazon S3 and AWS CloudFormation. Use AWS CloudFormation templates to provision resources.
B.
Update the IAM policy for the engineers’ IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.
C.
Update the IAM policy for the engineers’ IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.
D.
Provision resources in AWS CloudFormation stacks. Update the IAM policy for the engineers’ IAM role to only allow access to their own AWS CloudFormation stack.
AnswerDiscussion
Correct Answer: C
The correct solution involves updating the IAM policy for the engineers' IAM role to only allow AWS CloudFormation actions and creating a new IAM policy with permission to provision approved resources. This new policy should be assigned to a new IAM service role which CloudFormation will assume during stack creation. This approach ensures that engineers can only manage resources through CloudFormation and that only approved resources can be provisioned, effectively enforcing the company policy.
Question 75 of 529
A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The application is designed to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and needs to be stored in a durable location where it can be retrieved with low latency. The data is ephemeral and the company is required to store the data for 120 days only, after which the data can be deleted.
The solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.
Which storage strategy is the MOST cost-effective and meets the design requirements?
A.
Design the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieval. Configure a lifecycle policy to delete data older than 120 days.
B.
Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days.
C.
Design the application to store each incoming record in a single table in an Amazon RDS MySQL database. Run a nightly cron job that runs a query to delete any records older than 120 days.
D.
Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days.
AnswerDiscussion
Correct Answer: B
The best storage strategy involves storing each incoming record in an Amazon DynamoDB table, configured for the scale required and using the Time to Live (TTL) feature to automatically delete records older than 120 days. DynamoDB is designed for high scale and performance, capable of handling millions of small records efficiently with low latency. This makes it suitable for the requirements of low-latency retrieval and durability. Additionally, the TTL feature systematizes the deletion process, aligning with the 120-day data retention period, making this solution both effective and cost-efficient given the volume and frequency of the data.
Question 76 of 529
A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all times for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.
Which solution will provide the HIGHEST availability for the database?
A.
Configure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.
B.
Configure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.
C.
Configure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.
D.
Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.
AnswerDiscussion
Correct Answer: D
To achieve the highest availability for the database, configuring read replicas on Amazon RDS and promoting a cross-Region read replica to a standalone DB instance in case of disruption is the most effective solution. This configuration ensures that there is a backup ready to take over immediately if the primary database fails, minimizing downtime and data loss. Using cross-Region replicas also enhances availability by distributing the copies across different geographic locations, thereby protecting against regional outages and providing a resilient disaster recovery setup.
Question 77 of 529
Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to VPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A. Example Corp. just acquired AnyCompany, which has a VPC named VPC B. There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B.
Example Corp. wants to connect from its on-premise servers to VPC B. Example Corp. has properly set up the network ACL and security groups.
Which solution will meet this requirement with the LEAST operational effort?
A.
Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.
B.
Create a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN connection to the transit gateway. Add a route to direct traffic to the peered VPCs, and add an authorization rule to give clients access to the VPCs A and B.
C.
Update the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks. Wait for up to 5 minutes for BGP propagation to finish.
D.
Modify the Site-to-Site VPN’s virtual private gateway definition to include VPC A and VPC B. Split the two routers of the virtual private getaway between the two VPCs.
AnswerDiscussion
Correct Answer: A
To enable seamless communication between the on-premises network and VPC B with minimal operational effort, the best solution is to create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway, and update the transit gateway route tables for all networks to include IP range routes for all other networks. This approach simplifies network management and provides a centralized point for managing connectivity between the on-premises network and both VPCs.
Question 78 of 529
A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the migrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends outbound email messages to the company’s customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.
The company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has created and validated the SES domain. The company has lifted the SES limits.
What should the company do to modify the application to send email messages from Amazon SES?
A.
Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.
B.
Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.
C.
Configure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Use the IAM role as a service role for Amazon SES.
D.
Configure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use the access keys to authenticate with Amazon SES.
AnswerDiscussion
Correct Answer: B
To send email messages from Amazon SES using an application that can use SMTP only, configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials and use them to authenticate with Amazon SES. STARTTLS is used to secure connections, and this approach requires minimal changes to the application while adhering to security best practices.
Question 79 of 529
A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method. The acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found it difficult to generate a cost report that contains meaningful groups for all the teams.
The acquiring company’s finance team needs a solution to report on costs for all the companies through a self-managed application.
Which solution will meet these requirements?
A.
Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team.
B.
Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.
C.
Create an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the finance team.
D.
Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.
AnswerDiscussion
Correct Answer: A
To meet the requirements of generating detailed and meaningful cost reports for all the companies within the consolidated AWS organization, the best solution is to create an AWS Cost and Usage Report for the organization and define tags and cost categories in the report. This approach ensures a granular level of detail for the cost reporting. By creating a table in Amazon Athena and then an Amazon QuickSight dataset based on that Athena table, the finance team will be able to perform detailed queries and generate reports on the costs. The dataset can be easily shared with the finance team, enabling them to build and view the reports as needed.
Question 80 of 529
A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company’s Node.js API servers on Amazon EC2 instances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume.
The number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are consistently overloaded and RDS metrics show high write latency.
Which of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-efficient? (Choose two.)
A.
Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume’s IOPS.
B.
Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.
C.
Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.
D.
Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.
E.
Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.
AnswerDiscussion
Correct Answer: B, C
To address the API servers being consistently overloaded and the high write latency in RDS, leveraging Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data is an effective solution. This setup allows real-time data processing and can reduce the load on the API servers. Additionally, re-architecting the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and adding read replicas can significantly improve performance and scalability. Aurora offers enhanced write performance, automatic storage scaling, and read replicas which help in distributing the read load, thus enabling the system to handle the expected increase in data from new sensors.
Question 81 of 529
A company is building an electronic document management system in which users upload their documents. The application stack is entirely serverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for delivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.
The company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of Europe.
Which combination of actions will meet these requirements? (Choose two.)
A.
Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs.
B.
Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution.
C.
Change the API Gateway Regional endpoints to edge-optimized endpoints.
D.
Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.
E.
Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database.
AnswerDiscussion
Correct Answer: A, C
To improve latency for users outside of Europe, two main actions can be taken. First, enabling S3 Transfer Acceleration on the S3 bucket and using Transfer Acceleration signed URLs will speed up the upload of documents by utilizing Amazon CloudFront's globally distributed edge locations. This helps reduce latency by routing data through the AWS edge network, closer to users, before reaching the S3 bucket. Second, changing the API Gateway Regional endpoints to edge-optimized endpoints will improve latency by routing API requests to the nearest CloudFront Point of Presence, thereby reducing the time it takes for users' requests to reach the server and receive a response. These two measures directly address latency issues by leveraging AWS’s global infrastructure to bring data transfer and API processing closer to users, leading to notable performance improvements.
Question 82 of 529
An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and videos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.
The company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions architect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.
Which solution will meet these requirements?
A.
Configure S3 Intelligent-Tiering on the S3 bucket.
B.
Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days.
C.
Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances.
D.
Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days.
AnswerDiscussion
Correct Answer: A
The company needs a cost-effective storage solution that still provides millisecond retrieval availability. Amazon S3 Intelligent-Tiering automatically moves objects between two access tiers (frequent and infrequent) based on access patterns, making it suitable for situations where data access frequency changes over time. This tiering ensures cost optimization without manual interventions and maintains millisecond retrieval availability, which is crucial for frequently accessed data post 30 days. Other options either involve deeper archiving which increases retrieval times, introduce additional complexities and costs with other services, or focus on caching which doesn't optimize storage costs effectively.
Question 83 of 529
A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during the past year.
A solutions architect needs to review data trends for the past 12 months and identity the appropriate storage class for the objects.
Which solution will meet these requirements?
A.
Download AWS Cost and Usage Reports for the last 12 months of S3 usage. Review AWS Trusted Advisor recommendations for cost savings.
B.
Use S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard to analyze storage trends.
C.
Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends.
D.
Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 months. Import the .csv file to an Amazon QuickSight dashboard.
AnswerDiscussion
Correct Answer: C
To review data trends for the past 12 months and identify the appropriate storage class for objects in Amazon S3, Amazon S3 Storage Lens is the suitable solution. S3 Storage Lens provides comprehensive metrics and insights on storage usage and activity trends up to 15 months. It allows for in-depth analysis of historical data, making it ideal for cost optimization and determining the best storage class based on actual usage patterns.
Question 84 of 529
A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company’s business expansion plan includes deployments in multiple Regions across multiple AWS accounts.
What should the solutions architect do to meet these requirements?
A.
Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.
B.
Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.
C.
Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.
D.
Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.
AnswerDiscussion
Correct Answer: C
To manage deployments across multiple AWS accounts and regions, the best solution involves using AWS Organizations along with AWS CloudFormation StackSets. AWS Organizations allows centralized management of multiple accounts, facilitating governance and scalability. AWS CloudFormation StackSets extends the capability of CloudFormation by enabling the deployment of a single template across multiple accounts and regions, ensuring consistent provisioning and management of resources. This combination ensures efficient and scalable infrastructure management in a multi-account, multi-region setup.
Question 85 of 529
A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company’s business expansion plan includes deployments in multiple Regions across multiple AWS accounts.
What should the solutions architect do to meet these requirements?
A.
Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.
B.
Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.
C.
Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.
D.
Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.
AnswerDiscussion
Correct Answer: C
To meet the requirement of defining infrastructure as code and deploying it across multiple AWS Regions and accounts, the best approach is to use AWS Organizations and AWS CloudFormation StackSets. AWS Organizations allows for centralized management of multiple accounts, providing the necessary organizational structure. AWS CloudFormation StackSets enable the deployment of CloudFormation stacks across multiple AWS accounts and Regions from a central administrator account. This combination ensures streamlined management and deployment of resources across the required infrastructure.
Question 86 of 529
A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be upgraded to support the modern design for the application with the following requirements:
• It should allow changes to be released several times every hour.
• It should be able to roll back the changes as quickly as possible.
Which design will meet these requirements?
A.
Deploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances.
B.
Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs.
C.
Use AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and use Amazon Route 53 weighted routing to point to the new environment.
D.
Roll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances. and phase out all instances that use the previous AMI version with the configured termination policy during a deployment event.
AnswerDiscussion
Correct Answer: B
Specifying AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application and swapping the staging and production environment URLs will allow changes to be released several times every hour and enable quick rollbacks. Elastic Beanstalk natively supports environment swapping, which minimizes downtime and provides a simple and efficient way to manage deployment and rollback processes.
Question 87 of 529
A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where the application needs to access an Amazon Aurora DB Cluster. The EC2 instances are all associated with the same security group. The DB cluster is associated with its own security group.
The solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster.
Which combination of steps will meet these requirements? (Choose two.)
A.
Add an inbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the source over the default Aurora port.
B.
Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port.
C.
Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port.
D.
Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the default Aurora port.
E.
Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the ephemeral ports.
AnswerDiscussion
Correct Answer: B, C
To ensure the application on the EC2 instances has least privilege access to the Amazon Aurora DB Cluster, the necessary security group rules should allow the appropriate traffic. First, add an outbound rule to the EC2 instances' security group specifying the DB cluster's security group as the destination over the default Aurora port. This allows the EC2 instances to initiate connections to the DB cluster. Second, add an inbound rule to the DB cluster's security group specifying the EC2 instances' security group as the source over the default Aurora port. This allows the DB cluster to accept connections from the EC2 instances. This combination ensures that only the necessary traffic is permitted, adhering to the principle of least privilege.
Question 88 of 529
A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for each business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold.
Which solution is the MOST cost-effective way to meet these requirements?
A.
Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit.
B.
Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization's management account to create monthly reports for each business unit.
C.
Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit.
D.
Enable AWS Cost and Usage Reports in the organization's management account and configure reports grouped by application, environment. and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit's email list.
AnswerDiscussion
Correct Answer: B
The most cost-effective solution is to configure AWS Budgets in the organization's management account and configure budget alerts grouped by application, environment, and owner. This configuration enables centralized management of budgets and alerts, eliminating the need to set up and manage these settings individually in each account. Using Cost Explorer in the management account allows for consolidated spending views and report creation, which simplifies and reduces the cost of managing cloud spending for each business unit.
Question 89 of 529
A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is deleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.
How can the company prevent users from accidentally deleting data in this way?
A.
Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.
B.
Configure a stack policy that disallows the deletion of RDS and EBS resources.
C.
Modify IAM policies lo deny deleting RDS and EBS resources that are tagged with an "aws:cloudformation:stack-name" tag.
D.
Use AWS Config rules to prevent deleting RDS and EBS resources.
AnswerDiscussion
Correct Answer: A
To prevent important data stored in Amazon RDS databases or Amazon EBS volumes from being deleted when a CloudFormation stack is deleted, the company can add a DeletionPolicy attribute to their CloudFormation templates. This attribute can be set to 'Retain' or 'Snapshot' for the specific resources, ensuring that the data is preserved or a snapshot is created rather than the resources being deleted. This approach directly aligns with the concern of retaining data even when the stack itself is deleted.
Question 90 of 529
A company has VPC flow logs enabled for Its NAT gateway. The company is seeing Action = ACCEPT for inbound traffic that comes from public IP address 198.51.100.2 destined for a private Amazon EC2 instance.
A solutions architect must determine whether the traffic represents unsolicited inbound connections from the internet. The first two octets of the VPC CIDR block are 203.0.
Which set of steps should the solutions architect take to meet these requirements?
A.
Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interlace. Run a query to filter with the destination address set as "like 203.0" and the source address set as "like 198.51.100.2". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.
B.
Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as "like 203.0" and the source address set as "like 198.51.100.2". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.
C.
Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance’s elastic network interface. Run a query to filter with the destination address set as "like 198.51.100.2" and the source address set as "like 203.0". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.
D.
Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as "like 198.51.100.2" and the source address set as "like 203.0". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.
AnswerDiscussion
Correct Answer: D
To determine whether the traffic represents unsolicited inbound connections from the internet, the solutions architect should investigate if the traffic to the public IP 198.51.100.2 was initially solicited from an internal private IP (such as those within the 203.0.x.x range). This can be done by checking the flow logs to see traffic patterns starting from the internal private IP and destined for the public IP 198.51.100.2 in Amazon CloudWatch. Filtering the logs with the destination address set to '198.51.100.2' and the source address set to '203.0' will help determine if the inbound traffic from the internet was a response to a request initiated from within the VPC.
Question 91 of 529
A company consists or two separate business units. Each business unit has its own AWS account within a single organization in AWS Organizations. The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3 bucket in each account and configured low-way replication between the S3 buckets. The S3 buckets have millions of objects.
Recently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be stored with encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).
What is the MOST operationally efficient solution that meets these requirements?
A.
Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location.
B.
Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Encrypt the existing objects by using an S3 copy command in the AWS CLI.
C.
Turn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI.
D.
Create an AWS Key Management Service, (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Use S3 Batch Operations to copy the objects into the same location.
AnswerDiscussion
Correct Answer: A
To meet the company policy requirement of encrypting all documents at rest and considering the operational efficiency due to the large number of objects, enabling SSE-S3 directly on both S3 buckets and using S3 Batch Operations to encrypt the existing objects in place is the most efficient solution. S3 Batch Operations is designed to handle operations on a large scale efficiently, and copying objects in place allows for encryption without moving data unnecessarily.
Question 92 of 529
A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3 bucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes every day.
The company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must retain all the data indefinitely for compliance reasons.
Which solution will meet these requirements MOST cost-effectively?
A.
Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.
B.
Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old 10 S3 Glacier Deep Archive.
C.
Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.
D.
Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering.
AnswerDiscussion
Correct Answer: C
To meet the company's requirements most cost-effectively, using an AWS Glue Data Catalog and Amazon Athena to query the data is a suitable solution. AWS Glue Data Catalog is a managed metadata repository that helps in defining and organizing the data stored in Amazon S3. Amazon Athena is a serverless, interactive query service, allowing efficient data analysis directly in S3 using SQL. This combination is especially cost-effective for analyzing large amounts of unstructured or semi-structured data. Additionally, creating an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive ensures that storage costs are minimized while retaining the data indefinitely for compliance. This approach minimizes costs more effectively than maintaining data in the S3 Standard storage class.
Question 93 of 529
A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and wants to use AWS.
The company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps. and multiple departments share the connection.
Which solution will meet these requirements MOST cost-effectively?
A.
Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.
B.
Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.
C.
Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN connection.
D.
Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway.
AnswerDiscussion
Correct Answer: A
The most cost-effective solution for transferring 600 TB of compressed data to AWS within 3 weeks, while ensuring data encryption in transit, is to order several AWS Snowball Edge Storage Optimized devices. These devices are designed specifically for transferring large amounts of data quickly and securely. The devices handle the data transfer without relying on the company's internet connection, eliminating bandwidth constraints. Once the data is copied, the devices are shipped back to AWS, where the data is loaded into the specified S3 bucket.
Question 94 of 529
A company has migrated Its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files through a web application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on Amazon EC2 instances and an Amazon RDS for PostgreSQL database.
When forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team member then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before entering the information into another system that uses an API.
A solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction. minimize time to market, and minimize tong-term operational overhead.
Which solution will meet these requirements?
A.
Develop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tier. Use this tier to process the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. Submit the data to the target system's APL. Host the new application tier on EC2 instances.
B.
Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on an EC2 instance to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.
C.
Host a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine teaming (AI/ML) models that are trained and hosted in Amazon SageMaker to perform optical character recognition (OCR) on the forms. Store the output in Amazon ElastiCache. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.
D.
Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.
AnswerDiscussion
Correct Answer: D
Extending the system with an application tier that uses AWS Step Functions and AWS Lambda, configured to use Amazon Textract and Amazon Comprehend, is the best option. This approach effectively addresses the requirements of automating the manual processing of forms with accurate form extraction while also minimizing time to market and long-term operational overhead. Amazon Textract can perform optical character recognition (OCR) to automatically extract text and data from scanned documents, and Amazon Comprehend can analyze text for key insights. Both services are fully managed and serverless, minimizing the need for custom development and ongoing maintenance. Additionally, using Step Functions and Lambda allows for easy orchestration and scalability.
Question 95 of 529
A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not want to make any major changes to the application.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.
B.
Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.
C.
Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend.
D.
Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.
AnswerDiscussion
Correct Answer: A
To meet the requirements with the least operational overhead, the best solution is to create an Amazon Machine Image (AMI) of the web server VM and use it to launch EC2 instances within an EC2 Auto Scaling group, along with an Application Load Balancer for the web front end. This approach allows for easy scaling and high availability. Replacing RabbitMQ with Amazon MQ, a managed service compatible with RabbitMQ, ensures no major changes to the messaging system, maintaining compatibility and reliability. Finally, using Amazon Elastic Kubernetes Service (EKS) to host the containerized backend is efficient because it manages the Kubernetes control plane, thus reducing operational overhead and providing a managed environment for the existing Kubernetes cluster. This solution maintains as much of the existing infrastructure as possible, aligning with the company's requirement to avoid major changes.
Question 96 of 529
A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The solutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.
The solutions architect created the following IAM policy and attached it to an IAM role:
During tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object resulted in an error message. The error message stated that the action was forbidden.
Which action must the solutions architect add to the IAM policy to meet all the requirements?
A.
kms:GenerateDataKey
B.
kms:GetKeyPolicy
C.
kms:GetPublicKey
D.
kms:Sign
AnswerDiscussion
Correct Answer: A
To implement client-side encryption for objects stored in Amazon S3, the solutions architect needs the ability to generate a data key for encryption and decryption operations. This requires the addition of the 'kms:GenerateDataKey' action to the IAM policy, which allows the role to request AWS Key Management Service (KMS) to generate a unique data key for encrypting S3 objects. Without this permission, the IAM role would not have the necessary access to perform the encryption, leading to the error encountered during the upload of new objects.
Question 97 of 529
A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer. The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affect legitimate traffic to the application.
How should a solutions architect configure the web ACLs to meet these requirements?
A.
Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.
B.
Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking.
C.
Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs.
D.
Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Allow to Block.
AnswerDiscussion
Correct Answer: A
To implement AWS WAF web ACLs effectively without affecting legitimate traffic, setting the action of the web ACL rules to Count initially is prudent. This approach allows monitoring and logging of traffic to understand the patterns and identify false positives. Over time, analyzing the logged data helps in fine-tuning the rules to enhance accuracy. Once the rules are verified to minimize false positives, the action can be gradually changed from Count to Block, thus improving security without disrupting legitimate traffic.
Question 98 of 529
A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company manages common security group rules for the AWS accounts in the organization.
The company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company’s on-premises network. Developers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own AWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.
The solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.
Which solution meets these requirements with the LEAST amount of operational overhead?
A.
Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS account. Deploy an AWS Lambda function in each AWS account. Configure the Lambda function to run every time an SNS topic receives a message. Configure the Lambda function to take an IP address as input and add it to a list of security groups in the account. Instruct the security team to distribute changes by publishing messages to its SNS topic.
B.
Create new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all internal CIDR ranges. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups. Instruct the security team to share updates with each AWS account owner.
C.
Create a new customer-managed prefix list in the security team’s AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups.
D.
Create an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in the security team’s AWS account. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account.
AnswerDiscussion
Correct Answer: C
To meet the requirements with the least operational overhead, the solution should leverage a centralized management approach. Creating a new customer-managed prefix list in the security team's AWS account and populating it with all internal CIDR ranges allows centralized control. By sharing this prefix list with the organization using AWS Resource Access Manager, each AWS account can then reference the shared prefix list in their security groups. This approach minimizes the need for setting up additional resources in each account and reduces ongoing maintenance tasks, such as manually updating or notifying account owners of changes, which significantly lowers the overall operational overhead.
Question 99 of 529
A company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a VPN. The company is hosting internal applications with VPCs in multiple AWS accounts. Currently, the applications are accessible from the company's on-premises office network through an AWS Site-to-Site VPN connection. The VPC in the company's main AWS account has peering connections established with VPCs in other AWS accounts.
A solutions architect must design a scalable AWS Client VPN solution for employees to use while they work from home.
What is the MOST cost-effective solution that meets these requirements?
A.
Create a Client VPN endpoint in each AWS account. Configure required routing that allows access to internal applications.
B.
Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications.
C.
Create a Client VPN endpoint in the main AWS account. Provision a transit gateway that is connected to each AWS account. Configure required routing that allows access to internal applications.
D.
Create a Client VPN endpoint in the main AWS account. Establish connectivity between the Client VPN endpoint and the AWS Site-to-Site VPN.
AnswerDiscussion
Correct Answer: B
Creating a Client VPN endpoint in the main AWS account is the most cost-effective solution. This setup leverages the existing VPC peering connections to allow access to internal applications across multiple AWS accounts. By centralizing the Client VPN endpoint in the main account, additional endpoints and the costs associated with them are avoided. While transit gateways provide scalability, they introduce unnecessary costs compared to the simpler peering connections which are already established and sufficient for this scenario.
Question 100 of 529
A company is running an application in the AWS Cloud. Recent application metrics show inconsistent response times and a significant increase in error rates. Calls to third-party services are causing the delays. Currently, the application calls third-party services synchronously by directly invoking an AWS Lambda function.
A solutions architect needs to decouple the third-party service calls and ensure that all the calls are eventually completed.
Which solution will meet these requirements?
A.
Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function.
B.
Use an AWS Step Functions state machine to pass events to the Lambda function.
C.
Use an Amazon EventBridge rule to pass events to the Lambda function.
D.
Use an Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function.
AnswerDiscussion
Correct Answer: A
To address inconsistent response times and increased error rates caused by synchronous third-party service calls, implementing Amazon Simple Queue Service (SQS) to store events and then invoke the Lambda function is a robust solution. SQS decouples the application from the third-party services by queuing the requests, which enables the application to continue processing without waiting for responses. This asynchronous approach mitigates response time issues and ensures that all service calls get completed, addressing the problem while improving reliability and scalability.
Question 101 of 529
A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS accounts in AWS Organizations.
The sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The marketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key Management Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in the marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket.
B.
Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sates account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.
C.
Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.
D.
Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight rote, to create a trust relationship with the new IAM role in the sales account.
AnswerDiscussion
Correct Answer: D
To provide secure access to the data in the S3 bucket across AWS accounts with the least operational overhead, creating an IAM role in the sales account and granting it access to the S3 bucket allows the marketing account to assume this role for access. This approach does not require duplicating data or updating the bucket policy or KMS keys, simplifying management and maintaining security. Establishing a trust relationship from the marketing account to the IAM role in the sales account aligns with best practices for cross-account access in AWS.
Question 102 of 529
A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises installation of a Microsoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service. A solutions architect must design a heterogeneous database migration on AWS.
Which solution will meet these requirements?
A.
Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities.
B.
Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.
C.
Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS.
D.
Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.
AnswerDiscussion
Correct Answer: C
For a successful heterogeneous database migration from an on-premises Microsoft SQL Server to an AWS managed service like Amazon RDS for MySQL, the AWS Schema Conversion Tool (SCT) is required to convert the database schema. SCT can translate the schema and database code from Microsoft SQL Server to MySQL seamlessly. After the schema conversion, the AWS Database Migration Service (DMS) can handle the actual data migration, ensuring that the data is correctly and efficiently transferred to the target RDS for MySQL instance. This combination of SCT and DMS provides a streamlined process for migrating between different database engines.
Question 103 of 529
A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the icons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account that members of the design team can access.
After the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the production account. A solutions architect must provide the design team with access to the production account without exposing other parts of the web application to the risk of unwanted changes.
Which combination of steps will meet these requirements? (Choose three.)
A.
In the production account, create a new IAM policy that allows read and write access to the S3 bucket.
B.
In the development account, create a new IAM policy that allows read and write access to the S3 bucket.
C.
In the production account, create a role Attach the new policy to the role. Define the development account as a trusted entity.
D.
In the development account, create a role. Attach the new policy to the role Define the production account as a trusted entity.
E.
In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role In the production account.
F.
In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account.
AnswerDiscussion
Correct Answer: A, C, E
To allow the design team to update static assets in the production S3 bucket while maintaining security, follow these steps: First, in the production account, create a new IAM policy that grants read and write access to the S3 bucket. This ensures the necessary permissions are in place to manage the assets. Next, in the production account, create a role and attach the new policy to this role, defining the development account as a trusted entity. This setup allows the design team from the development account to assume this role and gain access to the S3 bucket. Lastly, in the development account, create a group containing all IAM users of the design team and attach a policy to this group that allows the sts:AssumeRole action on the role in the production account. This enables the design team to assume the role created in the production account, thus accessing and managing the S3 bucket assets securely.
Question 104 of 529
A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's development team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU than expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.
A solutions architect must mitigate the performance issues before the company launches the application to production.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes.
B.
Create a second Elastic Beanstalk environment. Apply the traffic-splitting deployment policy. Specify a percentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes.
C.
Modify the existing environment’s capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes.
D.
Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes.
AnswerDiscussion
Correct Answer: C
To mitigate performance issues with the least operational overhead, the existing environment's capacity configuration should be modified to use a load-balanced environment type. This approach allows the current setup to be adapted without the need to create a new application or environment, utilizing AWS Elastic Beanstalk's built-in capabilities to easily scale out based on CPU utilization. This ensures minimal disruption and makes efficient use of existing resources while meeting the application's performance requirements.
Question 105 of 529
A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed MySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.
Which of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?
A.
Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.
B.
Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.
C.
Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.
D.
Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards.
AnswerDiscussion
Correct Answer: B
To handle the elevated load during the month-end reporting period, migrating the database cluster to Amazon RDS and creating additional read replicas is the optimal solution. Amazon RDS is a managed database service that simplifies database management tasks such as provisioning, setup, patching, and backups. By using read replicas, you can scale out read-heavy workloads during peak periods, ensuring that the database can handle increased read traffic without significant performance degradation. This approach minimizes operational overhead and allows for better handling of the read-intensive requirements typical during month-end reporting.
Question 106 of 529
A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable. but the company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative overhead to maintain the servers.
Which solution will meet these requirements with the LEAST code changes?
A.
Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application.
B.
Migrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration. Use API Gateway to interact with the application.
C.
Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Give the EKS nodes permission to access the ECR image repository. Use Amazon API Gateway to interact with the application.
D.
Migrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use the ALB to interact with the application.
AnswerDiscussion
Correct Answer: A
Migrating the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container and storing container images in Amazon Elastic Container Registry (Amazon ECR) meets the requirements with the least code changes and minimizes the administrative overhead. This solution allows the company to package the existing application code into a container, which can be deployed on ECS Fargate without the need to manage the underlying infrastructure. Additionally, using an Application Load Balancer (ALB) to interact with the application simplifies the process. AWS Fargate is a serverless compute engine that reduces operational overhead, making it ideal for this scenario.
Question 107 of 529
A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes the Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to support failover to another AWS Region.
Which solution will meet these requirements?
A.
Create an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.
B.
Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct traffic to the SQS queue instead of to the Lambda function. Configure the Lambda function to pull messages from the queue for processing.
C.
Deploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 10 direct traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints.
D.
Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.
AnswerDiscussion
Correct Answer: D
To support failover to another AWS Region, you should deploy the Lambda function and an API Gateway endpoint to another region (in this case, us-west-2). This setup ensures that if the primary region (us-east-1) is unavailable, traffic can be routed to the backup region (us-west-2). Amazon Route 53's failover routing policy can be used to automatically route traffic to a healthy endpoint, thereby ensuring availability and reliability across regions.
Question 108 of 529
A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated billing and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has multiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and production.
The HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved Instances (RIs) in its production AWS account. The HR department will install the new application on this account. The HR department wants to make sure that other departments cannot share the RI discounts.
Which solution will meet these requirements?
A.
In the AWS Billing and Cost Management console for the HR department's production account turn off RI sharing.
B.
Remove the HR department's production AWS account from the organization. Add the account 10 the consolidating billing configuration only.
C.
In the AWS Billing and Cost Management console. use the organization’s management account 10 turn off RI Sharing for the HR departments production AWS account.
D.
Create an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments.
AnswerDiscussion
Correct Answer: C
To prevent other departments from sharing the reserved instance (RI) discounts purchased by the HR department's production AWS account, the appropriate action is to use the organization’s management account to turn off RI sharing for that specific account in the AWS Billing and Cost Management console. This setting ensures that the RI benefits are applied only within the HR department’s production account, preventing other accounts within the organization from accessing these discounts.
Question 109 of 529
A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a private subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager is configured, and AWS Systems Manager Agent is running on all the EC2 instances.
The company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being terminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but the logs are inconclusive.
How should the solutions architect gain access to an EC2 instance to troubleshoot the issue?
A.
Suspend the Auto Scaling group’s HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy.
B.
Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy.
C.
Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an unhealthy.
D.
Suspend the Auto Scaling group’s Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy.
AnswerDiscussion
Correct Answer: D
In order to investigate why instances are being marked as unhealthy and terminated, it's crucial to prevent their termination first. Suspending the Auto Scaling group's Terminate process ensures that unhealthy instances are not terminated immediately, allowing the solutions architect to use Session Manager to access the instance and perform detailed troubleshooting. This approach keeps the instances available for investigation without interfering with their health checks.
Question 110 of 529
A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under different OUs in AWS Organizations.
Administrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the ability to automatically update and remediate noncompliant AWS WAF rules in all accounts.
Which solution meets these requirements with the LEAST amount of operational overhead?
A.
Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account.
B.
Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied.
C.
Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers and OUs to manage. Update environment variables as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts. Assume the roles by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts.
D.
Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts.
AnswerDiscussion
Correct Answer: A
The best solution is to use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. AWS Firewall Manager centralizes the configuration and management of firewall rules across multiple accounts, facilitating the process with minimal operational overhead. Using an AWS Systems Manager Parameter Store parameter to store account numbers and OUs allows flexibility in adding or removing accounts or OUs as needed. An Amazon EventBridge rule can detect changes to this parameter and trigger an AWS Lambda function to update the security policy in the Firewall Manager administrative account. This setup ensures automatic updates and remediation of noncompliant AWS WAF rules in all accounts efficiently.
Question 111 of 529
A solutions architect is auditing the security setup or an AWS Lambda function for a company. The Lambda function retrieves, the latest changes from an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the database credentials to the Lambda function.
The Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with AWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised, the company needs a solution that minimizes the impact of the compromise.
What should the solutions architect recommend to meet these requirements?
A.
Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.
B.
Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers.
C.
Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.
D.
Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers.
AnswerDiscussion
Correct Answer: A
The best approach to ensure both secure access to the Aurora database and to prevent data from traveling across the Internet is to enable IAM database authentication and use a VPC endpoint for S3. By enabling IAM database authentication, the Lambda function can access the database without requiring embedded credentials, thus minimizing the impact of potential credential compromise. Additionally, deploying a gateway VPC endpoint for Amazon S3 ensures that all communication between the Lambda function and S3 remains within the AWS network, securing the traffic and preventing it from traversing the Internet.
Question 112 of 529
A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is reviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected Framework.
While reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several large instance types account for a high proportion of the costs. The solutions architect finds out that the company’s developers are launching new Amazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types.
The solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.
Which solution will meet these requirements?
A.
Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to an event to run each time a new EC2 instance is launched.
B.
In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the developers’ IAM accounts.
C.
Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers
D.
Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image.
AnswerDiscussion
Correct Answer: C
To control which instance types the developers can launch, creating a new IAM policy is the appropriate solution. This policy will specify the permitted instance types and will be attached to an IAM group containing the developers’ IAM accounts. This ensures that the developers are only able to launch instances of the allowed types, thereby preventing the creation of larger, more costly instances.
Question 113 of 529
A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the same organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team responsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.
Which actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.)
A.
Create an AWS Config rule in each account to find resources with missing tags.
B.
Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.
C.
Use Amazon Inspector in the organization to find resources with missing tags.
D.
Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing.
E.
Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.
F.
Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag.
AnswerDiscussion
Correct Answer: A, B, E
To ensure that all Amazon EC2 instances across multiple AWS accounts have the required Project tag for cost allocation, creating an AWS Config rule in each account will help identify resources with missing tags. Then, using a Service Control Policy (SCP) within the AWS organization can enforce a policy to prevent the creation of EC2 instances that lack the required tag, ensuring that future instances are correctly tagged. Lastly, an AWS Config aggregator can be set up to gather compliance data regarding tags across all accounts, providing a centralized view to manage and rectify any tagging issues.
Question 114 of 529
A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage.
The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:
• Managed AWS services to minimize operational complexity.
• A buffer that automatically scales to match the throughput of data and requires no ongoing administration.
• A visualization tool to create dashboards to observe events in near-real time.
• Support for semi-structured JSON data and dynamic schemas.
Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)
A.
Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.
B.
Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.
C.
Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
D.
Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.
E.
Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
AnswerDiscussion
Correct Answer: A, D
To design a solution that adheres to the specified requirements, Amazon Kinesis Data Firehose and Amazon Elasticsearch Service (Amazon ES) are the suitable components. Amazon Kinesis Data Firehose is a fully managed service capable of automatically scaling to match the throughput of data and requires no ongoing administration, which makes it suitable for buffering events. Furthermore, it supports the use of AWS Lambda functions for processing and transforming data, accommodating semi-structured JSON data and dynamic schemas. Amazon Elasticsearch Service, on the other hand, is a managed service that can effectively receive events and provides Kibana for creating near-real-time visualizations and dashboards. This combination ensures a scalable, managed solution with minimal operational complexity while supporting JSON data and providing robust visualization capabilities.
Question 115 of 529
A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private subnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company’s applications read from and write to Amazon Kinesis Data Streams. Most of the workloads run in private subnets.
A solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications. The solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category.
What should the solutions architect do to meet these requirements?
A.
Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs.
B.
Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint.
C.
Enable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic.
D.
Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications.
AnswerDiscussion
Correct Answer: B
Adding an interface VPC endpoint for Kinesis Data Streams to the VPC allows applications in private subnets to directly access Kinesis Data Streams without routing traffic through the NAT gateway, which significantly reduces data transfer costs. This is because data transferred through VPC endpoints is cheaper compared to data transferred through NAT gateways. It is not necessary to specify that the VPC endpoint policy allows traffic from the applications as the primary concern is reducing NAT gateway costs.
Question 116 of 529
A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to route network traffic from its on-premises infrastructure into VPCs in either of those Regions. The company also needs to support traffic that is routed directly between VPCs in those Regions. No single points of failure can exist on the network.
The company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that is configured to route all inter-VPC traffic within that Region.
Which solution will meet these requirements?
A.
Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.
B.
Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-8 connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross-Region routing.
C.
Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Configure the Direct Connect gateway to route traffic between the transit gateways.
D.
Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.
AnswerDiscussion
Correct Answer: D
To meet the requirements of routing network traffic between the on-premises infrastructure and the VPCs in both the eu-west-1 and us-east-1 Regions, including traffic routed directly between VPCs in those Regions without any single points of failure, the correct solution involves setting up transit VIFs. Specifically, a transit VIF from both Direct Connect connections, DX-A and DX-B, into a single Direct Connect gateway provides high availability. By associating both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway and peering the transit gateways with each other, cross-region routing is supported, fulfilling the requirement for high availability and resilience.
Question 117 of 529
A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new IAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user. The company has a multi-Region AWS CloudTrail trail in the AWS account.
Which combination of steps will meet these requirements? (Choose three.)
A.
Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.
B.
Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.
C.
Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access.
D.
Invoke an AWS Step Functions state machine to remove access.
E.
Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.
F.
Use Amazon Pinpoint to notify the security team.
AnswerDiscussion
Correct Answer: A, D, E
To meet the requirements of having the security team approve the creation of all new IAM users while removing their access automatically and notifying the security team, the following steps can be employed: Firstly, create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect the CreateUser event. This EventBridge rule (A) is necessary to monitor when a new IAM user is created. Secondly, use Step Functions to automate the process of removing access. AWS Step Functions (D) can be utilized to orchestrate the process of removing the user's access automatically upon creation. Finally, employ Amazon Simple Notification Service (SNS) (E) to send a notification to the security team for their approval. SNS is well-suited for sending notifications directly to the team, fulfilling the requirement of notification. Pinpoint is incorrect because it is used for customer engagement rather than notifications for internal security teams. ECS with Fargate adds unnecessary complexity and overhead for this solution compared to Step Functions.
Question 118 of 529
A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and applications. The company also wants to keep the traffic on a private network. Multi-factor authentication (MFA) is required at login, and specific roles are assigned to user groups.
The company must create separate accounts for development. staging, production, and shared network. The production account and the shared network account must have connectivity to all accounts. The development account and the staging account must have access only to each other.
Which combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)
A.
Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations.
B.
Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login.
C.
Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables.
D.
Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts.
E.
Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA login.
F.
Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognoto user pools and Identity pools to manage access to accounts and between accounts.
AnswerDiscussion
Correct Answer: A, C, D
To meet the requirements of a multi-account structure with centrally managed access and private network traffic, the following steps should be taken. First, deploy a landing zone environment using AWS Control Tower, as this helps in setting up a secure and well-architected multi-account AWS environment. Then, create transit gateways and transit gateway VPC attachments in each account to ensure proper routing and private connectivity between the accounts. Lastly, set up and enable AWS IAM Identity Center (AWS Single Sign-On) to manage user access with multi-factor authentication and assign specific roles to user groups, ensuring secure and centralized access management.
Question 119 of 529
A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production. All the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases. The databases are between 500 GB and 800 GB in size.
The development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7 days a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or production as the key.
What should a solutions architect do to reduce costs with the LEAST operational effort?
A.
Create an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or slops instances based on me tag, day, and time.
B.
Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that starts instances based on the tag.
C.
Create an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that terminates, instances based on the lag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that restores the instances from their last backup based on the tag.
D.
Create an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores instances from their last backup based on the tag. day, and time.
AnswerDiscussion
Correct Answer: B
To reduce costs with the least operational effort, the solution should focus on managing the instances based on the usage pattern of the different environments. Development and testing environments are only needed during business hours on business days, whereas the production environment operates continuously. By creating an Amazon EventBridge rule that runs every business day in the evening to stop instances and another rule that runs every business day in the morning to start instances based on the tag, the company can ensure that the instances for development and testing are only running when needed. This approach minimizes unnecessary costs by stopping instances when they are not being used without the complexity of creating and restoring instances. Additionally, it preserves the state of the instances, ensuring data integrity and quick start-up times when needed. Therefore, creating separate EventBridge rules to start and stop instances twice a day is the most effective solution for reducing costs with minimal operational effort.
Question 120 of 529
A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed an Amazon API Gateway REST API with AWS Lambda integration in multiple AWS Regions and in the same production account.
The company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of API calls per second. The premium tier offers up to 3,000 calls per second, and customers are identified by a unique API key. Several premium tier customers in various Regions report that they receive error responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logs indicate that the Lambda function is never invoked.
What could be the cause of the error messages for these customers?
A.
The Lambda function reached its concurrency limit.
B.
The Lambda function its Region limit for concurrency.
C.
The company reached its API Gateway account limit for calls per second.
D.
The company reached its API Gateway default per-method limit for calls per second.
AnswerDiscussion
Correct Answer: C
The error message 429 Too Many Requests from the API Gateway indicates that the account has exceeded its limit for the number of API calls per second. Amazon API Gateway enforces default account-level throttling limits on the rate of requests. Specifically, it has a limit of 10,000 requests per second per account per region. Since the premium tier allows up to 3,000 calls per second and multiple premium customers are using the service in different regions, it is likely that the aggregate number of requests is exceeding the 10,000 requests per second limit, resulting in the 429 errors. This also explains why the Lambda function is not invoked, as the requests are being throttled at the API Gateway level. Therefore, the company reached its API Gateway account limit for calls per second.
Question 121 of 529
A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor the inbound traffic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available from its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology.
The company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a dedicated VPC. The company needs to use the security tool to inspect all packets that come in and out of the VPC. This inspection must occur in real time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available within an AWS Region.
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)
A.
Deploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC
B.
Deploy the web application behind a Network Load Balancer
C.
Deploy an Application Load Balancer in front of the security tool instances
D.
Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool
E.
Provision a transit gateway to facilitate communication between VPCs.
AnswerDiscussion
Correct Answer: D, E
To meet the requirements of integrating a 15-year-old, non-cloud native security tool with AWS for traffic inspection in an Auto Scaling group within a dedicated VPC, you should use a Gateway Load Balancer (GWLB). The GWLB facilitates redirecting traffic seamlessly to your security tool instances without affecting application performance. Additionally, provisioning a transit gateway will help facilitate communication between VPCs, which is important for ensuring high availability and proper routing of traffic through the security inspection tool across multiple VPCs within the AWS Region.
Question 122 of 529
A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the vendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique format. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis.
The company needs to design a new data analysis solution that can deliver faster and optimize costs.
Which solution will meet these requirements?
A.
Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon. S3 Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis.
B.
Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a relational format. Save the parsed information to Amazon Redshlft for analysis.
C.
Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis.
D.
Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon Redshift to perform global analysis.
AnswerDiscussion
Correct Answer: A
The most effective solution leverages AWS IoT Core to connect the IoT sensors. This setup allows for real-time data collection and immediate processing. By invoking an AWS Lambda function to parse the data and save it as a .csv file to Amazon S3, the solution ensures that data is handled efficiently. AWS Glue can then be used to catalog the files, facilitating seamless querying and analysis with Amazon Athena and Amazon QuickSight. This architecture optimizes costs and delivers fast data analysis by utilizing scalable and serverless AWS services.
Question 123 of 529
A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes networking and security strategies. The company has set up an AWS Direct Connect connection in a central network account.
The company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the resources on AWS seamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to the internet through its on-premises data center.
Which combination of steps will meet these requirements? (Choose three.)
A.
Create a Direct Connect gateway in the central account. In each of the accounts, create an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway.
B.
Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF.
C.
Provision an internet gateway. Attach the internet gateway to subnets. Allow internet traffic through the gateway.
D.
Share the transit gateway with other accounts. Attach VPCs to the transit gateway.
E.
Provision VPC peering as necessary.
F.
Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center.
AnswerDiscussion
Correct Answer: B, D, F
To meet the company's requirements of migrating and modernizing applications quickly, while ensuring seamless access to AWS resources and communication between VPCs, and routing cloud resources to the internet through the on-premises data center, the following steps can be taken: Create a Direct Connect gateway and a transit gateway in the central network account. This allows connecting the on-premises data center to AWS resources. Share the transit gateway with other accounts and attach VPCs to it, enabling communication between all the VPCs. Provision only private subnets and configure necessary routes on the transit gateway and customer gateway to route outbound internet traffic from AWS through NAT services in the data center.
Question 124 of 529
A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved Instances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances in their own respective AWS accounts autonomously.
A solutions architect needs to enforce the new process in the most secure way possible.
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)
A.
Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.
B.
Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.
C.
In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.
D.
Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization.
E.
Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature.
AnswerDiscussion
Correct Answer: A, D
To enforce the new centralized purchase process in the most secure way, the solutions architect should ensure all AWS accounts are part of an organization in AWS Organizations with all features enabled. This centralizes management and control across all accounts. Then, creating an SCP (Service Control Policy) that denies the `ec2:PurchaseReservedInstancesOffering` and `ec2:ModifyReservedInstances` actions and attaching it to each organizational unit ensures that all business units adhere to the new process, effectively preventing autonomous actions by individual units.
Question 125 of 529
A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-AZ mode.
A recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the outage time to less than 20 seconds.
Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)
A.
Use Amazon ElastiCache for Memcached in front of the database
B.
Use Amazon ElastiCache for Redis in front of the database
C.
Use RDS Proxy in front of the database.
D.
Migrate the database to Amazon Aurora MySQL.
E.
Create an Amazon Aurora Replica.
F.
Create an RDS for MySQL read replica
AnswerDiscussion
Correct Answer: C, D, E
To reduce outage time to less than 20 seconds for a critical application using Amazon RDS for MySQL in Multi-AZ mode, the most effective approach involves: 1) Using RDS Proxy, which helps by managing and pooling database connections, speeding up the failover process by quickly rerouting traffic to the healthy instance; 2) Migrating the database to Amazon Aurora MySQL, which generally has faster failover capabilities compared to standard RDS instances; and 3) Creating an Amazon Aurora Replica, which can take over as the primary instance much more quickly than traditional MySQL read replicas due to its design for high availability and low-latency failover.
Question 126 of 529
An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account.
What is the MOST secure way to allow org1 to access resources in org2?
A.
The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.
B.
The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks.
C.
The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role’s Amazon Resource Name (ARN) when requesting access to perform the required tasks.
D.
The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role’s Amazon Resource Name (ARN), including the external ID in the IAM role’s trust policy, when requesting access to perform the required tasks.
AnswerDiscussion
Correct Answer: D
The most secure way to allow an AWS partner company access to resources in a customer account within a separate AWS organization is to use an IAM role with the appropriate permissions. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role’s Amazon Resource Name (ARN), including the external ID in the IAM role’s trust policy, when requesting access to perform the required tasks. This approach ensures that the partner company can only access the resources they need and only from the specific customer account, minimizing the risk of security breaches.
Question 127 of 529
A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a public registry. The image can run in as many containers as required to generate the route map.
The company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the hubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area.
The company needs the ability to allocate resources cost-effectively based on the number of running containers.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning application in pods by using the --tags option to assign a custom tag to the pod.
B.
Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning application. Use the AWS CLI tag-resource API call to assign a custom tag to the pod.
C.
Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch the planning application by using the --tags option to assign a custom tag to the task.
D.
Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task.
AnswerDiscussion
Correct Answer: D
Using Amazon Elastic Container Service (Amazon ECS) with AWS Fargate is the optimal solution for this scenario because it minimizes operational overhead. AWS Fargate allows you to run containers without managing the underlying infrastructure, making it easier and more cost-effective to scale based on the number of running containers. Moreover, the AWS CLI run-task command enables you to launch the planning application with managed tags effectively. This setup simplifies resource management and ensures a seamless experience in handling multiple Docker containers with a custom configuration for each section.
Question 128 of 529
A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of Amazon EC2 instances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account, a shared services VPC is located in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS CloudFormation to attempt to peer the application VPC with the shared services VPC, an error message indicates a peering failure.
Which factors could cause this error? (Choose two.)
A.
The IPv4 CIDR ranges of the two VPCs overlap
B.
The VPCs are not in the same Region
C.
One or both accounts do not have access to an Internet gateway
D.
One of the VPCs was not shared through AWS Resource Access Manager
E.
The IAM role in the peer accepter account does not have the correct permissions
AnswerDiscussion
Correct Answer: A, E
The error peering the VPCs could be caused by overlapping IPv4 CIDR ranges and incorrect IAM permissions. The application VPC uses a CIDR block of 10.10.0.0/16, and the shared services VPC uses a CIDR block of 10.10.10.0/24. These ranges overlap since 10.10.10.0/24 falls within the 10.10.0.0/16 range, which can cause routing conflicts. Additionally, ensuring the IAM role in the peer accepter account has the correct permissions is crucial for the VPC peering connection to be successfully established.
Question 129 of 529
An external audit of a company’s serverless application reveals IAM policies that grant too many permissions. These policies are attached to the company's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to Amazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function needs to complete its task.
A solutions architect must determine which permissions each Lambda function needs.
What should the solutions architect do to meet this requirement with the LEAST amount of effort?
A.
Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and resources for each Lambda function. Create new IAM access policies for each Lambda function. Review the new policies to ensure that they meet the company's business requirements.
B.
Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements.
C.
Turn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda execution role, and create a summary report. Review the report. Create IAM access policies that provide more restrictive permissions for each Lambda function.
D.
Turn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resources used by each execution role. Create a new IAM access policy for each role. Export the generated roles to an S3 bucket. Review the generated policies to ensure that they meet the company’s business requirements.
AnswerDiscussion
Correct Answer: B
To determine the minimum permissions needed for each Lambda function with minimal effort, you should leverage AWS services that automatically analyze and generate policies. By turning on AWS CloudTrail logging, you can track and record API calls made by Lambda functions. AWS Identity and Access Management (IAM) Access Analyzer can then use this log data to automatically generate policies based on actual usage, ensuring that each function has only the permissions it needs. This method reduces the manual effort involved in analyzing and creating policies, making it the most efficient solution.
Question 130 of 529
A solutions architect must analyze a company’s Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine whether the company is using resources efficiently. The company is running several large, high-memory EC2 instances to host database clusters that are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and the company has not identified a pattern.
The solutions architect must analyze the environment and take action based on the findings.
Which solution meets these requirements MOST cost-effectively?
A.
Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.
B.
Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.
C.
Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed.
D.
Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed.
AnswerDiscussion
Correct Answer: C
The most cost-effective solution to analyze the company’s Amazon EC2 instances and Amazon EBS volumes is to install the Amazon CloudWatch agent on each of the EC2 instances, turn on AWS Compute Optimizer, and let it run for at least 12 hours. AWS Compute Optimizer leverages machine learning algorithms to generate recommendations for right-sizing the EC2 instances based on resource usage. This approach is streamlined, efficient, and allows for optimized resource utilization without the need for extensive manual monitoring or support plans, making it the most cost-effective option.
Question 131 of 529
A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts.
In an AWS application account, the company’s application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's database administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed m the application account.
The application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the secrets.
Which solution will meet these requirements?
A.
Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.
B.
In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets
C.
In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.
D.
In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.
AnswerDiscussion
Correct Answer: B
To meet the requirement of eliminating manual sharing of secrets while allowing database administrators to access the database, the solution involves creating IAM roles that facilitate secure access across accounts. In the application account, an IAM role named DBA-Secret is created with permissions to access the secrets. In the DBA account, an IAM role named DBA-Admin is created with permissions to assume the DBA-Secret role in the application account. By attaching the DBA-Admin role to the EC2 instance, administrators can access the secrets securely without manual sharing, ensuring that cross-account access is properly managed.
Question 132 of 529
A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs: Research and DataOps.
Because of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region. Additionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types.
A solutions architect must implement a solution that applies these restrictions. The solution must maximize operational efficiency and must minimize ongoing maintenance.
Which combination of steps will meet these requirements? (Choose two.)
A.
Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict access to specific instance type.
B.
Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1.
C.
Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.
D.
Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU, the DataOps OU, and the Research OU.
E.
Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.
AnswerDiscussion
Correct Answer: C, E
To meet the regulatory requirements and ensure that the solution maximizes operational efficiency and minimizes ongoing maintenance, the following steps should be taken. First, create a Service Control Policy (SCP) using the aws:RequestedRegion condition key to restrict access to all AWS Regions except for ap-northeast-1 and apply this SCP to the root Organizational Unit (OU). This ensures that all resources deployed across the organization will reside in the ap-northeast-1 Region. Second, create another SCP using the ec2:InstanceType condition key to restrict access to specific EC2 instance types and apply this policy to the DataOps OU. This ensures that EC2 instances in the DataOps OU will use only the predefined list of instance types. These steps effectively enforce the required restrictions while minimizing maintenance.
Question 133 of 529
A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites. The company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an Amazon S3 bucket.
The company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the existing Region. Results must be written to the existing S3 bucket in the current Region.
Which combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)
A.
Deploy the SQS queue with the Lambda function to other Regions.
B.
Subscribe the SNS topic in each Region to the SQS queue.
C.
Subscribe the SQS queue in each Region to the SNS topic.
D.
Configure the SQS queue to publish URLs to SNS topics in each Region.
E.
Deploy the SNS topic and the Lambda function to other Regions.
AnswerDiscussion
Correct Answer: A, C
To achieve multi-Region deployment, two key changes must be made. Firstly, deploying the SQS queue with the Lambda function to other Regions is essential. This allows the processing of URLs to occur in other Regions, enabling comparison of site localization differences. Secondly, subscribing the SQS queue in each Region to the SNS topic ensures that URLs published from the existing Region can be processed in all other Regions. This integration meets the requirements of processing URLs in multiple Regions while publishing the results to a single S3 bucket in the current Region.
Question 134 of 529
A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.
Which strategy should the solutions architect use?
A.
Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.
B.
Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.
C.
Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.
D.
Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours.
AnswerDiscussion
Correct Answer: C
For a highly CPU-intensive, single-threaded, and stateless ETL application that runs every 4 hours for up to 20 minutes, AWS Fargate is an ideal choice. AWS Fargate is suited for containerized applications, eliminating the need to manage the underlying server infrastructure. It can efficiently handle CPU-intensive tasks within a containerized environment. Amazon EventBridge (formerly Amazon CloudWatch Events) is well-suited to schedule tasks at fixed intervals, such as every 4 hours. This combination ensures a scalable, cost-efficient, and managed solution where the application runs in a serverless mode, incurring costs only when the task is active.
Question 135 of 529
A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week after launch. Currently, the game consists of the following components deployed in a single AWS Region:
• Amazon S3 bucket that stores game assets
• Amazon DynamoDB table that stores player scores
A solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.
What should the solutions architect do to meet these requirements?
A.
Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.
B.
Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC).
C.
Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region.
D.
Create another S3 bucket in the sine Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.
AnswerDiscussion
Correct Answer: A
To meet the requirements of reducing latency, improving reliability, and requiring the least effort, the best solution is to create an Amazon CloudFront distribution to serve assets from the S3 bucket. CloudFront will cache content at edge locations globally, reducing latency for users. Additionally, configuring S3 Cross-Region Replication will ensure the game assets are stored in multiple regions, improving reliability. For the DynamoDB component, using DynamoDB global tables and setting up a new table in a new region as a replica target leverages built-in functionality for multi-region data replication, reducing complexity and configuration effort. This setup addresses the key objectives effectively without adding unnecessary complexities.
Question 136 of 529
A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a NoSQL MongoDB database to store subscriber data.
The company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company cannot make changes to the application.
Which solution will meet these requirements?
A.
Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.
B.
Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application.
C.
Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.
D.
Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.
AnswerDiscussion
Correct Answer: C
The company needs to migrate its Java backend and MongoDB database to AWS without making any changes to the application. Amazon DocumentDB (with MongoDB compatibility) is specifically designed to be compatible with MongoDB, making it a suitable choice for the subscriber data without necessitating any application changes. By deploying Amazon DocumentDB with appropriately sized instances in multiple Availability Zones, the company ensures high availability. For the Java backend application, deploying Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones will also provide high availability and scalability. This setup meets all the requirements specified by the company.
Question 137 of 529
A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS account to securely store images and media files that are used as content for the company’s marketing campaigns. The creative team wants to share the S3 bucket with the strategy team so that the strategy team can view the objects.
A solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a custom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when users from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.
The solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only the minimum permissions that they need.
Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)
A.
Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.
B.
Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.
C.
Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.
D.
Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user.
E.
Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role.
F.
Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.
AnswerDiscussion
Correct Answer: A, C, F
To allow the Strategy team to access the objects in the S3 bucket, the solutions architect should take the following steps: 1) Create a bucket policy that includes read permissions for the S3 bucket and set the principal of the bucket policy to the account ID of the Strategy account. This ensures that the Strategy account has the necessary access to the S3 bucket. 2) Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role. This allows the Strategy team to decrypt the objects in the S3 bucket. 3) Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and decrypt permissions for the custom KMS key, which gives the Strategy team the necessary permissions to read and decrypt the objects. These actions provide the minimum necessary permissions without granting excessive access.
Question 138 of 529
A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days.
The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day.
Which solution meets these requirements?
A.
Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.
B.
Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.
C.
Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data.
D.
Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data.
AnswerDiscussion
Correct Answer: C
The best solution leverages AWS DataSync to transfer the sequencing data to Amazon S3. DataSync is designed for fast and secure data transfer from on-premises storage into AWS. Using S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow provides a scalable and automated data processing pipeline. Docker images stored in Amazon Elastic Container Registry (Amazon ECR) can be triggered by AWS Batch to run containers and process the sequencing data, efficiently handling the required compute capacity and job scheduling. This approach ensures seamless integration, scalability, and reduced processing times.
Question 139 of 529
A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones.
A solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in time.
Which solution will meet these requirements with the LEAST management overhead?
A.
Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share.
B.
Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system.
C.
Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain.
D.
Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three Instances. Perform a seamless domain join to join the instance to the AD domain.
AnswerDiscussion
Correct Answer: C
To achieve high availability and fault tolerance while ensuring the same content on all running instances in a Windows environment, the optimal solution is to use Amazon FSx for Windows File Server. This service is specifically designed for Windows-based workloads and supports Windows Access Control Lists (ACLs), which are essential for controlling access to file contents. It also seamlessly integrates with Active Directory for domain join operations. Using an Auto Scaling group across multiple Availability Zones ensures that the system is highly available. Therefore, creating an FSx for Windows File Server, configuring an Auto Scaling group, using user data for application installation, and performing a seamless domain join meets all the given requirements with minimal management overhead.
Question 140 of 529
A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a standalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email template for acknowledgement email messages that populate customer data before the application sends the email message to the customer.
The company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.
Which solution will meet these requirements MOST cost-effectively?
A.
Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.
B.
Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.
C.
Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameters. Use the AWS Marketplace SMTP server to send the email message.
D.
Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination.
AnswerDiscussion
Correct Answer: D
The most cost-effective solution to migrate the messaging functionality to the AWS Cloud and minimize operational overhead is to use Amazon Simple Email Service (Amazon SES) to send email messages. By storing the email template on Amazon SES with parameters for the customer data and using an AWS Lambda function to call the SendTemplatedEmail API operation, passing in the customer data to replace the parameters and the email destination, the company can leverage a fully managed service. This eliminates the need to set up and manage an SMTP server on EC2 instances, which can be costly and require more maintenance.
Question 141 of 529
A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a video Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.
The company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The company has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the development team when there are messages in the dead-letter queue.
Several times during the day. the development team receives notification that messages are in the dead-letter queue and that videos have not been processed property. An investigation finds no errors m the application logs.
How can the company solve this problem?
A.
Turn on termination protection tor the EC2 Instances
B.
Update the visibility timeout for the SQS queue to 3 hours
C.
Configure scale-in protection for the instances during processing
D.
Update the redrive policy and set maxReceiveCount to 0.
AnswerDiscussion
Correct Answer: C
The company can solve the problem by configuring scale-in protection for the instances during processing. This will ensure that the instances are not terminated while they are processing videos. Terminating instances in the middle of processing can result in incomplete work and cause the messages to move to the dead-letter queue. By protecting the instances from termination during processing, the company can prevent these disruptions and ensure that videos are processed properly. Options such as turning on termination protection for all instances or changing the redrive policy will not address the root cause of the issue, which is the premature termination of instances during processing.
Question 142 of 529
A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API Gateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.
The solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an authenticated user
Which solution will meet these requirements with the LEAST amount of effort?
A.
Create an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to call the API from the VPC.
B.
Remove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in the hosted zone. Update the API in API Gateway with the CNAME record. Use the CNAME record to call the API from the VPC.
C.
Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPCreate a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC.
D.
Deploy the Lambda functions inside the VPC Provision an EC2 instance, and install an Apache server. From the Apache server, call the Lambda functions. Use the internal CNAME record of the EC2 instance to call the API from the VPC.
AnswerDiscussion
Correct Answer: C
To make a set of APIs accessible only from a VPC with the least amount of effort, the best approach is to update the API endpoint from Regional to private in API Gateway. Then, create an interface VPC endpoint in the VPC and attach a resource policy to the API. This ensures the APIs are accessible only from within the VPC, leveraging VPC endpoints without needing additional infrastructure like load balancers or EC2 instances. This method is most efficient and straightforward for restricting access while maintaining the necessary authentication and security.
Question 143 of 529
A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.
The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time.
Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)
A.
Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.
B.
Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.
C.
Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.
D.
Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.
E.
Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.
AnswerDiscussion
Correct Answer: B, D
To resolve the performance issues for users in us-east-1, creating a new S3 bucket in us-east-1 and configuring cross-Region replication to synchronize with the S3 bucket in eu-west-1 helps by delivering the content from a geographically closer location, thus reducing latency. Additionally, using Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1 ensures that the requests are directed to the appropriate bucket based on the user's region, further enhancing the performance by minimizing the retrieval time.
Question 144 of 529
A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis indicates that the issue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as the profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.
The solutions architect discovers that the file system has reached Its maximum capacity. The solutions architect must ensure that users can regain access. The solution also must prevent the problem from occurring again.
Which solution will meet these requirements?
A.
Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system.
B.
Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required.
C.
Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity as required.
D.
Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection for 50% of the users to use the new file system.
AnswerDiscussion
Correct Answer: B
To address the issue of the file system reaching its maximum capacity and to prevent it from occurring again, the best solution is to increase the file system's capacity and set up monitoring and automatic scaling. By using the update-file-system command, the storage capacity can be increased. Implementing an Amazon CloudWatch metric to monitor free space allows for proactive management, and using Amazon EventBridge to trigger an AWS Lambda function to automatically increase the capacity as required ensures that the file system does not reach full capacity again. This combination addresses both the immediate need for more storage and provides a robust solution to prevent future issues.
Question 145 of 529
An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery. Confirmation includes the recipient’s signature or a photo of the package with the recipient. The driver’s handheld device uploads signatures and photos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file name matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information. The file is then placed in Amazon S3 for archiving.
As the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped connections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports that files are not always in the archive and that the central system is not always updated.
A solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems are always updated. The handheld devices cannot be modified, so the company cannot deploy a new application.
Which solution will meet these requirements?
A.
Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure the Auto Scaling group to have a minimum of three instances.
B.
Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the new path for file processing.
C.
Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.
D.
Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.
AnswerDiscussion
Correct Answer: C
Since handheld devices cannot be modified, the solution should avoid dependencies on a single EC2 instance, which is currently causing scalability issues. Using AWS Transfer Family to create an FTP server that places the files in Amazon S3 leverages S3's scalability and durability. An S3 event notification through Amazon Simple Notification Service (SNS) can then invoke an AWS Lambda function to add metadata and update the delivery system. This ensures that the archive always receives the files and that systems are always updated without modifying the handheld devices or relying on a single point of failure.
Question 146 of 529
A company is running an application in the AWS Cloud. The application runs on containers m an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost.
Which solution will meet these requirements with the LEAST amount of operational overhead?
A.
Provision an Aurora Replica in a different Region.
B.
Set up AWS DataSync for continuous replication of the data to a different Region.
C.
Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.
D.
Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes.
AnswerDiscussion
Correct Answer: A
Provisioning an Aurora Replica in a different Region ensures that the data is continuously and asynchronously replicated to the replica. This setup allows for the promotion of the replica to the primary database with zero data loss in the event of a failure. Aurora Replicas are designed to handle this scenario with minimal operational overhead, as Aurora manages the replication and failover process automatically, providing a managed solution that meets the requirement of no data loss and cross-region recovery.
Question 147 of 529
A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.
Which solutions will meet these requirements?
A.
Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Invoke a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.
B.
Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.
C.
Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.
D.
Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster.
AnswerDiscussion
Correct Answer: C
The most appropriate solution involves using AWS Glue, a managed ETL service that is designed to handle extract, transform, and load tasks. This solution involves invoking an AWS Glue ETL job that can transform the data according to the requirements, including masking sensitive information, removing and merging fields, and converting records into JSON format. AWS Glue is easily scalable and can be expanded to handle additional data feeds in the future. Using AWS Glue for these tasks ensures that the company can efficiently manage data transformation with minimal operational overhead.
Question 148 of 529
A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.
Which solution will achieve the company's goal with the LEAST operational overhead?
A.
Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail over the workload in the case of a failure event.
B.
Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time.
C.
Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI.
D.
Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the MySQL database on the new volumes. Take regular snapshots. Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a Volume Gateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes on the EC2 instances in the case of a failure event.
AnswerDiscussion
Correct Answer: B
The most suitable option for achieving business continuity with the least operational overhead involves using AWS Elastic Disaster Recovery. This allows the company to install the AWS Replication Agent on the source servers, including MySQL servers, and initialize AWS Elastic Disaster Recovery in the relevant AWS region. The solution requires defining launch settings and maintaining regular synchronization, which minimizes the need for complex setups and ongoing management. Other options involve additional and continuous manual efforts, such as setting up and managing AWS DMS tasks or configuring and maintaining AWS Storage Gateway, which introduces more operational overhead.
Question 149 of 529
A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the company's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The solution must comply with AWS security best practices.
Which solution will meet these requirements?
A.
In the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS account. Assign a unique external ID to the resource policy.
B.
In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.
C.
In the company's AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM user. Share the access keys with the auditors.
D.
In the company's AWS account, create an IAM group that has the required permissions. Create an IAM user in the company's account for each auditor. Add the IAM users to the IAM group.
AnswerDiscussion
Correct Answer: B
To meet the requirement of providing the external auditors with secure, read-only access to the company's AWS account while complying with AWS security best practices, the solution involves creating an IAM role that trusts the auditors' AWS account. This role should have an IAM policy with the specific permissions needed for read-only access. The role's trust policy must include a unique external ID for added security. This approach ensures that the auditors can assume the role and access the resources without sharing access keys or creating individual IAM users for each auditor, which enhances security and manageability.
Question 150 of 529
A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB table to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The new solution must ensure high availability for the trading platform.
Which solution will meet these requirements with the LEAST latency?
A.
Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.
B.
Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.
C.
Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX.
D.
Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.
AnswerDiscussion
Correct Answer: B
To improve the performance of the latency-sensitive trading platform while ensuring high availability, the best solution is to create a three-node DynamoDB Accelerator (DAX) cluster. The application should be configured to read data using DAX and write data directly to the DynamoDB table. This setup leverages the benefits of DAX for read performance without the latency overhead of write operations through DAX. Additionally, a three-node cluster provides high availability, as recommended for production environments.
Question 151 of 529
A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind another ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak usage of the application.
The application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives minimal traffic during the rest of the day.
A solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability.
Which combination of steps will meet these requirements? (Choose two.)
A.
Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances.
B.
Move the application frontend to a static website that is hosted on Amazon S3.
C.
Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes.
D.
Change all the backend EC2 instances to Spot Instances.
E.
Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances.
AnswerDiscussion
Correct Answer: B, E
To optimize the infrastructure cost of the application without negatively affecting the application availability, the best approach involves moving the application frontend to a static website hosted on Amazon S3 and deploying the backend Python application on general purpose burstable EC2 instances. Hosting the static frontend on Amazon S3 is more cost-effective and scalable compared to running it on EC2 instances. For the backend, general purpose burstable instances offer a cost-effective solution since they handle variable workloads efficiently by accumulating credits during low usage and utilizing them during peaks, thus aligning with the application's usage pattern.
Question 152 of 529
A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is developing new application features to run on Amazon EKS with AWS Fargate.
The platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.
Which solution will provide the MOST cost-effective setup for the platform?
A.
Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year.
B.
Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale out database read replicas during peaks.
C.
Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.
D.
Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.
AnswerDiscussion
Correct Answer: D
The most cost-effective setup involves using Compute Savings Plans for the predicted base load of the EKS cluster as these plans provide flexibility and apply to both EC2 and AWS Fargate usage. Scaling the cluster with Spot Instances is a cost-effective way to handle peaks since Spot Instances are considerably less expensive than On-Demand Instances, though they come with a risk of interruption, their use in a non-stateful, scalable cluster can be managed effectively. For the database, purchasing 1-year All Upfront Reserved Instances for the predicted base load ensures cost savings over time. Temporarily scaling up the DB instance manually during peaks allows for additional capacity during high demand periods without incurring unnecessary permanent costs.
Question 153 of 529
A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon CloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured with an alternate domain name that visitors use when they access the application.
Each week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the company wants visitors to receive an informational message instead of a CloudFront error message.
A solutions architect creates an Amazon S3 bucket as the first step in the process.
Which combination of steps should the solutions architect take next to meet the requirements? (Choose three.)
A.
Upload static informational content to the S3 bucket.
B.
Create a new CloudFront distribution. Set the S3 bucket as the origin.
C.
Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI).
D.
During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete.
E.
During the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \ Set the precedence to 0. Delete the cache behavior when the maintenance is complete.
F.
During the weekly maintenance, configure Elastic Beanstalk to serve traffic from the S3 bucket.
AnswerDiscussion
Correct Answer: A, C, D
To meet the requirements, follow these steps: First, upload static informational content to the S3 bucket, which will be displayed during maintenance. Next, set the S3 bucket as a second origin in the original CloudFront distribution and configure both to use an origin access identity (OAI) to secure access. Finally, during weekly maintenance, edit the default cache behavior to use the S3 origin and revert this change once maintenance is complete. This approach ensures visitors receive the informational message from the S3 bucket without encountering a CloudFront error.
Question 154 of 529
A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that processes and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN.
The Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment variables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new function version with the updated environment variables after validating results. This update process also requires frequent changes to the custom application to invoke the new function version ARN. These changes cause interruptions for users.
A solutions architect needs to simplify this process to minimize disruption to users.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Directly modify the environment variables of the published Lambda function version. Use the SLATEST version to test image processing parameters.
B.
Create an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image processing parameters from the DynamoDB table.
C.
Directly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function version when the company updates the parameters.
D.
Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing.
AnswerDiscussion
Correct Answer: D
To minimize disruption and operational overhead, using a Lambda function alias is the most effective solution. By creating an alias, the custom application can consistently reference the alias, and the alias can be updated to point to new function versions as needed. This approach eliminates the need to modify the client application each time environment variables are updated and a new function version is published. It allows for seamless updates and testing of new parameters by simply changing which version the alias points to, ensuring minimal interruption for users.
Question 155 of 529
A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex domain.
Which solution will meet these requirements with the LEAST effort?
A.
Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route traffic based on user location.
B.
Place a Network Load Balancer (NLB) in front of the ALMigrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB’s static IP address. Use a geolocation routing policy to route traffic based on user location.
C.
Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator’s static IP address to create a record in public DNS for the apex domain.
D.
Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL.
AnswerDiscussion
Correct Answer: C
To make the application available through an apex domain with the least effort, the best solution is to use an AWS Global Accelerator. This approach involves creating an AWS Global Accelerator with multiple endpoint groups targeting the appropriate AWS Regions. The accelerator provides a static IP address, which can then be used to create an A record in the public DNS for the apex domain. This method eliminates the need for CNAME records, which are not allowed for apex domains, and simplifies the deployment process by automatically routing traffic based on health and geography. Alternative methods involve more manual configuration and potential latency issues.
Question 156 of 529
A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions with API Gateway to use several shared libraries and custom classes.
A solutions architect needs to simplify the deployment of the solution and optimize for code reuse.
Which solution will meet these requirements?
A.
Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.
B.
Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.
C.
Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer.
D.
Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.
AnswerDiscussion
Correct Answer: D
To simplify deployment and optimize code reuse, deploying the shared libraries, custom classes, and code for the Lambda functions into a Docker image and storing this image in Amazon Elastic Container Registry (ECR) is the most efficient solution. This method allows the company to consolidate all dependencies and code into a single container image, which can then be utilized directly by the Lambda functions. This approach eliminates the limitations around Lambda layers not supporting Docker images, ensuring a streamlined and reusable deployment process.
Question 157 of 529
A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The company has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images.
The company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback even if the factory’s internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the workers.
How should the company deploy the ML model to meet these requirements?
A.
Set up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams. Upload the images to an Amazon S3 bucket. Deploy a SageMaker endpoint with the ML model. Invoke an AWS Lambda function to call the inference endpoint when new images are uploaded. Configure the Lambda function to call the local API when a defect is detected.
B.
Deploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still images from the cameras and run inference. Configure the component to call the local API when a defect is detected.
C.
Order an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still images from the cameras. Run inference from the EC2 instance. Configure the instance to call the local API when a defect is detected.
D.
Deploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the Amazon Monitron devices. Use Amazon Monitron health state alarms to call the local API from an AWS Lambda function when a defect is detected.
AnswerDiscussion
Correct Answer: B
To deploy the ML model and provide local feedback to factory workers even when the factory's internet connectivity is down, the company should use AWS IoT Greengrass. AWS IoT Greengrass can be deployed on the local Linux server to allow the company to run the ML model locally. It can handle taking still images from the IP cameras, running inferences, and calling the local API to provide feedback when a defect is detected. This setup ensures that the factory workers receive feedback in real-time without depending on an internet connection.
Question 158 of 529
A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect will use a configuration management database (CMDB) export of all the company's servers to create the case.
Which solution will meet these requirements MOST cost-effectively?
A.
Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations.
B.
Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.
C.
Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in bulk.
D.
Use AWS Application Discovery Service to import the CMDB data to perform an analysis.
AnswerDiscussion
Correct Answer: B
The most cost-effective solution for creating a business case for the migration of on-premises data to the AWS Cloud by using a CMDB export of all company servers is AWS Migration Evaluator. This tool is specifically designed to help companies analyze their current environments, including data from third-party tools like CMDBs, and provide detailed cost estimates and recommendations for migrating to AWS. It is a complimentary service that generates a comprehensive report with cost projections and savings, making it an ideal choice for creating a business case for migration.
Question 159 of 529
A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB is associated with an AWS WAF web ACL.
The website often encounters attacks in the application layer. The attacks produce sudden and significant increases in traffic on the application server. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to mitigate these attacks.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm action that adds the IP address to the web ACL’s deny list.
B.
Deploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource.
C.
Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application server’s subnet route table for any IP addresses that activate the alarm.
D.
Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses.
AnswerDiscussion
Correct Answer: B
To mitigate application layer attacks with the least operational overhead, deploying AWS Shield Advanced in addition to AWS WAF provides comprehensive protection. AWS Shield Advanced offers advanced DDoS protection and integrates seamlessly with AWS WAF to address web exploits. By adding the Application Load Balancer as a protected resource, attacks can be dynamically detected and mitigated automatically, minimizing the operational burden.
Question 160 of 529
A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and an Amazon Aurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already deployed in two Regions.
Company policy states that critical applications must have application tier components and data tier components deployed across two Regions. The RTO and RPO must be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant with company policy.
Which combination of steps will meet these requirements? (Choose two.)
A.
Add another Region to the Aurora MySQL DB cluster
B.
Add another Region to each table in the Aurora MySQL DB cluster
C.
Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster
D.
Convert the existing DynamoDB table to a global table by adding another Region to its configuration
E.
Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region
AnswerDiscussion
Correct Answer: A, D
To meet the company's policy of deploying application and data tier components across two Regions with an RTO and RPO of no more than a few minutes, we need to add additional Regions to both Amazon Aurora MySQL DB cluster and the DynamoDB table. Adding another Region to the Aurora MySQL DB cluster will allow data in Aurora MySQL to be replicated across Regions, supporting global read and write operations and providing high availability. Converting the existing DynamoDB table to a global table by adding another Region ensures that the data is replicated across multiple AWS Regions, enabling high availability and low-latency access.
Question 161 of 529
A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the company's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones behind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS terminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path.
The company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must develop a solution to allow traffic flow to AWS from the on-premises network so that the clients can continue to access the application.
Which solution will meet these requirements?
A.
Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP addresses to the firewall appliance.
B.
Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB.
C.
Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing target groups to the NLB. Update the clients to connect to the NLB. Delete the ALB Add the NLB IP addresses to the firewall appliance.
D.
Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB. Add the GWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB.
AnswerDiscussion
Correct Answer: B
The telecommunications company needs to allow traffic flow to AWS from the on-premises network using an on-premises firewall appliance. The firewall can be configured with an allow list based on IP addresses, but ALBs do not support static IP addresses, making option A unsuitable. To meet the company's requirements, a Network Load Balancer (NLB) can be used since it can have static IP addresses. By creating an ALB-type target group for the NLB and adding the existing ALB, the firewall appliance can whitelist the NLB's static IP addresses, and clients can then connect to the NLB, which routes traffic to the ALB. This setup retains the ALB's path-based routing capabilities, making option B the appropriate solution.
Question 162 of 529
A company runs an application on a fleet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is associated with the CloudFront distribution.
The company needs a solution that will prevent internet traffic from directly accessing the ALB.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB.
B.
Associate the existing web ACL with the ALB.
C.
Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.
D.
Add a security group rule to the ALB to allow only the various CloudFront IP address ranges.
AnswerDiscussion
Correct Answer: C
To prevent internet traffic from directly accessing the ALB while requiring the least operational overhead, the best solution is to add a security group rule to the ALB that allows traffic only from the AWS managed prefix list for CloudFront. This ensures that only traffic routed through CloudFront can reach the ALB and no direct internet traffic can access it. Using the AWS managed prefix list simplifies maintenance as it is automatically updated by AWS, reducing the need for manual updates to IP address ranges.
Question 163 of 529
A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that the company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit. Additionally, users can access the cache without authentication.
A solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.
Which solution will meet these requirements?
A.
Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with AUTH, and configure encryption in transit. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the AUTH token for authentication.
B.
Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure encryption in transit. Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication.
C.
Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication.
D.
Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the existing cluster to configure encryption in transit. Update the application to retrieve the SSL certificate from Parameter Store when necessary and to use the certificate for authentication.
AnswerDiscussion
Correct Answer: A
To meet the requirements of user authentication and end-to-end encryption, create an AUTH token to use for authentication and store it securely as an encrypted parameter in AWS Systems Manager Parameter Store. Create a new ElastiCache cluster with the AUTH token configured and enable encryption in transit. This ensures both the encryption in transit and the authentication using the AUTH token are properly set up. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the token for authentication. This approach ensures that the cache access is secure and encrypted end-to-end.
Question 164 of 529
A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement groups and a single instance type.
Recently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to improve the overall reliability of the workload.
Which solution will meet this requirement?
A.
Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.
B.
Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.
C.
Update the launch template Auto Scaling group to increase the number of placement groups.
D.
Update the launch template to use a larger instance type.
AnswerDiscussion
Correct Answer: B
To improve the overall reliability of the workload running on Amazon EC2 Spot Instances, the best solution is to use attribute-based instance type selection. This approach allows Amazon EC2 to automatically select from a wide range of instance types based on defined attributes such as vCPUs, memory, and other characteristics. This increases the chances of finding available compute capacity and reduces the likelihood of instance launch failures due to Spot Instance termination or unavailability. Creating a new launch template version with attribute-based instance type selection and configuring the Auto Scaling group to use this new version will enable the system to adapt to changing availability and improve reliability.
Question 165 of 529
A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3 API to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the document processing is finished, customers can download the documents directly from Amazon S3.
During the migration, the company discovered that it could not immediately update the processing server that generates many documents to support the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server finishes processing, the files must be available to the public for download within 30 minutes.
Which solution will meet these requirements with the LEAST amount of effort?
A.
Migrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company stores directly in Amazon S3.
B.
Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway.
C.
Configure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS.
D.
Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon S3.
AnswerDiscussion
Correct Answer: B
The best solution for this scenario is to set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. The processing server, which cannot be updated to use the S3 API immediately and requires fast local access to files, can mount the file share on an Amazon EC2 instance using NFS. The S3 File Gateway provides a way to efficiently manage and synchronize the local file system with the S3 bucket, fulfilling both the requirements of fast local file access and making the files available to customers within 30 minutes.
Question 166 of 529
A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.
The company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.
Which solution will meet these requirements?
A.
Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table.
B.
Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table.
C.
Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.
D.
Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table.
AnswerDiscussion
Correct Answer: C
The solution requires immediate deletion of user information across multiple microservices once the central user service deletes a user. Each microservice must be notified instantly to delete its copy of the data. The best approach is to use Amazon EventBridge because it is designed for building event-driven applications and can handle complex event patterns. When the central user service deletes a user, it can post an event to a custom EventBridge event bus. Then, EventBridge rules can be configured for each microservice to match the user deletion event and invoke the necessary logic to delete the user data from their respective storage services. This ensures that all microservices are notified instantly and can perform the required deletions without relying on polling mechanisms or dealing with SQS limitations in a fan-out pattern.
Question 167 of 529
A company is running a web application in a VPC. The web application runs on a group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is using AWS WAF.
An external customer needs to connect to the web application. The company must provide IP addresses to all external customers.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Replace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB.
B.
Allocate an Elastic IP address. Assign the Elastic IP address to the ALProvide the Elastic IP address to the customer.
C.
Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP addresses to the customer.
D.
Configure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution's DNS name to determine the distribution's public IP address. Provide the IP address to the customer.
AnswerDiscussion
Correct Answer: C
To meet the requirement of providing IP addresses with the least operational overhead, the best solution is to use AWS Global Accelerator with the Application Load Balancer (ALB) as the endpoint. AWS Global Accelerator provides static IP addresses and routes traffic optimally to ALB endpoints, minimizing configuration and maintenance efforts. Elastic IPs cannot be directly assigned to an ALB, and Network Load Balancers cannot be used with AWS WAF, making the other options less suitable.
Question 168 of 529
A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce Amazon Elastic Block Store (Amazon EBS) encryption at rest current production accounts and future production accounts only. The company needs a solution that includes built-in blueprints and guardrails.
Which combination of steps will meet these requirements? (Choose three.)
A.
Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts.
B.
Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.
C.
Create a new AWS Control Tower landing zone in the company’s management account. Add production and development accounts to production and development OUs. respectively.
D.
Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.
E.
Create a guardrail from the management account to detect EBS encryption.
F.
Create a guardrail for the production OU to detect EBS encryption.
AnswerDiscussion
Correct Answer: C, D, F
To meet the requirements, first, create a new AWS Control Tower landing zone in the company’s management account to ensure central governance and policy enforcement. Next, invite existing accounts to join the organization in AWS Organizations and create Service Control Policies (SCPs) to ensure compliance with organization rules. Finally, create a guardrail for the production Organizational Unit (OU) to detect EBS encryption at rest, ensuring that encryption is enforced specifically for production accounts. This approach leverages AWS Control Tower's built-in blueprints and guardrails, ensuring both current and future compliance within production accounts.
Question 169 of 529
A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an Amazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must recommend a solution to improve the resiliency of the application.
The solution must meet the following objectives:
• Application tier: RPO of 2 minutes. RTO of 30 minutes
• Database tier: RPO of 5 minutes. RTO of 30 minutes
The company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after a failover.
Which solution will meet these requirements?
A.
Configure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.
B.
Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS automated backups. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.
C.
Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Configure an Amazon CloudFront distribution in front of the ALB. Update DNS records to point to CloudFront.
D.
Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs.
AnswerDiscussion
Correct Answer: A
The solution requires ensuring optimal latency after a failover and meeting specified RPO and RTO objectives for both the application and database tiers. Using AWS Elastic Disaster Recovery (DRS) for EC2 instances provides a comprehensive and efficient solution for disaster recovery, ensuring that the state of the instances is consistently maintained, which DLM and backup solutions alone might not achieve as quickly. Creating a cross-Region read replica for the RDS DB instance ensures that the database can meet the required RPO and RTO. Additionally, utilizing AWS Global Accelerator helps optimize latency and provide fast, reliable failovers by directing traffic to the healthy ALB instances. This overall architecture meets the resiliency objectives effectively without making significant changes to the existing infrastructure.
Question 170 of 529
A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants to ensure that the instances are optimized based on CPU, memory, and network metrics.
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)
A.
Purchase AWS Business Support or AWS Enterprise Support for the account.
B.
Turn on AWS Trusted Advisor and review any “Low Utilization Amazon EC2 Instances” recommendations.
C.
Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.
D.
Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.
E.
Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest.
AnswerDiscussion
Correct Answer: C, D
To cost-optimize and appropriately size Amazon EC2 instances based on CPU, memory, and network metrics, the solutions architect should configure AWS Compute Optimizer and install the Amazon CloudWatch agent. AWS Compute Optimizer provides machine learning-based recommendations for optimal instance types, and installing the CloudWatch agent enables the collection of memory metrics, which improves these recommendations.
Question 171 of 529
A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS Region.
Which solution will meet these requirements?
A.
Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region.
B.
Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.
C.
Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.
D.
Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workflow to copy the snapshot to an S3 bucket in the second Region
AnswerDiscussion
Correct Answer: C
To back up an AWS CodeCommit repository and ensure the backup is stored in a second AWS Region, you can configure an Amazon EventBridge rule to trigger AWS CodeBuild whenever code is pushed to the repository. CodeBuild can then clone the repository, create a .zip file of the contents, and copy the file to an S3 bucket in the second region. This method allows for automatic and consistent backups aligned with the event-driven architecture provided by AWS services.
Question 172 of 529
A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several VPCs that have CIDR ranges that overlap. The company’s marketing team has created a new internal application and wants to make the application accessible to all the other business units. The solution must use private IP addresses only.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Instruct each business unit to add a unique secondary CIDR range to the business unit's VPC. Peer the VPCs and use a private NAT gateway in the secondary range to route traffic to the marketing team.
B.
Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPC. Create an AWS Site-to-Site VPN connection between the marketing team and each business unit's VPC. Perform NAT where necessary.
C.
Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses.
D.
Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB. Activate IAM authorization for the API. Grant access to the accounts of the other business units.
AnswerDiscussion
Correct Answer: C
To make an internal application accessible to multiple business units with the least operational overhead, the best solution is to use AWS PrivateLink. By creating an AWS PrivateLink endpoint service for the marketing application, specific AWS accounts can be granted permission to connect to the service. This enables the other business units to create interface VPC endpoints in their own accounts to access the application using private IP addresses. This approach avoids the need for complex VPC peering, NAT configurations, or additional infrastructure, thus minimizing operational overhead.
Question 173 of 529
A company needs to audit the security posture of a newly acquired AWS account. The company’s data security team requires a notification only when an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon SNS) topic that has the data security team's email address subscribed.
Which solution will meet these requirements?
A.
Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications.
B.
Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type “Access Analyzer Finding” with a filter for “isPublic: true.” Select the SNS topic as the EventBridge rule target.
C.
Create an Amazon EventBridge rule for the event type “Bucket-Level API Call via CloudTrail” with a filter for “PutBucketPolicy.” Select the SNS topic as the EventBridge rule target.
D.
Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type “Config Rules Re-evaluation Status” with a filter for “NON_COMPLIANT.” Select the SNS topic as the EventBridge rule target.
AnswerDiscussion
Correct Answer: B
To receive notifications when an Amazon S3 bucket becomes publicly exposed, creating an analyzer in AWS Identity and Access Management Access Analyzer and setting up an Amazon EventBridge rule for the event type 'Access Analyzer Finding' with a filter for 'isPublic: true' is the most effective solution. This approach ensures continuous monitoring of access control configurations and triggers notifications to the SNS topic whenever a publicly accessible bucket is detected.
Question 174 of 529
A solutions architect needs to assess a newly acquired company’s portfolio of applications and databases. The solutions architect must create a business case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is not well documented. The solutions architect cannot immediately determine how many applications and databases exist. Traffic for the applications is variable. Some applications are batch processes that run at the end of each month.
The solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.
Which solution will meet these requirements?
A.
Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service Catalog to understand application and database dependencies.
B.
Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs and database dependencies.
C.
Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies.
D.
Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone for core accounts and resources.
AnswerDiscussion
Correct Answer: C
To gain a comprehensive understanding of an on-premises application and database portfolio, it is crucial to evaluate both the infrastructure and dependencies. Migration Evaluator helps generate a list of servers and provides a detailed report, which is essential for creating a business case. AWS Migration Hub allows viewing the overall portfolio and managing migrations. AWS Application Discovery Service provides insight into application dependencies, crucial for an accurate assessment. These tools combined offer a robust approach to understanding and planning the migration.
Question 175 of 529
A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances of the application. The company needs to back up the files and retain the backups for 1 year.
Which solution will meet these requirements while providing the FASTEST storage performance?
A.
Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct the application to store files in the file system. Configure AWS Backup to back up and retain copies of the data for 1 year.
B.
Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume. Configure AWS Backup to back up and retain copies of the data for 1 year.
C.
Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecycle policy to delete objects after 1 year.
D.
Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool to back up the EKS cluster for 1 year.
AnswerDiscussion
Correct Answer: A
The solution must support the application running in multiple Availability Zones and provide fast storage performance. Amazon Elastic File System (EFS) meets these requirements as it provides shared file storage that is highly available and durable across multiple AZs. EFS offers high levels of aggregate throughput and IOPS, making it suitable for applications that generate many small files accessible across various instances. Additionally, AWS Backup can be configured to back up and retain copies of the data for 1 year. This makes EFS the optimal choice for the given scenario.
Question 176 of 529
A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience survey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises data center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to migrate the system to AWS to improve reliability.
Which solution will meet these requirements with the LEAST ongoing operational overhead?
A.
Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.
B.
Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers.
C.
Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers.
D.
Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers.
AnswerDiscussion
Correct Answer: A
To meet the requirements with the least ongoing operational overhead, the company should use Amazon Connect to replace the old call center hardware, and Amazon Pinpoint to send text message surveys to customers. Amazon Connect is a fully managed, cloud-based contact center service that simplifies the setup and management of customer interactions. Amazon Pinpoint is designed for sending targeted and personalized messages, including interactive, two-way experience surveys. This combination leverages fully managed services, reducing the need for the company to manage infrastructure or significant operational tasks.
Question 177 of 529
A company is building a call center by using Amazon Connect. The company’s operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows, hundreds of users, and dozens of claimed phone numbers.
Which solution will provide DR with the LOWEST RTO?
A.
Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact flows, users, and claimed phone numbers by using an AWS CloudFormation template.
B.
Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region.
C.
Provision a new Amazon Connect instance with all existing contact flows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function.
D.
Provision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function.
AnswerDiscussion
Correct Answer: D
The solution that will provide the lowest Recovery Time Objective (RTO) for a disaster recovery strategy in Amazon Connect is to have a standby instance already provisioned with most components pre-configured. In this scenario, provisioning a new Amazon Connect instance with all existing users and contact flows in a second Region is critical. By setting up an Amazon Route 53 health check and using an Amazon CloudWatch alarm to monitor for issues, any failure can quickly trigger an AWS Lambda function to deploy the remaining missing component, which in this case is the claimed phone numbers. This setup ensures that the recovery process is swift as most elements are already in place, reducing the time needed to fully restore functionality.
Question 178 of 529
A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to perform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift tables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits the files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has become difficult.
The company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants to confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the company publishes the data.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Use AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company that produces the data, create an Amazon API Gateway Data API service integration with Amazon Redshift. Require the data customers to subscribe to the data product.
B.
In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster. Configure subscription verification. Require the data customers to subscribe to the data product.
C.
Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product.
D.
Publish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS Data Exchange. In the AWS account of the company that produces the data, attach IAM resource-based policies to the Amazon Redshift tables to allow access only to verified AWS accounts.
AnswerDiscussion
Correct Answer: B
To meet the requirements with the least operational overhead and ensure customers have access to the most recent data while confirming their identities, using AWS Data Exchange to create a datashare connection to the Redshift cluster is the most efficient solution. This approach allows data to be shared directly from Amazon Redshift without additional steps like exporting data to S3 or managing external APIs, thus minimizing operational overhead. Configuring subscription verification ensures that only validated customers can access the data.
Question 179 of 529
A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of events that the solution receives. If a processing error occurs, the event must move into a separate queue for review.
Which solution will meet these requirements?
A.
Send event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Add an on-failure destination to the function. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target.
B.
Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messages to a dead-letter queue.
C.
Write events to an Amazon DynamoDB table. Configure a DynamoDB stream for the table. Configure the stream to invoke an AWS Lambda function. Configure the Lambda function to process the events.
D.
Publish events to an Amazon EventBndge event bus. Create and run an application on an Amazon EC2 instance with an Auto Scaling group that is behind an Application Load Balancer (ALB). Set the ALB as the event bus target. Configure the event bus to retry events. Write messages to a dead-letter queue if the application cannot process the messages.
AnswerDiscussion
Correct Answer: A
The best solution for processing events with automatic scaling based on the number of events and the ability to handle processing errors by moving events to a separate queue is to use Amazon SNS with an AWS Lambda function. In this configuration, events are sent to an Amazon SNS topic, which triggers an AWS Lambda function to process the events. AWS Lambda automatically scales based on the number of events, ensuring efficient handling of varying event loads. Additionally, AWS Lambda supports the configuration of an on-failure destination, where a separate Amazon SQS queue can be set as the target for events that fail to process. This setup meets all the requirements: it scales efficiently based on the number of events and provides a separate queue for reviewing processing errors.
Question 180 of 529
A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a sustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the processing engine through a RESTful API.
The API experiences unpredictable bursts of traffic. The company must implement a solution to process all data that the devices send to the processing engine. Data loss is unacceptable.
Which solution will meet these requirements?
A.
Create an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a listener and a target group for the ALB Add the SQS queue as the target. Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to process messages in the queue.
B.
Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue.
C.
Create an Amazon API Gateway REST API that implements the RESTful API. Create a fleet of Amazon EC2 instances in an Auto Scaling group. Create an API Gateway Auto Scaling group proxy integration. Use the EC2 instances to process incoming data.
D.
Create an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream as the origin for the distribution. Create an AWS Lambda function to consume and process data in the data stream.
AnswerDiscussion
Correct Answer: B
In this scenario, using Amazon API Gateway HTTP API integrated with Amazon Simple Queue Service (SQS) is the optimal solution. API Gateway provides a scalable interface to handle bursts of traffic efficiently. SQS acts as a buffer to store incoming data, ensuring that no data is lost during traffic spikes. AWS Lambda can then process the messages from the SQS queue, providing a serverless compute solution that scales automatically as needed. This combination guarantees data durability and reliable processing without loss.
Question 181 of 529
A company is designing its network configuration in the AWS Cloud. The company uses AWS Organizations to manage a multi-account setup. The company has three OUs. Each OU contains more than 100 AWS accounts. Each account has a single VPC, and all the VPCs in each OU are in the same AWS Region.
The CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution in which VPCs in the same OU can communicate with each other but cannot communicate with VPCs in other OUs.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create an AWS CloudFormation stack set that establishes VPC peering between accounts in each OU. Provision the stack set in each OU.
B.
In each OU, create a dedicated networking account that has a single VPC. Share this VPC with all the other accounts in the OU by using AWS Resource Access Manager (AWS RAM). Create a VPC peering connection between the networking account and each account in the OU.
C.
Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM). Create transit gateway VPC attachments for each VPC.
D.
In each OU, create a dedicated networking account that has a single VPC. Establish a VPN connection between the networking account and the other accounts in the OU. Use third-party routing software to route transitive traffic between the VPCs.
AnswerDiscussion
Correct Answer: C
Provisioning a transit gateway in an account in each OU and sharing the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM) is the optimal solution. This setup allows VPCs within the same OU to communicate with one another while isolating them from VPCs in different OUs. AWS Transit Gateway simplifies the network architecture by acting as a hub that interconnects VPCs, avoiding the complexity and limit constraints of VPC peering. Additionally, this method minimizes operational overhead compared to managing numerous individual VPC peering connections or VPNs with third-party routing software.
Question 182 of 529
A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company needs to store large important documents within the application with the following requirements:
1. The data must be highly durable and available
2. The data must always be encrypted at rest and in transit
3. The encryption key must be managed by the company and rotated periodically
Which of the following solutions should the solutions architect recommend?
A.
Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.
B.
Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.
C.
Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.
D.
Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data.
AnswerDiscussion
Correct Answer: B
Amazon S3 is a fully managed service designed to store and retrieve any amount of data at any time. S3 provides high durability and availability for stored data. It also supports server-side encryption and allows you to enforce encryption in transit using a bucket policy to enforce HTTPS for connections. Additionally, AWS Key Management Service (KMS) integration allows the company to manage and periodically rotate encryption keys. Thus, it meets all the specified requirements for high durability, availability, and encryption management.
Question 183 of 529
A company’s public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application Load Balancer (ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for several months.
Recently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection attacks had occurred against the API and that the API service had scaled to its maximum amount.
A solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must allow legitimate traffic through and must maximize operational efficiency.
Which solution meets these requirements?
A.
Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks.
B.
Create a new AWS WAF Bot Control implementation. Add a rule in the AWS WAF Bot Control managed rule group to monitor traffic and allow only legitimate traffic to the ALB in front of the ECS tasks.
C.
Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other traffic that does not match those rules. Attach the web ACL to the ALB in front of the ECS tasks.
D.
Create a new AWS WAF web ACL. Create a new empty IP set in AWS WAF. Add a new rule to the web ACL to block requests that originate from IP addresses in the new IP set. Create an AWS Lambda function that scrapes the API logs for IP addresses that send SQL injection attacks, and add those IP addresses to the IP set. Attach the web ACL to the ALB in front of the ECS tasks.
AnswerDiscussion
Correct Answer: C
To prevent SQL injection attacks while allowing legitimate traffic and maximizing operational efficiency, the best solution involves creating a new AWS WAF web ACL and adding a rule that blocks requests matching the SQL database rule group. The SQL database rule group contains predefined rules to block request patterns that could exploit SQL databases, such as SQL injection attacks. Attaching this web ACL to the Application Load Balancer ensures that harmful requests are blocked before reaching the ECS tasks, thereby preventing attacks and maintaining the performance of the API service.
Question 184 of 529
An environmental company is deploying sensors in major cities throughout a country to measure air quality. The sensors connect to AWS IoT Core to ingest timeseries data readings. The company stores the data in Amazon DynamoDB.
For business continuity, the company must have the ability to ingest and store data in two AWS Regions.
Which solution will meet these requirements?
A.
Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core data endpoints in both Regions Migrate data to Amazon Aurora global tables.
B.
Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Migrate the data to Amazon MemoryDB for Redis and configure cross-Region replication.
C.
Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations. Update the DynamoDB table to a global table.
D.
Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Configure DynamoDB streams and cross-Region data replication.
AnswerDiscussion
Correct Answer: C
To meet the requirements of ingesting and storing data in two AWS Regions, you need to configure a domain for AWS IoT Core in each Region and ensure business continuity through a failover mechanism. Using Amazon Route 53 health checks and a failover routing policy ensures that data can be ingested if one Region becomes unavailable. Updating DynamoDB to a global table provides seamless cross-Region replication, allowing data to be stored consistently across Regions. This setup ensures high availability and data redundancy, fulfilling the company's requirements.
Question 185 of 529
A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application that uses AWS Lambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table.
The DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table. The finance team and the marketing team have separate AWS accounts.
What should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?
A.
Create an SCP to grant the marketing team's AWS account access to the specific attributes of the DynamoDB table. Attach the SCP to the OU of the finance team.
B.
Create an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access control). Establish trust with the marketing team's account. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.
C.
Create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). Attach the policy to the DynamoDB table. In the marketing team's account, create an IAM role that has permissions to access the DynamoDB table in the finance team's account.
D.
Create an IAM role in the finance team's account to access the DynamoDB table. Use an IAM permissions boundary to limit the access to the specific attributes. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.
AnswerDiscussion
Correct Answer: B
To provide the marketing team with access to specific attributes in the DynamoDB table while ensuring security and proper access control, the best approach is to create an IAM role in the finance team's account with policy conditions for fine-grained access control. This role should establish trust with the marketing team's account. The marketing team's account should then be able to assume this role, allowing them the necessary access determined by the policies set. This setup leverages IAM roles and policy conditions effectively to control access and ensure that only the required attributes are accessible, maintaining the confidentiality of the data.
Question 186 of 529
A solutions architect is creating an application that stores objects in an Amazon S3 bucket. The solutions architect must deploy the application in two AWS Regions that will be used simultaneously. The objects in the two S3 buckets must remain synchronized with each other.
Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)
A.
Create an S3 Multi-Region Access Point Change the application to refer to the Multi-Region Access Point
B.
Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets
C.
Modify the application to store objects in each S3 bucket
D.
Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket
E.
Enable S3 Versioning for each S3 bucket
F.
Configure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3 bucket
AnswerDiscussion
Correct Answer: A, B, E
To ensure that objects in the two S3 buckets remain synchronized with each other while minimizing operational overhead, you should take the following steps: 1. Create an S3 Multi-Region Access Point. This allows the application to refer to a single global endpoint to access the S3 service, simplifying the application's configuration and traffic management. 2. Configure two-way S3 Cross-Region Replication (CRR). This ensures that any changes made in one S3 bucket are automatically replicated to the other bucket, keeping them in sync. 3. Enable S3 Versioning for both S3 buckets. Versioning is a prerequisite for enabling Cross-Region Replication, as it helps in tracking changes to objects and ensuring data consistency between the two buckets.
Question 187 of 529
A company has an IoT platform that runs in an on-premises environment. The platform consists of a server that connects to IoT devices by using the MQTT protocol. The platform collects telemetry data from the devices at least once every 5 minutes. The platform also stores device metadata in a MongoDB cluster.
An application that is installed on an on-premises machine runs periodic jobs to aggregate and transform the telemetry and device metadata. The application creates reports that users view by using another web application that runs on the same on-premises machine. The periodic jobs take 120-600 seconds to run. However, the web application is always running.
The company is moving the platform to AWS and must reduce the operational overhead of the stack.
Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)
A.
Use AWS Lambda functions to connect to the IoT devices
B.
Configure the IoT devices to publish to AWS IoT Core
C.
Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance
D.
Write the metadata to Amazon DocumentDB (with MongoDB compatibility)
E.
Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use Amazon CloudFront with an S3 origin to serve the reports
F.
Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress controller in the EKS cluster to serve the reports
AnswerDiscussion
Correct Answer: B, D, E
To meet the requirements with the least operational overhead, first configure the IoT devices to publish to AWS IoT Core, which is a managed service designed to handle IoT connectivity seamlessly and securely. Next, to handle the device metadata efficiently, use Amazon DocumentDB (with MongoDB compatibility), a fully managed, scalable, and highly available database service that is compatible with your existing MongoDB workloads. Finally, utilize AWS Step Functions along with AWS Lambda to prepare and write the reports to Amazon S3. Step Functions can orchestrate the steps of your workflow, while Lambda enables you to run code without provisioning servers. Using Amazon CloudFront to serve the reports stored in S3 ensures quick, reliable access to the reports with minimal maintenance.
Question 188 of 529
A global manufacturing company plans to migrate the majority of its applications to AWS. However, the company is concerned about applications that need to remain within a specific country or in the company's central on-premises data center because of data regulatory requirements or requirements for latency of single-digit milliseconds. The company also is concerned about the applications that it hosts in some of its factory sites, where limited network infrastructure exists.
The company wants a consistent developer experience so that its developers can build applications once and deploy on premises, in the cloud, or in a hybrid architecture. The developers must be able to use the same tools, APIs, and services that are familiar to them.
Which solution will provide a consistent hybrid experience to meet these requirements?
A.
Migrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on-premises data center and AWS. Deploy a Direct Connect gateway.
B.
Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Retain the devices on premises. Deploy AWS Wavelength to host the workloads in the factory sites.
C.
Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites.
D.
Migrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local Zone. Deploy AWS Wavelength to host the workloads in the factory sites.
AnswerDiscussion
Correct Answer: C
To provide a consistent hybrid experience and meet data regulatory requirements or latency needs, the company should use AWS Outposts for applications that require data compliance and low-latency, single-digit milliseconds performance, as Outposts can be installed on premises. For factory sites with limited network infrastructure, using AWS Snowball Edge Compute Optimized devices is suitable because they provide local compute and storage capabilities and can run workloads even with poor network conditions. This combination allows developers to use the same AWS tools, APIs, and services across on-premises and cloud environments, ensuring a unified development experience.
Question 189 of 529
A company is updating an application that customers use to make online orders. The number of attacks on the application by bad actors has increased recently.
The company will host the updated application on an Amazon Elastic Container Service (Amazon ECS) cluster. The company will use Amazon DynamoDB to store application data. A public Application Load Balancer (ALB) will provide end users with access to the application. The company must prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack.
Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)
A.
Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain. Configure the ALB to conditionally forward traffic if the header and value match.
B.
Deploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight.
C.
Configure auto scaling for Amazon ECS tasks Create a DynamoDB Accelerator (DAX) cluster.
D.
Configure Amazon ElastiCache to reduce overhead on DynamoDB.
E.
Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution.
AnswerDiscussion
Correct Answer: A, E
To prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack, two measures are cost-effective and aligned with the requirements. First, creating an Amazon CloudFront distribution with the ALB as the origin, adding a custom header and random value on the CloudFront domain, and configuring the ALB to conditionally forward traffic if the header and value match helps filter out potentially malicious requests. This setup provides a layer of security against attacks. Second, deploying an AWS WAF web ACL that includes an appropriate rule group and associating it with the Amazon CloudFront distribution adds an additional layer of protection by blocking common attack patterns like SQL injection and cross-site scripting (XSS). This combination ensures that the system is well protected at the front-end (CloudFront and WAF) while addressing cost-effectiveness and security requirements.
Question 190 of 529
A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon CloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fleet of Amazon EC2 instances in Auto Scaling groups. The application uses a domain name setup in Amazon Route 53.
Some users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the ALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors occur. The page should be displayed immediately for this error code.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Set up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the failover S3 bucket endpoint.
B.
Create a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy. Use an active-passive configuration between the two distributions.
C.
Create a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket that is configured to host a static website Set up origin failover for the CloudFront distribution. Update the S3 static website to incorporate the custom error page.
D.
Create a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket. Upload the custom error page to the S3 bucket as a failover. Update the function to read the S3 bucket and to serve the error page to the end users.
AnswerDiscussion
Correct Answer: C
The best solution to meet the requirement of displaying a custom error message page immediately when an HTTP 503 error occurs is to create a CloudFront origin group with two origins. The ALB endpoint will be set as the primary origin, and an S3 bucket hosting a static website with the custom error page will be set as the secondary origin. By setting up origin failover for the CloudFront distribution, CloudFront will automatically switch to the secondary origin and serve the custom error page promptly whenever the primary origin returns a 503 error code. This approach minimizes operational overhead by leveraging CloudFront's built-in capabilities for origin failover and custom error responses, ensuring a seamless experience for users during peak hours when errors occur.
Question 191 of 529
A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.
A solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the underlying infrastructure.
Which solution will meet these requirements?
A.
Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition.
B.
Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx for Lustre for shared storage. Reference the FSx for Lustre file system ID, container mount point, and FSx for Lustre authorization IAM role in the ECS task definition.
C.
Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic File System (Amazon EFS) for shared storage. Mount the EFS file system on the ECS container instances. Add the EFS authorization IAM role to the EC2 instance profile.
D.
Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic Block Store (Amazon EBS) volumes with Multi-Attach enabled for shared storage. Attach the EBS volumes to ECS container instances. Add the EBS authorization IAM role to an EC2 instance profile.
AnswerDiscussion
Correct Answer: A
For migrating an application running as a Docker container with NFS version 4 file share to AWS without requiring the management or provisioning of underlying infrastructure, the best solution is to use Amazon ECS with the Fargate launch type. Amazon EFS provides a scalable file storage service specifically designed to be used with NFS, making it an ideal choice for shared storage in this context. Amazon ECS with Fargate abstracts the server management, allowing the company to focus on deploying and managing applications instead of the underlying infrastructure. By referencing the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition, this setup ensures secure and scalable containerized solution.
Question 192 of 529
A company is running an application in the AWS Cloud. The core business logic is running on a set of Amazon EC2 instances in an Auto Scaling group. An Application Load Balancer (ALB) distributes traffic to the EC2 instances. Amazon Route 53 record api.example.com is pointing to the ALB.
The company's development team makes major updates to the business logic. The company has a rule that when changes are deployed, only 10% of customers can receive the new logic during a testing window. A customer must use the same version of the business logic during the testing window.
How should the company deploy the updates to meet these requirements?
A.
Create a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling group. Configure the ALB to distribute traffic to the EC2 instances. Update the Route 53 record to use weighted routing, and point the record to both of the ALBs.
B.
Create a second target group that is referenced by the ALDeploy the new logic to EC2 instances in this new target group. Update the ALB listener rule to use weighted target groups. Configure ALB target group stickiness.
C.
Create a new launch configuration for the Auto Scaling group. Specify the launch configuration to use the AutoScalingRollingUpdate policy, and set the MaxBatchSize option to 10. Replace the launch configuration on the Auto Scaling group. Deploy the changes.
D.
Create a second Auto Scaling group that is referenced by the ALB. Deploy the new logic on a set of EC2 instances in this new Auto Scaling group. Change the ALB routing algorithm to least outstanding requests (LOR). Configure ALB session stickiness.
AnswerDiscussion
Correct Answer: B
To meet the requirement that only 10% of customers receive the new business logic during a testing window and that they stick with the same version throughout the window, the company should create a second target group in the ALB and deploy the new logic to it. Using weighted target groups and configuring ALB target group stickiness ensures that a specified percentage of traffic is directed to the new logic and that each customer continues to be routed to the same instance of the business logic.
Question 193 of 529
A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The company is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is connected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels.
An investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput of 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.
What should the solutions architect do to meet these requirements with the LEAST administrative effort?
A.
Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system. Select SSD as the storage type. Select 32 MBps as the throughput capacity. When the backup and restore process is completed, adjust the DNS alias accordingly. Delete the original file system.
B.
Disconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to SSD. Reconnect users to the file system.
C.
Deploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location. Configure a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput as the target location. Schedule the task. When the task is completed, adjust the DNS alias accordingly. Delete the original file system.
D.
Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a point-in-time backup of the file system. Choose to restore previous versions. Create a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput. When the copy job is completed, adjust the DNS alias. Delete the original file system.
AnswerDiscussion
Correct Answer: B
To meet the performance improvement requirements with the least administrative effort, the best approach is to update the throughput capacity and storage type directly within the Amazon FSx console. By disconnecting users temporarily, increasing the throughput capacity to 32 MBps, and upgrading the storage type to SSD, the performance of the file system can be significantly improved without needing to create and manage backups, deploy additional agents, or handle complex restoration processes. This method leverages the built-in capabilities of Amazon FSx for efficient upgrades and minimizes administrative overhead.
Question 194 of 529
A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company must modify the application to deploy the application in two AWS Regions.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Set up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region Modify the application to use the CloudFront distribution. Use AWS Global Accelerator to access the data in the S3 bucket.
B.
Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions.
C.
Create a new S3 bucket in a second Region Deploy the application in the second Region. Configure the application to use the new S3 bucket. Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket.
D.
Set up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the new S3 gateway endpoint. Use S3 Intelligent-Tiering on the S3 bucket.
AnswerDiscussion
Correct Answer: B
To meet the requirement of deploying the application in two AWS Regions with the least operational overhead, the best solution is to create a new S3 bucket in the second Region and set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Additionally, configuring an S3 Multi-Region Access Point that uses both S3 buckets simplifies access from the application. This setup ensures that data is synchronized across both Regions and provides a seamless way for the application to interact with the data, thereby minimizing the operational overhead involved in managing data consistency manually.
Question 195 of 529
An online gaming company needs to rehost its gaming platform on AWS. The company's gaming application requires high performance computing (HPC) processing and has a leaderboard that changes frequently. An Ubuntu instance that is optimized for compute generation hosts a Node.js application for game display. Game state is tracked in an on-premises Redis instance.
The company needs a migration strategy that optimizes application performance.
Which solution will meet these requirements?
A.
Create an Auto Scaling group of m5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon ElastlCache for Redis cluster to maintain the leaderboard.
B.
Create an Auto Scaling group of c5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon OpenSearch Service cluster to maintain the leaderboard.
C.
Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard.
D.
Create an Auto Scaling group of m5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon DynamoDB table to maintain the leaderboard.
AnswerDiscussion
Correct Answer: C
The compute-optimized c5.large Amazon EC2 On-Demand Instances will provide the necessary high performance computing required for the gaming application. Using an Auto Scaling group behind an Application Load Balancer ensures scalability and reliability. Additionally, Amazon ElastiCache for Redis is well-suited for maintaining the frequently changing leaderboard due to its low latency and high throughput.
Question 196 of 529
A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be submitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run monthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.
Which combination of steps meets these requirements while minimizing operational overhead? (Choose two.)
A.
Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume of submissions on Fridays.
B.
Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability Zones. Use scheduled Service
Auto Scaling to add capacity before the high volume of submissions on Fridays.
C.
Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration.
D.
Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the data source.
E.
Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source.
AnswerDiscussion
Correct Answer: B, E
Deploying the application in a container using Amazon Elastic Container Service (ECS) with load balancing across multiple Availability Zones ensures high availability and allows for automatic scaling to handle the expected load on Fridays. This setup minimizes operational overhead because ECS manages the container orchestration and the scheduled scaling can handle peak times without manual intervention. Storing the timesheet submission data in Amazon S3 and using Amazon Athena to generate reports through Amazon QuickSight also minimizes operational overhead. S3 is highly durable and cost-effective for storing large amounts of data, while Athena provides a serverless query service that scales automatically. Using QuickSight for reporting further reduces the need for managing complex reporting infrastructure.
Question 197 of 529
A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the logs for 5 years. The company's security team also must receive an email notification every time there is an attempt to delete data in the S3 bucket.
Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)
A.
Configure AWS CloudTrail to log S3 data events.
B.
Configure S3 server access logging for the S3 bucket.
C.
Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES).
D.
Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic.
E.
Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering.
F.
Configure a new S3 bucket to store the logs with an S3 Lifecycle policy.
AnswerDiscussion
Correct Answer: B, D, F
To meet the requirements cost-effectively, the following combination of steps is appropriate: Configure S3 server access logging for the S3 bucket to log all activities, as it is cheaper than CloudTrail. Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (SNS) topic, ensuring the security team receives email notifications for deletion attempts. Configure a new S3 bucket to store the logs with an S3 Lifecycle policy to automatically transition logs to a more cost-effective storage class, ensuring the logs are stored cost-effectively for the required 5 years.
Question 198 of 529
A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed Amazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct. Connect connection to the data center from the Region that is closest to the data center.
The company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-premises data center also must have access to AWS public services.
Which combination of steps will meet these requirements with the LEAST cost? (Choose two.)
A.
Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions.
B.
Set up additional Direct Connect connections from the on-premises data center to the other two Regions.
C.
Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions.
D.
Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions.
E.
Use VPC peering to establish a connection between the VPCs across the Regions Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs.
AnswerDiscussion
Correct Answer: A, E
To meet the requirements with the least cost, you should create a Direct Connect gateway and use VPC peering. The Direct Connect gateway allows global access and can connect multiple VPCs across different regions to your on-premises data center, reducing the need for additional Direct Connect connections. VPC peering enables you to establish connections between VPCs across regions. This combination leverages existing connections and reduces the need for extra infrastructure, meeting the requirements cost-effectively.
Question 199 of 529
A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to provide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect is using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.
Which combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)
A.
Enable AWS Config in all accounts
B.
Enable Amazon GuardDuty in all accounts
C.
Enable all features for the organization
D.
Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions
E.
Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions
F.
Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions
AnswerDiscussion
Correct Answer: A, C, D
To provide baseline protection for the OWASP top 10 web application vulnerabilities using AWS WAF across all CloudFront distributions in an organization, the following steps should be taken: Enabling AWS Config in all accounts allows AWS Firewall Manager to detect newly created resources and provides necessary compliance checks. Enabling all features in AWS Organizations is required to leverage AWS Firewall Manager capabilities. Using AWS Firewall Manager to deploy AWS WAF rules ensures consistent application of security measures across all accounts and CloudFront distributions. This combination provides a comprehensive baseline protection as required.
Question 200 of 529
A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to authenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal, access to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are not able to access the AWS environment.
Which items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)
A.
The IAM user's permissions policy has allowed the use of SAML federation for that user.
B.
The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.
B. Test users are not in the AWSFederatedUsers group in the company's IdP.
C.
The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.
D.
The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.
E.
The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions.
AnswerDiscussion
Correct Answer: B, C, E
To ensure identity federation is properly configured, the solutions architect should verify that the IAM roles created for federated users or groups have trust policies setting the SAML provider as the principal. Next, confirm that the web portal correctly calls the AWS STS AssumeRoleWithSAML API, passing the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from the IdP. Lastly, ensure the company's IdP provides SAML assertions that correctly map users or groups to IAM roles with the necessary permissions in AWS.
Question 201 of 529
A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB instance that is experiencing overloaded connections. Most of the application’s operations insert records into the database. The application currently stores credentials in a text-based configuration file.
The solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the credentials secure and must provide the ability to rotate the credentials automatically on a regular basis.
Which solution will meet these requirements?
A.
Deploy an Amazon RDS Proxy layer. In front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager.
B.
Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials in AWS Systems Manager Parameter Store
C.
Create an Aurora Replica. Store the connection credentials as a secret in AWS Secrets Manager
D.
Create an Aurora Replica. Store the connection credentials in AWS Systems Manager Parameter Store.
AnswerDiscussion
Correct Answer: A
The proper solution involves deploying an Amazon RDS Proxy layer in front of the DB instance and storing the connection credentials as a secret in AWS Secrets Manager. RDS Proxy helps to manage an increased number of connections by establishing a connection pool and reusing these connections, thus effectively handling high loads and reducing the memory and CPU overhead associated with opening new connections. Additionally, AWS Secrets Manager provides a secure method for storing credentials and supports automatic rotation of these credentials, enhancing security and compliance.
Question 202 of 529
A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fleet of t3.large Amazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across multiple Availability Zones.
In the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes.
Which solution will meet these requirements MOST cost-effectively?
A.
Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Recover the EC2 instances from the latest EC2 backup. Use an Amazon Route 53 geolocation routing policy to automatically fail over to the DR Region in the event of a disaster.
B.
Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the EC2 instances at the minimum capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. Increase the desired capacity of the Auto Scaling group.
C.
Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Manually restore the backed-up data on new instances. Use an Amazon Route 53 simple routing policy to automatically fail over to the DR Region in the event of a disaster.
D.
Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the Auto Scaling group of EC2 instances at full capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster.
AnswerDiscussion
Correct Answer: B
To build a cost-effective disaster recovery solution with an RPO of 30 seconds and an RTO of 10 minutes, the company should use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Creating a cross-Region read replica for the DB ensures the database is replicated and maintained in sync within the required RPO. AWS Elastic Disaster Recovery can continuously replicate the EC2 instances to the DR Region, ensuring the latest data is available. Running the EC2 instances at the minimum capacity in the DR Region minimizes costs, and using an Amazon Route 53 failover routing policy allows for automatic failover to the DR Region, meeting the RTO requirement efficiently.
Question 203 of 529
A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's current internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a month to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database more quickly.
Which solution will migrate the database in the LEAST amount of time?
A.
Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises MySQL database to Aurora MySQL.
B.
Use AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use AWS Application Migration Service to migrate the on-premises MySQL database to Aurora MySQL.
C.
Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL.
D.
Order an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application Migration Service to migrate the data from Amazon S3 to Aurora MySQL.
AnswerDiscussion
Correct Answer: C
For a one-time migration of a large 60 TB MySQL database with limited internet bandwidth, the quickest option would be to use an AWS Snowball Edge device. The Snowball Edge device can securely and quickly transfer large amounts of data to AWS. Once the data is on Amazon S3, AWS Database Migration Service (AWS DMS) is used to migrate it to Amazon Aurora MySQL. This method avoids the constraints of the limited bandwidth of the current internet connection and significantly speeds up the migration process.
Question 204 of 529
A company has an application in the AWS Cloud. The application runs on a fleet of 20 Amazon EC2 instances. The EC2 instances are persistent and store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.
The company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration within 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes operational efficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network configuration in a secondary Region.
Which solution will meet these requirements?
A.
Create a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots by using AWS Systems Manager Automation runbooks. Copy the snapshots to the secondary Region. In the event of a failure launch the CloudFormation templates, restore the EBS volumes from snapshots, and transfer usage to the secondary Region.
B.
Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure, launch the CloudFormation template and use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region.
C.
Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in the secondary Region. In the event of a failure, launch the CloudFormation template, restore the instance volumes and configurations from the backup vault, and transfer usage to the secondary Region.
D.
Deploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the primary Region to the secondary Region. In the event of a failure, launch the CloudFormation template and transfer usage to the secondary Region.
AnswerDiscussion
Correct Answer: C
AWS Backup is a comprehensive and centralized service that can manage backups across multiple AWS services, including EC2 instances and EBS volumes. By creating a scheduled daily backup plan, AWS Backup ensures that both the instances and the data volumes are regularly backed up. The backups can be configured to be stored in a vault in a secondary AWS region, which meets the requirement of maintaining backups in a separate region. In case of a failure, the combination of AWS Backup and the CloudFormation template allows for the quick restoration of both instance configurations and data, ensuring minimal downtime and data loss. This solution optimizes operational efficiency and cost, aligning with the company's needs. Other options do not fully address the requirement to restore both instance configurations and data volumes efficiently while maintaining backups in a separate region.
Question 205 of 529
A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files. According to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using Amazon S3 and Amazon CloudFront.
Which combination of steps will meet the encryption requirements? (Choose three.)
A.
Turn on S3 server-side encryption for the S3 bucket that the web application uses.
B.
Add a policy attribute of "aws:SecureTransport": "true" for read and write operations in the S3 ACLs.
C.
Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses.
D.
Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).
E.
Configure redirection of HTTP requests to HTTPS requests in CloudFront.
F.
Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses.
AnswerDiscussion
Correct Answer: A, C, E
To meet the encryption requirements for a website hosted on Amazon S3 and Amazon CloudFront, the following steps should be taken: (1) Enabling server-side encryption on the S3 bucket ensures that all data stored in S3 is encrypted at rest. (2) Creating a bucket policy that denies any unencrypted operations ensures that only encrypted data can be uploaded or accessed, reinforcing security policies. (3) Configuring CloudFront to redirect HTTP requests to HTTPS ensures that all data in transit is securely encrypted, thus preventing any intercepted data from being readable. These measures together ensure compliance with the encryption requirements for both data at rest and in transit.
Question 206 of 529
A company is implementing a serverless architecture by using AWS Lambda functions that need to access a Microsoft SQL Server DB instance on Amazon RDS. The company has separate environments for development and production, including a clone of the database system.
The company's developers are allowed to access the credentials for the development database. However, the credentials for the production database must be encrypted with a key that only members of the IT security team's IAM user group can access. This key must be rotated on a regular basis.
What should a solutions architect do in the production environment to meet these requirements?
A.
Store the database credentials in AWS Systems Manager Parameter Store by using a SecureString parameter that is encrypted by an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the SecureString parameter. Restrict access to the SecureString parameter and the customer managed key so that only the IT security team can access the parameter and the key.
B.
Encrypt the database credentials by using the AWS Key Management Service (AWS KMS) default Lambda key. Store the credentials in the environment variables of each Lambda function. Load the credentials from the environment variables in the Lambda code. Restrict access to the KMS key so that only the IT security team can access the key.
C.
Store the database credentials in the environment variables of each Lambda function. Encrypt the environment variables by using an AWS Key Management Service (AWS KMS) customer managed key. Restrict access to the customer managed key so that only the IT security team can access the key.
D.
Store the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the secret. Restrict access to the secret and the customer managed key so that only the IT security team can access the secret and the key.
AnswerDiscussion
Correct Answer: D
To meet the requirements, the database credentials must be stored securely and associated with a key that can be rotated regularly. AWS Secrets Manager is specifically designed for managing sensitive information such as database credentials, with built-in support for key rotation using AWS Key Management Service (AWS KMS) customer managed keys. This setup ensures that only the IT security team can access both the secret and the key, fulfilling the security requirements for production environments.
Question 207 of 529
An online retail company is migrating its legacy on-premises .NET application to AWS. The application runs on load-balanced frontend web servers, load-balanced application servers, and a Microsoft SQL Server database.
The company wants to use AWS managed services where possible and does not want to rewrite the application. A solutions architect needs to implement a solution to resolve scaling issues and minimize licensing costs as the application scales.
Which solution will meet these requirements MOST cost-effectively?
A.
Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database.
B.
Create images of all the servers by using AWS Database Migration Service (AWS DMS). Deploy Amazon EC2 instances that are based on the on-premises imports. Deploy the instances in an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon DynamoDB as the database tier.
C.
Containerize the web frontend tier and the application tier. Provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon RDS for SQL Server to host the database.
D.
Separate the application functions into AWS Lambda functions. Use Amazon API Gateway for the web frontend tier and the application tier. Migrate the data to Amazon S3. Use Amazon Athena to query the data.
AnswerDiscussion
Correct Answer: A
The most cost-effective solution involves deploying EC2 instances in an Auto Scaling group behind an Application Load Balancer to handle the web and application tiers. Utilizing Amazon Aurora PostgreSQL with Babelfish will allow the legacy SQL Server database to be replatformed with minimal changes to the existing application code. This approach leverages AWS managed services, addresses scaling issues, and minimizes licensing costs without the need for significant rewrites of the application.
Question 208 of 529
A software-as-a-service (SaaS) provider exposes APIs through an Application Load Balancer (ALB). The ALB connects to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is deployed in the us-east-1 Region. The exposed APIs contain usage of a few non-standard REST methods: LINK, UNLINK, LOCK, and UNLOCK.
Users outside the United States are reporting long and inconsistent response times for these APIs. A solutions architect needs to resolve this problem with a solution that minimizes operational overhead.
Which solution meets these requirements?
A.
Add an Amazon CloudFront distribution. Configure the ALB as the origin.
B.
Add an Amazon API Gateway edge-optimized API endpoint to expose the APIs. Configure the ALB as the target.
C.
Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin.
D.
Deploy the APIs to two additional AWS Regions: eu-west-1 and ap-southeast-2. Add latency-based routing records in Amazon Route 53.
AnswerDiscussion
Correct Answer: C
To mitigate latency issues for users outside the United States, using AWS Global Accelerator is ideal. Global Accelerator enhances application availability and performance by routing traffic through the best performing and lowest-latency network paths, leveraging the vast AWS global network. This directly addresses the need for consistent and improved response times without the need to re-deploy across multiple regions, thereby minimizing operational overhead. It is designed to work with existing infrastructures like the Application Load Balancer, ensuring it supports the required non-standard REST methods effectively.
Question 209 of 529
A company runs an IoT application in the AWS Cloud. The company has millions of sensors that collect data from houses in the United States. The sensors use the MQTT protocol to connect and send data to a custom MQTT broker. The MQTT broker stores the data on a single Amazon EC2 instance. The sensors connect to the broker through the domain named iot.example.com. The company uses Amazon Route 53 as its DNS service. The company stores the data in Amazon DynamoDB.
On several occasions, the amount of data has overloaded the MQTT broker and has resulted in lost sensor data. The company must improve the reliability of the solution.
Which solution will meet these requirements?
A.
Create an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. Use the Auto Scaling group as the target for the ALB. Update the DNS record in Route 53 to an alias record. Point the alias record to the ALB. Use the MQTT broker to store the data.
B.
Set up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record in Route 53 to point to the AWS IoT Core Data-ATS endpoint. Configure an AWS IoT rule to store the data.
C.
Create a Network Load Balancer (NLB). Set the MQTT broker as the target. Create an AWS Global Accelerator accelerator. Set the NLB as the endpoint for the accelerator. Update the DNS record in Route 53 to a multivalue answer record. Set the Global Accelerator IP addresses as values. Use the MQTT broker to store the data.
D.
Set up AWS IoT Greengrass to receive the sensor data. Update the DNS record in Route 53 to point to the AWS IoT Greengrass endpoint. Configure an AWS IoT rule to invoke an AWS Lambda function to store the data.
AnswerDiscussion
Correct Answer: B
The issue is that the single EC2 instance hosting the custom MQTT broker is overloaded. To improve the reliability of the solution, leveraging AWS managed services designed for IoT data handling is more appropriate. AWS IoT Core is a managed service specifically designed for handling IoT data securely and reliably. By using AWS IoT Core, the company can scale the solution automatically and handle the influx of data more efficiently. Additionally, AWS IoT Core offers built-in rules to process and route incoming data, including the capability to store it directly into Amazon DynamoDB as required by the company. This ensures high availability, reliability, and efficient data processing, addressing the core issue of overloading the existing MQTT broker.
Question 210 of 529
A company has Linux-based Amazon EC2 instances. Users must access the instances by using SSH with EC2 SSH key pairs. Each machine requires a unique EC2 key pair.
The company wants to implement a key rotation policy that will, upon request, automatically rotate all the EC2 key pairs and keep the keys in a securely encrypted place. The company will accept less than 1 minute of downtime during key rotation.
Which solution will meet these requirements?
A.
Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Secrets Manager.
B.
Store all the keys in Parameter Store, a capability of AWS Systems Manager, as a string. Define a Systems Manager maintenance window to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Parameter Store.
C.
Import the EC2 key pairs into AWS Key Management Service (AWS KMS). Configure automatic key rotation for these key pairs. Create an Amazon EventBridge scheduled rule to invoke an AWS Lambda function to initiate the key rotation in AWS KMS.
D.
Add all the EC2 instances to Fleet Manager, a capability of AWS Systems Manager. Define a Systems Manager maintenance window to issue a Systems Manager Run Command document to generate new key pairs and to rotate public keys to all the instances in Fleet Manager.
AnswerDiscussion
Correct Answer: A
To implement a key rotation policy for Linux-based Amazon EC2 instances with unique key pairs, storing the keys securely and automatically rotating them upon request with minimal downtime, using AWS Secrets Manager is the optimal solution. AWS Secrets Manager can securely store and manage access to secrets, such as key pairs. It can invoke an AWS Lambda function to generate new key pairs, update public keys on the EC2 instances, and update private keys securely within Secrets Manager. This setup ensures that the keys are rotated automatically and securely with minimal downtime.
Question 211 of 529
A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no configuration management database and has little knowledge about the utilization of the VMware portfolio.
A solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight. Identify servers that have high utilization. Remove the servers that have high utilization from the migration list. Import the data to AWS Migration Hub.
B.
Export the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the data to AWS Application Migration Service. Use AWS Server Migration Service (AWS SMS) to migrate the remaining servers.
C.
Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub.
D.
Deploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze the data. Use Amazon QuickSight for data visualization.
AnswerDiscussion
Correct Answer: C
To achieve an accurate inventory with the least operational overhead, deploying the Migration Evaluator agentless collector to the ESXi hypervisor is the best solution. This approach leverages an agentless collector, reducing the need for individual VM agent deployment and minimizing operational tasks. The collected data can be reviewed directly within Migration Evaluator, allowing for easy identification of inactive servers and seamless integration with AWS Migration Hub for planning the migration.
Question 212 of 529
A company runs a microservice as an AWS Lambda function. The microservice writes data to an on-premises SQL database that supports a limited number of concurrent connections. When the number of Lambda function invocations is too high, the database crashes and causes application downtime. The company has an AWS Direct Connect connection between the company's VPC and the on-premises data center. The company wants to protect the database from crashes.
Which solution will meet these requirements?
A.
Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write to the existing database. Set a reserved concurrency limit on the Lambda function that is less than the number of connections that the database supports.
B.
Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the data from the existing database to Aurora Serverless. Reconfigure the Lambda function to write to Aurora.
C.
Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the Amazon RDS DB instance. Reconfigure the Lambda function to write to the RDS Proxy DB instance.
D.
Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the Lambda function to write to the existing database when the topic receives new messages. Configure provisioned concurrency for the Lambda function to be equal to the number of connections that the database supports.
AnswerDiscussion
Correct Answer: A
To protect the on-premises SQL database from crashes due to a high number of Lambda function invocations, the best solution is to write the data to an Amazon Simple Queue Service (Amazon SQS) queue and configure the Lambda function to read from the queue and write to the existing database. By setting a reserved concurrency limit on the Lambda function that is less than the number of connections that the database supports, the database is prevented from being overwhelmed by too many concurrent connections. This approach ensures that the database operates within its capacity while the SQS queue manages the flow of incoming data.
Question 213 of 529
A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS workloads. The company has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be highly available and cannot be down for longer than 10 minutes. The company needs to minimize ongoing maintenance.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Migrate to Amazon CloudWatch dashboards. Recreate the dashboards to match the existing Grafana dashboards. Use automatic dashboards where possible.
B.
Create an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing Grafana instance. Import the dashboards into the new workspace.
C.
Create an AMI that has Grafana pre-installed. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto Scaling group that uses the new AMI. Set the Auto Scaling group's minimum, desired, and maximum number of instances to one. Create an Application Load Balancer that serves at least two Availability Zones.
D.
Configure AWS Backup to back up the EC2 instance that runs Grafana once each hour. Restore the EC2 instance from the most recent snapshot in an alternate Availability Zone when required.
AnswerDiscussion
Correct Answer: B
Amazon Managed Grafana provides a fully managed Grafana service that offloads the operational overhead of managing, maintaining, and scaling the Grafana infrastructure. This ensures high availability and minimizes downtime to meet the company's requirement of not being down for longer than 10 minutes. Additionally, exporting the dashboards from the existing Grafana instance and importing them into the new Managed Grafana workspace preserves the time and effort invested in creating the dashboards. Other options either require significant ongoing maintenance or do not guarantee the required availability.
Question 214 of 529
A company needs to migrate its customer transactions database from on premises to AWS. The database resides on an Oracle DB instance that runs on a Linux server. According to a new security requirement, the company must rotate the database password each year.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool (AWS SCT). Store the password in AWS Systems Manager Parameter Store. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly passtard rotation.
B.
Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a yearly rotation schedule.
C.
Migrate the database to an Amazon EC2 instance. Use AWS Systems Manager Parameter Store to keep and rotate the connection string by using an AWS Lambda function on a yearly schedule.
D.
Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool (AWS SCT). Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly password rotation.
AnswerDiscussion
Correct Answer: B
Migrating the database to Amazon RDS for Oracle and using AWS Secrets Manager with automatic rotation turned on and configured for a yearly schedule provides the least operational overhead. AWS Secrets Manager is designed to manage and rotate database passwords automatically, which simplifies the management process and ensures compliance with the company's security requirement to rotate the database password each year. This approach leverages managed services that minimize the need for manual intervention and additional infrastructure management compared to other options.
Question 215 of 529
A solutions architect is designing an AWS account structure for a company that consists of multiple teams. All the teams will work in the same AWS Region. The company needs a VPC that is connected to the on-premises network. The company expects less than 50 Mbps of total traffic to and from the on-premises network.
Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)
A.
Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account.
B.
Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager.
C.
Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by using AWS Resource Access Manager.
D.
Use AWS Site-to-Site VPN for connectivity to the on-premises network.
E.
Use AWS Direct Connect for connectivity to the on-premises network.
AnswerDiscussion
Correct Answer: B, D
To meet the requirements cost-effectively, the best approach includes creating a shared VPC for all teams and using AWS Site-to-Site VPN for connectivity to the on-premises network. Sharing the VPC and subnets simplifies management and optimizes resource utilization across teams, while AWS Site-to-Site VPN provides a cost-effective means of connecting to the on-premises network at the expected traffic levels without incurring the higher costs associated with AWS Direct Connect or the complexity of AWS Transit Gateway.
Question 216 of 529
A solutions architect at a large company needs to set up network security for outbound traffic to the internet from all AWS accounts within an organization in AWS Organizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a centralized AWS Transit Gateway. Each account has both an internet gateway and a NAT gateway for outbound traffic to the internet. The company deploys resources only into a single AWS Region.
The company needs the ability to add centrally managed rule-based filtering on all outbound traffic to the internet for all AWS accounts in the organization. The peak load of outbound traffic will not exceed 25 Gbps in each Availability Zone.
Which solution meets these requirements?
A.
Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Create an Auto Scaling group of Amazon EC2 instances that run an open-source internet proxy for rule-based filtering across all Availability Zones in the Region. Modify all default routes to point to the proxy's Auto Scaling group.
B.
Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Use an AWS Network Firewall firewall for rule-based filtering. Create Network Firewall endpoints in each Availability Zone. Modify all default routes to point to the Network Firewall endpoints.
C.
Create an AWS Network Firewall firewall for rule-based filtering in each AWS account. Modify all default routes to point to the Network Firewall firewalls in each account.
D.
In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for rule-based filtering. Modify all default routes to point to the proxy's Auto Scaling group.
AnswerDiscussion
Correct Answer: B
The best solution for setting up centrally managed rule-based filtering on all outbound traffic to the internet for all AWS accounts in the organization is to create a new VPC specifically for outbound traffic. Connect the existing transit gateway to this new VPC and configure a new NAT gateway for outbound connectivity. AWS Network Firewall is a managed service designed for such purposes and ensures centralized management of network security policies. Create Network Firewall endpoints in each Availability Zone and modify all default routes to point to these endpoints. This configuration provides a robust, scalable, and centrally managed way to handle outbound traffic security.
Question 217 of 529
A company uses a load balancer to distribute traffic to Amazon EC2 instances in a single Availability Zone. The company is concerned about security and wants a solutions architect to re-architect the solution to meet the following requirements:
• Inbound requests must be filtered for common vulnerability attacks.
• Rejected requests must be sent to a third-party auditing application.
• All resources should be highly available.
Which solution meets these requirements?
A.
Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Use Amazon Inspector to monitor traffic to the ALB and EC2 instances. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB. Use an AWS Lambda function to frequently push the Amazon Inspector report to the third-party auditing application.
B.
Configure an Application Load Balancer (ALB) and add the EC2 instances as targets. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB name and enable logging with Amazon CloudWatch Logs. Use an AWS Lambda function to frequently push the logs to the third-party auditing application.
C.
Configure an Application Load Balancer (ALB) along with a target group adding the EC2 instances as targets. Create an Amazon Kinesis Data Firehose with the destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber.
D.
Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Create an Amazon Kinesis Data Firehose with a destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the WebACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber.
AnswerDiscussion
Correct Answer: D
To meet the requirements, the solution must include features for filtering inbound requests for common vulnerabilities, sending rejected requests to a third-party auditing application, and ensuring high availability. Configuring a Multi-AZ Auto Scaling group using the application's AMI ensures high availability. Creating an Application Load Balancer (ALB) and selecting the Auto Scaling group as the target distributes the traffic effectively. Using AWS WAF in conjunction with a web ACL filters inbound requests for vulnerabilities. Rejected requests can be logged and sent to a third-party auditing application via Amazon Kinesis Data Firehose. This setup satisfies all the specified requirements.
Question 218 of 529
A company is running an application in the AWS Cloud. The application consists of microservices that run on a fleet of Amazon EC2 instances in multiple Availability Zones behind an Application Load Balancer. The company recently added a new REST API that was implemented in Amazon API Gateway. Some of the older microservices that run on EC2 instances need to call this new API.
The company does not want the API to be accessible from the public internet and does not want proprietary data to traverse the public internet.
What should a solutions architect do to meet these requirements?
A.
Create an AWS Site-to-Site VPN connection between the VPC and the API Gateway. Use API Gateway to generate a unique API Key for each microservice. Configure the API methods to require the key.
B.
Create an interface VPC endpoint for API Gateway, and set an endpoint policy to only allow access to the specific API. Add a resource policy to API Gateway to only allow access from the VPC endpoint. Change the API Gateway endpoint type to private.
C.
Modify the API Gateway to use IAM authentication. Update the IAM policy for the IAM role that is assigned to the EC2 instances to allow access to the API Gateway. Move the API Gateway into a new VPDeploy a transit gateway and connect the VPCs.
D.
Create an accelerator in AWS Global Accelerator, and connect the accelerator to the API Gateway. Update the route table for all VPC subnets with a route to the created Global Accelerator endpoint IP address. Add an API key for each service to use for authentication.
AnswerDiscussion
Correct Answer: B
To avoid the API being accessible from the public internet and ensure that proprietary data does not traverse the public internet, creating an interface VPC endpoint for API Gateway is the best option. This setup allows secure, private communication between the older microservices on EC2 instances and the new API. By setting an endpoint policy to only allow access to the specific API and adding a resource policy to API Gateway to only allow access from the VPC endpoint, the traffic remains within AWS's internal network. Changing the API Gateway endpoint type to private ensures it is not accessible from the public internet, meeting the security requirements specified.
Question 219 of 529
A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses Amazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account. Occasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment.
A solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make noncompliant changes to the security settings for the EC2 instances.
What is the FASTEST way for the solutions architect to meet these requirements?
A.
Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the AWS account.
B.
Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected.
C.
Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment.
D.
Enable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic.
AnswerDiscussion
Correct Answer: D
To track changes and send alerts when noncompliant changes are made to EC2 security settings, enabling AWS Config on the EC2 security groups is the fastest and most effective solution. AWS Config continuously monitors and records your AWS resource configurations and assesses them against the desired configuration. It can detect noncompliant changes and send alerts through an Amazon Simple Notification Service (SNS) topic. This approach provides both real-time tracking of changes and the necessary alerting mechanisms in the most efficient manner.
Question 220 of 529
A company has IoT sensors that monitor traffic patterns throughout a large city. The company wants to read and collect data from the sensors and perform aggregations on the data.
A solutions architect designs a solution in which the IoT devices are streaming to Amazon Kinesis Data Streams. Several applications are reading from the stream. However, several consumers are experiencing throttling and are periodically encountering a ReadProvisionedThroughputExceeded error.
Which actions should the solutions architect take to resolve this issue? (Choose three.)
A.
Reshard the stream to increase the number of shards in the stream.
B.
Use the Kinesis Producer Library (KPL). Adjust the polling frequency.
C.
Use consumers with the enhanced fan-out feature.
D.
Reshard the stream to reduce the number of shards in the stream.
E.
Use an error retry and exponential backoff mechanism in the consumer logic.
F.
Configure the stream to use dynamic partitioning.
AnswerDiscussion
Correct Answer: A, C, E
To resolve throttling and ReadProvisionedThroughputExceeded errors in Amazon Kinesis Data Streams, the solutions architect should reshard the stream to increase the number of shards, as this will enhance the stream's overall throughput capacity. Using consumers with the enhanced fan-out feature allows multiple consumers to read concurrently from the same shard, reducing read capacity limitations and minimizing throttling. Additionally, implementing an error retry with exponential backoff mechanism in the consumer logic helps manage throttling errors by retrying read operations with incremental delays, preventing overwhelming the system.
Question 221 of 529
A company uses AWS Organizations to manage its AWS accounts. The company needs a list of all its Amazon EC2 instances that have underutilized CPU or memory usage. The company also needs recommendations for how to downsize these underutilized instances.
Which solution will meet these requirements with the LEAST effort?
A.
Install a CPU and memory monitoring tool from AWS Marketplace on all the EC2 instances. Store the findings in Amazon S3. Implement a Python script to identify underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options.
B.
Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in the organization’s management account. Use the recommendations to downsize underutilized instances in all accounts of the organization.
C.
Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in each account of the organization. Use the recommendations to downsize underutilized instances in all accounts of the organization.
D.
Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Create an AWS Lambda function to extract CPU and memory usage from all the EC2 instances. Store the findings as files in Amazon S3. Use Amazon Athena to find underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options.
AnswerDiscussion
Correct Answer: B
To meet the requirements with the least effort, the most efficient solution is to install the Amazon CloudWatch agent on all the EC2 instances using AWS Systems Manager and then retrieve the resource optimization recommendations from AWS Cost Explorer in the organization's management account. This approach leverages existing AWS services and tools, eliminating the need for additional installations or custom scripts. By using the centralized management account, you can efficiently manage and downsize underutilized instances across all accounts within the organization with minimal administrative overhead.
Question 222 of 529
A company wants to run a custom network analysis software package to inspect traffic as traffic leaves and enters a VPC. The company has deployed the solution by using AWS CloudFormation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been established to direct traffic to the EC2 instances.
Whenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the instance replacement occurs.
Which combination of steps will resolve this issue? (Choose three.)
A.
Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance.
B.
Update the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to send process metrics for the application.
C.
Update the CloudFormation template to install AWS Systems Manager Agent on the EC2 instances. Configure Systems Manager Agent to send process metrics for the application.
D.
Create an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.
E.
Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out of service. Update the network routes to point to the replacement instance.
F.
In the CloudFormation template, write a condition that updates the network routes when a replacement instance is launched.
AnswerDiscussion
Correct Answer: B, D, E
To address the issue where the network routes are not updated when an instance is replaced, a combination of steps is necessary. First, updating the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances enables the collection of process metrics for the application, ensuring proper monitoring of software performance. Second, creating an alarm for the custom metric in Amazon CloudWatch for failure scenarios and configuring it to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic ensures that any detected issues trigger notifications. Third, creating an AWS Lambda function that responds to the Amazon SNS message to take the instance out of service and to update the network routes to point to the replacement instance ensures that routing changes are automatically handled whenever an instance replacement occurs. This combination effectively addresses the network route updating issue upon instance replacement.
Question 223 of 529
A company is developing a new on-demand video application that is based on microservices. The application will have 5 million users at launch and will have 30 million users after 6 months. The company has deployed the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. The company developed the application by using ECS services that use the HTTPS protocol.
A solutions architect needs to implement updates to the application by using blue/green deployments. The solution must distribute traffic to each ECS service through a load balancer. The application must automatically adjust the number of tasks in response to an Amazon CloudWatch alarm.
Which solution will meet these requirements?
A.
Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Request increases to the service quota for tasks per service to meet the demand.
B.
Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Implement Auto Scaling group for each ECS service by using the Cluster Autoscaler.
C.
Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement an Auto Scaling group for each ECS service by using the Cluster Autoscaler.
D.
Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for each ECS service.
AnswerDiscussion
Correct Answer: D
The solution requires blue/green deployments, HTTPS protocol support, and automatic adjustment of tasks based on CloudWatch alarms. Application Load Balancers (ALBs) are suitable for HTTPS traffic and provide advanced routing features necessary for blue/green deployments. Since AWS Fargate does not support cluster auto-scaling, but it does support service auto-scaling, configuring the ECS services to use an Application Load Balancer and implementing Service Auto Scaling is the appropriate solution to meet these requirements.
Question 224 of 529
A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service (Amazon ECS) on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group.
The company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the new image version receives a unique tag.
The company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically delete new image tags that have Critical or High severity findings. The solution also must notify the development team when such a deletion occurs.
Which solution meets these requirements?
A.
Configure scan on push on the repository. Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is complete for images that have Critical or High severity findings. Use the Step Functions state machine to delete the image tag for those images and to notify the development team through Amazon Simple Notification Service (Amazon SNS).
B.
Configure scan on push on the repository. Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Lambda function when a new message is added to the SQS queue. Use the Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES).
C.
Schedule an AWS Lambda function to start a manual image scan every hour. Configure Amazon EventBridge to invoke another Lambda function when a scan is complete. Use the second Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).
D.
Configure periodic image scan on the repository. Configure scan results to be added to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Step Functions state machine when a new message is added to the SQS queue. Use the Step Functions state machine to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES).
AnswerDiscussion
Correct Answer: A
The correct solution involves configuring scan on push on the repository, as it ensures that images are scanned immediately when they are uploaded. Using Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is complete allows for automation in the processing of scan results. The Step Functions state machine can then handle both the deletion of image tags with severe findings and the notification to the development team via Amazon SNS. This approach meets all the requirements: automated scanning, automated deletion of problematic images, and developer notification upon such deletions.
Question 225 of 529
A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. The workloads are hosted on Amazon EC2. AWS Fargate. and AWS Lambda. Some of the workloads have unpredictable demand. Accounts record high usage in some months and low usage in other months.
The company wants to optimize its compute costs over the next 3 years. A solutions architect obtains a 6-month average for each of the accounts across the organization to calculate usage.
Which solution will provide the MOST cost savings for all the organization's compute usage?
A.
Purchase Reserved Instances for the organization to match the size and number of the most common EC2 instances from the member accounts.
B.
Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level.
C.
Purchase Reserved Instances for each member account that had high EC2 usage according to the data from the last 6 months.
D.
Purchase an EC2 Instance Savings Plan for each member account from the management account based on EC2 usage data from the last 6 months.
AnswerDiscussion
Correct Answer: B
To optimize compute costs over the next three years for workloads hosted on Amazon EC2, AWS Fargate, and AWS Lambda, considering some workloads have unpredictable demand with high and low usage in different months, the best solution is to purchase a Compute Savings Plan for the organization from the management account. Compute Savings Plans provide the most flexibility and can cover usage across EC2, Fargate, and Lambda, which is essential given the varied compute services used and unpredictable demand. They also automatically apply to any EC2 instance regardless of instance family, size, region, or tenancy, making them well-suited for organization-wide cost optimization.
Question 226 of 529
A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations to manage all the accounts. The company has turned on all features.
A finance team has allocated a daily budget for AWS costs. The finance team must receive an email notification if the organization's AWS costs exceed 80% of the allocated budget. A solutions architect needs to implement a solution to track the costs and deliver the notifications.
Which solution will meet these requirements?
A.
In the organization's management account, use AWS Budgets to create a budget that has a daily period. Add an alert threshold and set the value to 80%. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.
B.
In the organization’s management account, set up the organizational view feature for AWS Trusted Advisor. Create an organizational view report for cost optimization. Set an alert threshold of 80%. Configure notification preferences. Add the email addresses of the finance team.
C.
Register the organization with AWS Control Tower. Activate the optional cost control (guardrail). Set a control (guardrail) parameter of 80%. Configure control (guardrail) notification preferences. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.
D.
Configure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket in the organization's management account. Use Amazon EventBridge to schedule a daily Amazon Athena query to calculate the organization’s costs. Configure Athena to send an Amazon CloudWatch alert if the total costs are more than 80% of the allocated budget. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.
AnswerDiscussion
Correct Answer: A
To meet the requirement of notifying the finance team if the organization's AWS costs exceed 80% of the allocated daily budget, the best solution is to use AWS Budgets in the organization's management account. AWS Budgets allows you to create a budget that can track costs on a daily basis and set alert thresholds. By setting the threshold to 80%, an alert can be triggered. Amazon Simple Notification Service (Amazon SNS) can then be utilized to send email notifications to the finance team, ensuring that they are informed when the costs approach the specified limit. This solution is both straightforward and efficient for the given scenario.
Question 227 of 529
A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon EC2 instances in the us-east-1 Region. Artists upload photos of their work as large-size. high-resolution image files from their mobile phones to a centralized Amazon S3 bucket created in the us-east-1 Region. The users in Europe are reporting slow performance for their image uploads.
How can a solutions architect improve the performance of the image upload process?
A.
Redeploy the application to use S3 multipart uploads.
B.
Create an Amazon CloudFront distribution and point to the application as a custom origin.
C.
Configure the buckets to use S3 Transfer Acceleration.
D.
Create an Auto Scaling group for the EC2 instances and create a scaling policy.
AnswerDiscussion
Correct Answer: C
To improve the performance of image uploads for users in Europe, configuring the bucket to use S3 Transfer Acceleration is the most effective solution. S3 Transfer Acceleration uses the Amazon CloudFront global network of edge locations to route the data to the S3 bucket, significantly reducing latency and speeding up data transfer, especially for large files being uploaded from regions far from the S3 bucket's location.
Question 228 of 529
A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application includes web. application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed data must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in traffic.
Which solution will meet these requirements with the LEAST ongoing operational overhead?
A.
Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS) for data that is frequently accessed between the web and application tiers. Store the frontend web server session data in Amazon Simple Queue Service (Amazon SQS).
B.
Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache frontend web server session data. Use Amazon Elastic Block Store (Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones.
C.
Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use ReplicaSets to run the web servers and applications. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system across all EKS pods to store frontend web server session data.
D.
Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the web servers and application as Kubernetes deployments in the EKS cluster. Store the frontend web server session data in an Amazon DynamoDB table. Create an Amazon Elastic File System (Amazon EFS) volume that all applications will mount at the time of deployment.
AnswerDiscussion
Correct Answer: D
Running the application on Amazon Elastic Kubernetes Service (Amazon EKS) with managed node groups and using Kubernetes deployments for the web servers and application tiers ensures scalability and fault tolerance. Storing the frontend web server session data in Amazon DynamoDB provides a reliable and fast storage solution suited for session persistence, while Amazon Elastic File System (Amazon EFS) provides cross-node shared storage for data needed by the application, which is essential for fault tolerance. This solution leverages managed services to minimize operational overhead.
Question 229 of 529
A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the solutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero downtime.
Which solution will meet these requirements?
A.
Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora.
B.
Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance.
C.
Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance.
D.
Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQL Server DB instance. Reattach the database and then cut over all networking.
AnswerDiscussion
Correct Answer: B
The appropriate solution for migrating Microsoft SQL Server databases to AWS with near-zero downtime is to use AWS Database Migration Service (AWS DMS). AWS DMS supports continuous data replication, capturing ongoing changes in the source database and applying them to the target, which ensures near real-time synchronization. AWS DMS can be configured to use Amazon S3 as an intermediary storage before loading the data into an Amazon RDS for Microsoft SQL Server DB instance. This choice supports the requirement of minimal downtime during the migration process.
Question 230 of 529
A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability Zones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has created multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created multiple service consumer applications in the other organization.
Data transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect must recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the whole environment.
Which guidelines meet these requirements? (Choose two.)
A.
Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization.
B.
Place the service provider applications and the service consumer applications in AWS accounts in the same organization.
C.
Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.
D.
Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.
E.
Create a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage.
AnswerDiscussion
Correct Answer: C, D
To minimize data transfer charges, it is essential to limit the cross-Availability Zone (AZ) traffic since inter-AZ data transfers incur additional costs. Turning off cross-zone load balancing for the Network Load Balancer (NLB) in all service provider application deployments ensures that requests are handled within the same AZ, reducing the data transferred across AZs, thereby lowering costs. Additionally, ensuring that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name directs the traffic within the same AZ, effectively minimizing costly inter-AZ data transfers.
Question 231 of 529
A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move the backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center and AWS.
Which solution meets these requirements MOST cost-effectively?
A.
Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share.
B.
Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.
C.
Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.
D.
Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket.
AnswerDiscussion
Correct Answer: A
The most cost-effective solution for a company to move nightly 200 GB exports of data from a Microsoft SQL Server database to Amazon S3 involves using AWS Storage Gateway with a file gateway. Deploying an AWS Storage Gateway file gateway within the VPC connected to the AWS Direct Connect connection and creating an SMB file share allows direct writing of the nightly database exports to an SMB file share. This avoids additional infrastructure and complexity while ensuring efficient transfer and integration with Amazon S3. Using a file gateway is more suitable for file-level access, making it the most cost-effective and appropriate choice for the company's requirements.
Question 232 of 529
A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are located in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound traffic costs, increase bandwidth throughput, and provide a consistent network experience for end users.
Which solution will meet these requirements?
A.
Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections that initiate from the central VPC to all other VPCs.
B.
Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region.
C.
Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPUse a transit gateway with dynamic routing. Connect the transit gateway to all other VPCs.
D.
Create an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection between all VPCs in each Region. Create VPC peering connections that initiate from the central VPC to all other VPCs.
AnswerDiscussion
Correct Answer: B
The company needs a solution that provides high bandwidth throughput, reduces network outbound traffic costs, and offers a consistent network experience for end users across different AWS Regions. The best solution is to create an AWS Direct Connect connection between the on-premises data center and AWS. By provisioning a transit Virtual Interface (VIF) and connecting it to a Direct Connect gateway, the company can achieve fast, reliable, and cost-efficient communication. The Direct Connect gateway can then be connected to all other VPCs using a transit gateway in each region, offering the required transitive routing capabilities between VPC networks.
Question 233 of 529
A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a separate member account for development and a separate member account for production. Consolidated billing is linked to the management account. In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in both member accounts.
Which solution will meet this requirement?
A.
Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to the member accounts.
B.
Create an IAM user in each member account. In the management account, create a cross-account role that has least privilege access. Grant the IAM users access to the cross-account role by using a trust policy.
C.
Create an IAM user in the management account. In the member accounts, create an IAM group that has least privilege access. Add the IAM user from the management account to each IAM group in the member accounts.
D.
Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy.
AnswerDiscussion
Correct Answer: D
To meet the requirement of allowing an IAM user in the management account to stop or terminate resources in both development and production member accounts, the correct approach is to create the IAM user in the management account. Then, in each member account, create cross-account roles with the necessary permissions for stopping or terminating resources. These roles should have trust policies that grant the necessary access to the IAM user in the management account. This setup ensures that the management account can securely control the resources in the member accounts while adhering to the principle of least privilege.
Question 234 of 529
A company wants to use AWS for disaster recovery for an on-premises application. The company has hundreds of Windows-based servers that run the application. All the servers mount a common share.
The company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support native failover and fallback capabilities.
Which solution will meet these requirements MOST cost-effectively?
A.
Create an AWS Storage Gateway File Gateway. Schedule daily Windows server backups. Save the data to Amazon S3. During a disaster, recover the on-premises servers from the backup. During tailback, run the on-premises servers on Amazon EC2 instances.
B.
Create a set of AWS CloudFormation templates to create infrastructure. Replicate all data to Amazon Elastic File System (Amazon EFS) by using AWS DataSync. During a disaster, use AWS CodePipeline to deploy the templates to restore the on-premises servers. Fail back the data by using DataSync.
C.
Create an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-active environment on AWS. Replicate data into Amazon S3 by using the s3 sync command. During a disaster, swap DNS endpoints to point to AWS. Fail back the data by using the s3 sync command.
D.
Use AWS Elastic Disaster Recovery to replicate the on-premises servers. Replicate data to an Amazon FSx for Windows File Server file system by using AWS DataSync. Mount the file system to AWS servers. During a disaster, fail over the on-premises servers to AWS. Fail back to new or existing servers by using Elastic Disaster Recovery.
AnswerDiscussion
Correct Answer: D
Using AWS Elastic Disaster Recovery to replicate the on-premises servers and replicating data to an Amazon FSx for Windows File Server by using AWS DataSync meets the requirements most cost-effectively. This approach ensures native failover and fallback capabilities while supporting the company's RTO of 15 minutes and RPO of 5 minutes. By mounting the file system to AWS servers and failing over the on-premises servers to AWS during a disaster, the company can maintain operational continuity. Failing back to new or existing servers using Elastic Disaster Recovery ensures a seamless transition back to normal operations.
Question 235 of 529
A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared files stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the cluster size to 1.000 EC2 instances, overall performance was well below expectations.
Which collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose three.)
A.
Ensure the HPC cluster is launched within a single Availability Zone.
B.
Launch the EC2 instances and attach elastic network interfaces in multiples of four.
C.
Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.
D.
Ensure the cluster is launched across multiple Availability Zones.
E.
Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array.
F.
Replace Amazon EFS with Amazon FSx for Lustre.
AnswerDiscussion
Correct Answer: A, C, F
To achieve maximum performance from the HPC cluster, three key design choices are essential: ensuring the HPC cluster is launched within a single Availability Zone to minimize network latency and maximize bandwidth as all instances are in the same location; selecting EC2 instance types with an Elastic Fabric Adapter (EFA) enabled to benefit from low-latency, high-bandwidth communication between instances essential for HPC workloads; and replacing Amazon EFS with Amazon FSx for Lustre, a high-performance file system optimized specifically for HPC workloads, thereby improving performance for the large number of shared files generated by the workload.
Question 236 of 529
A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire organization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique tag values.
Which solution will meet these requirements?
A.
Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.
B.
Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the organization's management account.
C.
Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.
D.
Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs.
AnswerDiscussion
Correct Answer: A
To meet the requirements of standardizing a process for applying tags with specific values unique to each OU, you would use a Service Control Policy (SCP) to deny the creation of resources that do not have the required tags. Then, you create tag policies that include the specific tag values for each OU. By attaching these tag policies to the respective OUs, you ensure each OU adheres to the required tagging strategy, providing granular control and customization without compromising overall standardization.
Question 237 of 529
A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry Transport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket.
Recently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new design on AWS that is highly available and scalable to prevent a similar occurrence.
Which solution will meet these requirements?
A.
Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a domain name in Amazon Route 53. Create a Route 53 failover policy. Route the sensors to send the data to the domain name.
B.
Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer (NLB) that points to the Amazon MSK broker. Enable NLB health checks. Route the sensors to send the data to the NLB.
C.
Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core.
D.
Deploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2 instance. Route the sensors to send the data to AWS IoT Core.
AnswerDiscussion
Correct Answer: C
Deploying AWS IoT Core along with Amazon Kinesis Data Firehose and AWS Lambda for data transformation is the best solution. AWS IoT Core is specifically designed to handle MQTT protocol, making it highly suitable for sensor data. Additionally, this solution provides scalability and high availability, preventing data loss issues experienced with the on-premises Kafka server. The use of AWS managed services significantly reduces the operational overhead and ensures that the system is resilient and efficient in processing and storing data in Amazon S3.
Question 238 of 529
A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances. Amazon Elastic File System (Amazon EFS) file systems, and Amazon RDS DB instances.
To meet regulatory and business requirements, the company must make the following changes for data backups:
• Backups must be retained based on custom daily, weekly, and monthly requirements.
• Backups must be replicated to at least one other AWS Region immediately after capture.
• The backup solution must provide a single source of backup status across the AWS environment.
• The backup solution must send immediate notifications upon failure of any resource backup.
Which combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)
A.
Create an AWS Backup plan with a backup rule for each of the retention requirements.
B.
Configure an AWS Backup plan to copy backups to another Region.
C.
Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.
D.
Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED.
E.
Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements.
F.
Set up RDS snapshots on each database.
AnswerDiscussion
Correct Answer: A, B, D
To meet the requirements, creating an AWS Backup plan with a backup rule for each of the retention requirements ensures backups are managed according to custom daily, weekly, and monthly schedules. Configuring an AWS Backup plan to copy backups to another Region meets the requirement to replicate backups to at least one other AWS Region immediately after capture. Adding an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED ensures immediate notifications upon failure of any resource backup.
Question 239 of 529
A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data from a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the data and provide information back to researchers. The data platform must meet the following requirements:
• Provide near-real-time analytics of the inbound genomic data
• Ensure the data is flexible, parallel, and durable
• Deliver results of processing to a data warehouse
Which strategy should a solutions architect use to meet these requirements?
A.
Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon RDS instance.
B.
Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon Redshift cluster using Amazon EMR.
C.
Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon Redshift cluster.
D.
Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR.
AnswerDiscussion
Correct Answer: B
To meet the requirement of near-real-time analytics, flexibility, parallelism, and durability, Amazon Kinesis Data Streams is an appropriate choice for collecting inbound sensor data. Kinesis Data Streams can handle large amounts of streaming data while providing the necessary scale and parallelism. For the analytics and processing, utilizing Kinesis clients ensures that the heavy lifting is done in real-time. Finally, using Amazon Redshift, which is a powerful data warehousing solution, ensures that the results are stored efficiently and are ready for complex querying and analysis, thereby fulfilling the data warehouse requirement.
Question 240 of 529
A solutions architect needs to define a reference architecture for a solution for three-tier applications with web. application, and NoSQL data layers. The reference architecture must meet the following requirements:
• High availability within an AWS Region
• Able to fail over in 1 minute to another AWS Region for disaster recovery
• Provide the most efficient solution while minimizing the impact on the user experience
Which combination of steps will meet these requirements? (Choose three.)
A.
Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.
B.
Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.
C.
Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.
D.
Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.
E.
Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.
F.
Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources.
AnswerDiscussion
Correct Answer: B, C, E
To meet the requirements of high availability within an AWS Region, the ability to failover in 1 minute to another AWS Region for disaster recovery, and provide the most efficient solution while minimizing user impact, use an Amazon Route 53 failover routing policy set to a Time to Live (TTL) of 30 seconds. This allows quick failover between regions. Implement a global table within Amazon DynamoDB for seamless data access across regions, ensuring data consistency and high availability. Finally, implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions, utilizing zonal Reserved Instances for cost-efficiency and On-Demand Instances for additional resources as needed.
Question 241 of 529
A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to connect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-premises storage. Custom applications analyze this data to detect anomalies.
The number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not able to scale for peak traffic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the scaling challenges.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Use AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache Kafka application to store the data in Amazon S3. Use a pretrained model in Amazon SageMaker to detect anomalies.
B.
Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies.
C.
Use AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use the built-in machine learning transforms in AWS Glue to detect anomalies.
D.
Use Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use Amazon Lookout for Metrics to detect anomalies.
AnswerDiscussion
Correct Answer: B
The most suitable solution involves using AWS IoT Core, as it handles data from IoT devices like the smart vehicles in question and supports the MQTT protocol which the vehicles use. AWS IoT Core can efficiently receive and process this data. By configuring rules to route the data to Amazon Kinesis Data Firehose, data can be efficiently transferred and stored in Amazon S3. For anomaly detection, Amazon Kinesis Data Analytics provides a robust and scalable solution to analyze the data in real-time, which reduces operational overhead compared to custom solutions.
Question 242 of 529
During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.
Which solution will ensure that the credentials are appropriately secured automatically?
A.
Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials
B.
Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS.
C.
Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user.
D.
Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.
AnswerDiscussion
Correct Answer: D
The correct solution is to configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. This approach ensures real-time monitoring and immediate remediation by disabling the credentials in AWS IAM and notifying the user, thereby preventing the credentials from being exposed in the repository.
Question 243 of 529
A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's information security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the minimum permissions necessary to function.
To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.
Which combination of steps should the solutions architect take to implement this solution? (Choose two.)
A.
Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application’s VPC. Update the bucket policy to require access from an access point.
B.
Create an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint.
C.
Create a gateway endpoint for Amazon S3 in each application's VPConfigure the endpoint policy to allow access to an S3 access point. Specify the route table that is used to access the access point.
D.
Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point.
E.
Create a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket.
AnswerDiscussion
Correct Answer: A, C
To securely grant access to the Amazon S3 data lake from various applications across multiple AWS accounts, two key steps need to be taken. First, create an S3 access point for each application in the AWS account that owns the S3 bucket, and configure these access points to be accessible only from the application’s VPC. This ensures that the access is restricted and permissions are minimized. Second, create a gateway endpoint for Amazon S3 in each application’s VPC and configure the endpoint policy to allow access to the S3 access points. Specifying the route table helps in managing the traffic flow securely and ensures adherence to the company's security policies by keeping access over the AWS internal network and not through the public internet.
Question 244 of 529
A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that send application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.
The company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring solution that uses Splunk on premises. A solutions architect needs to determine how to send networking traffic to Splunk.
How should the solutions architect meet these requirements?
A.
Enable VPC flows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an Amazon S3 bucket by using the pre-defined export function. Generate ACCESS_KEY and SECRET_KEY AWS credentials. Configure Splunk to pull the logs from the S3 bucket by using those credentials.
B.
Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function with a Kinesis Data Firehose stream processor that extracts individual log events from records sent by CloudWatch Logs subscription filters. Enable VPC flows logs, and send them to CloudWatch. Create a CloudWatch Logs subscription that sends log events to the Kinesis Data Firehose delivery stream.
C.
Ask the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs grouped by database name. Export Athena results to another S3 bucket. Invoke an AWS Lambda function to automatically send any new file that is put in the S3 bucket to Splunk.
D.
Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1-minute sliding window to collect the events. Create a SQL query that uses the anomaly detection template to monitor any networking traffic anomalies in near-real time. Send the result to an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination.
AnswerDiscussion
Correct Answer: B
To achieve near-real-time monitoring of networking traffic from Amazon EC2 instances to on-premises databases, the best solution is to use an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. This setup allows for the integration of VPC flow logs and CloudWatch Logs, which can be processed using an AWS Lambda function for preprocessing. The Kinesis Data Firehose delivery stream is effectively designed for near-real-time log delivery, especially when analyzing network traffic. This solution is optimal because it leverages existing AWS services for log processing and delivery to Splunk in near-real-time, minimizing complexity and ensuring efficient data handling.
Question 245 of 529
A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the development teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide the information to the company's finance team.
The company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States. However, some resources have been created in other Regions.
A solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the accounts. The solution also must ensure that the company can create resources only in Regions in the United States.
Which combination of steps will meet these requirements in the MOST operationally efficient way? (Choose three.)
A.
Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team's S3 bucket.
B.
Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation.
C.
Create an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the United States. Apply the SCP to the OU.
D.
Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU.
E.
Create an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role. Use AWS Cost Explorer and the Billing and Cost Management console to analyze cost.
F.
Create an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role.
AnswerDiscussion
Correct Answer: B, D, E
To meet the requirements efficiently, first, create a new account to serve as a management account and deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization to centralize management. Then, create an OU that includes all the development teams and apply an SCP that explicitly denies the creation of resources in Regions outside the United States. Finally, create an IAM role in the management account with permissions to view the Billing and Cost Management console and allow the finance team users to assume this role to analyze costs using AWS Cost Explorer and the Billing and Cost Management console.
Question 246 of 529
A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires read-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security team.
How should a solutions architect meet these requirements?
A.
Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a trust relationship between the IAM policy in each member account and the security account. Ask the security team to use the IAM policy to gain access.
B.
Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.
C.
Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the management account from the security account. Use the generated temporary credentials to gain access.
D.
Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the member account from the security account. Use the generated temporary credentials to gain access.
AnswerDiscussion
Correct Answer: B
To meet the requirements, the solutions architect should leverage the OrganizationAccountAccessRole to create a new IAM role with read-only access in each member account. This IAM role will allow the security team to view resources without making any modifications. By establishing a trust relationship between the read-only role in each member account and the security account, the security team can assume the role and gain read-only access. This approach follows AWS best practices for cross-account access by delegating permissions through IAM roles, ensuring the security team has the necessary visibility without excessive privileges.
Question 247 of 529
A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private subnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.
A solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.
Which set of additional steps should the solutions architect take to meet these requirements?
A.
Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.
B.
Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet.
C.
Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet.
D.
Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.
AnswerDiscussion
Correct Answer: B
With hundreds of VPCs in multiple AWS accounts, the most efficient way to manage connectivity is by using a transit gateway. A transit gateway acts as a central hub that simplifies and consolidates VPC peering connections in a hub-and-spoke architecture, allowing all VPCs to communicate with each other and route traffic through the central egress VPC with the NAT gateway for internet access. This eliminates the need for creating and managing numerous peering connections, and it scales well with a large number of VPCs and accounts.
Question 248 of 529
An education company is running a web application used by college students around the world. The application runs in an Amazon Elastic Container Service (Amazon ECS) cluster in an Auto Scaling group behind an Application Load Balancer (ALB). A system administrator detects a weekly spike in the number of failed login attempts, which overwhelm the application's authentication service. All the failed login attempts originate from about 500 different IP addresses that change each week. A solutions architect must prevent the failed login attempts from overwhelming the authentication service.
Which solution meets these requirements with the MOST operational efficiency?
A.
Use AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses.
B.
Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB.
C.
Use AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges.
D.
Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB.
AnswerDiscussion
Correct Answer: B
The most operationally efficient solution to prevent the failed login attempts from overwhelming the authentication service is to create an AWS WAF web ACL with a rate-based rule and set the rule action to Block. The rate-based rule allows you to monitor the rate of requests from different IP addresses and block those that exceed a threshold. This is ideal for handling scenarios with a large number of changing IP addresses, as it can dynamically adjust to patterns of attack without the need for manual updates. Connecting the web ACL to the Application Load Balancer (ALB) ensures that the traffic is blocked before reaching the application, thereby reducing the load on the authentication service.
Question 249 of 529
A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public SFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for outbound traffic. Changes to the SFTP endpoint IP addresses are not permitted.
The company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.
Which solution meets these requirements?
A.
Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWS Transfer to store the files in Amazon S3.
B.
Add a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign them to an Application Load Balancer (ALB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the ALStore the files in attached Amazon Elastic Block Store (Amazon EBS) volumes.
C.
Register the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load Balancer (NLB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the NLB. Store the files in Amazon S3.
D.
Register the customer-owned block of IP addresses in the company’s AWS account. Create Elastic IP addresses from the address pool and assign them to an Amazon S3 VPC endpoint. Enable SFTP support on the S3 bucket.
AnswerDiscussion
Correct Answer: A
To meet the requirements, the company should register the customer-owned block of IP addresses in their AWS account, create Elastic IP addresses from this address pool, and assign them to an AWS Transfer for SFTP endpoint. AWS Transfer for SFTP is a fully managed service that allows for secure file transfers directly into Amazon S3, significantly reducing operational overhead. This approach enables customers to continue using their existing firewall allow lists without needing changes, ensuring a seamless migration to the cloud.
Question 250 of 529
A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high-throughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the application to be fault tolerant.
Which solution will meet these requirements?
A.
Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.
B.
Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each EC2 instance.
C.
Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking.
D.
Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance.
AnswerDiscussion
Correct Answer: A
To achieve high-throughput, low-latency network connections between all instances, the best option is to launch the EC2 instances into a cluster placement group. Cluster placement groups are designed to provide the maximum possible network performance between instances. Ensuring the EC2 instance type supports enhanced networking further improves network performance. Other options, like Auto Scaling groups, partition placement groups, or spread placement groups, do not specifically target high-throughput, low-latency network requirements.
Question 251 of 529
A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon API Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.
After initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The company believes this traffic is originating from a botnet and wants to secure its API while minimizing cost.
Which approach should the company take to secure its API?
A.
Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Configure CloudFront with an origin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only the OAI can run the POST method.
B.
Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Add a custom header to the CloudFront distribution populated with an API key. Configure the API to require an API key on the POST method.
C.
Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a resource policy with a request limit and associate it with the API. Configure the API to require an API key on the POST method.
D.
Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan.
AnswerDiscussion
Correct Answer: D
The company should create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners and associate the web ACL with the API. Additionally, creating a usage plan with a request limit and associating it with the API will help manage and restrict usage, preventing overuse of resources. The usage plan provides throttling and quotas to control the rate of requests, while API keys enable tracking and controlling access. This combined approach helps secure the API against unauthorized access and botnet traffic, ensuring only the six partners can access it with minimal cost.
Question 252 of 529
A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor all data activity on all the databases.
Which solution will achieve this goal?
A.
Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch Service cluster for further analysis.
B.
Start a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program the Lambda function to decrypt the messages from EventBridge and to publish all database activity to Amazon S3 for further analysis.
C.
Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis.
D.
Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. Run queries on the Amazon Redshift data to determine database activities on the Aurora database.
AnswerDiscussion
Correct Answer: C
To monitor all data activity on all the databases in an Amazon Aurora PostgreSQL DB cluster, starting a database activity stream to push the activity stream to an Amazon Kinesis data stream is the most appropriate solution. This stream can then be consumed by Amazon Kinesis Data Firehose and delivered to Amazon S3 for further analysis. This solution leverages native AWS integration and provides a straightforward method to assess database activity. Utilizing Kinesis Data Firehose allows for seamless data delivery and storage, facilitating efficient monitoring and analysis of database activity.
Question 253 of 529
An entertainment company recently launched a new game. To ensure a good experience for players during the launch period, the company deployed a static quantity of 12 r6g.16xlarge (memory optimized) Amazon EC2 instances behind a Network Load Balancer. The company's operations team used the Amazon CloudWatch agent and a custom metric to include memory utilization in its monitoring strategy.
Analysis of the CloudWatch metrics from the launch period showed consumption at about one quarter of the CPU and memory that the company expected. Initial demand for the game has subsided and has become more variable. The company decides to use an Auto Scaling group that monitors the CPU and memory consumption to dynamically scale the instance fleet. A solutions architect needs to configure the Auto Scaling group to meet demand in the most cost-effective way.
Which solution will meet these requirements?
A.
Configure the Auto Scaling group to deploy c6g.4xlarge (compute optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.
B.
Configure the Auto Scaling group to deploy m6g.4xlarge (general purpose) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.
C.
Configure the Auto Scaling group to deploy r6g.4xlarge (memory optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.
D.
Configure the Auto Scaling group to deploy r6g.8xlarge (memory optimized) instances. Configure a minimum capacity of 2, a desired capacity of 2, and a maximum capacity of 6.
AnswerDiscussion
Correct Answer: C
The entertainment company initially deployed 12 r6g.16xlarge instances, which are memory optimized, but found that only about one quarter of the CPU and memory was being utilized. To fulfill the requirements in a cost-effective manner while still keeping the same performance characteristics, it makes sense to switch to smaller instances. The r6g.4xlarge instances are a quarter of the size of the r6g.16xlarge instances, which aligns with the observed usage pattern. Configuring the Auto Scaling group with a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12 using r6g.4xlarge instances will allow the system to scale efficiently and cost-effectively based on fluctuating demand. This configuration ensures efficient use of resources without over-provisioning or incurring unnecessary costs.
Question 254 of 529
A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity mode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts throughout the day. and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.
Which strategy should a solutions architect recommend to meet this requirement?
A.
Deploy an Amazon ElastiCache cluster in front of the DynamoDB table
B.
Deploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer.
C.
Use provisioned capacity mode. Purchase Savings Plans in Cost Explorer.
D.
Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling.
AnswerDiscussion
Correct Answer: D
To reduce costs while handling frequent and burst read activities, deploying DynamoDB Accelerator (DAX) is appropriate, as it provides an in-memory cache that significantly reduces read latency and improves throughput for frequently accessed data. Using provisioned capacity mode allows for cost control by setting a predictable capacity rather than paying for on-demand, which is usually more expensive. Auto scaling further optimizes costs by adjusting the provisioned capacity based on the actual workload, ensuring resources are allocated efficiently without over-provisioning.
Question 255 of 529
A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts. AWS PrivateLink is being used to provide connectivity between the client services and the logging service.
In each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC endpoint.
Which combination of steps should a solutions architect take to resolve this issue? (Choose two.)
A.
Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances.
B.
Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances.
C.
Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets.
D.
Check the security group for the logging service running on EC2 instances to ensure it allows ingress from the clients.
E.
Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets.
AnswerDiscussion
Correct Answer: B, E
To resolve the issue, it is essential to ensure proper communication between the interface endpoint subnets and the logging service subnets running on EC2 instances. First, the Network Access Control Lists (NACLs) must be configured correctly. This involves checking that the NACL attached to the logging service subnets allows communications to and from the interface endpoint subnets, and vice versa. Additionally, since the clients are unable to submit logs through the VPC endpoint, it is crucial to verify that the security group for the Network Load Balancer (NLB) permits ingress from the interface endpoint subnets. By setting these configurations, you ensure that the necessary traffic can flow between the endpoints and the logging service.
Question 256 of 529
A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed frequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side encryption with AWS KMS keys (SSE-KMS).
A solutions architect reviews the company’s monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of requests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C.
B.
Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Specify SSE-S3.
C.
Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM.
D.
Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive.
AnswerDiscussion
Correct Answer: B
The best solution to reduce AWS KMS costs involves moving from SSE-KMS, which incurs charges per API request, to SSE-S3, which does not have additional per-request costs. By creating a new S3 bucket with SSE-S3 and using S3 Batch Operations to copy the existing objects to this new bucket, the company can significantly reduce costs while maintaining server-side encryption managed by Amazon S3. This approach requires minimal changes to the application and has low operational overhead.
Question 257 of 529
A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions. Application state is stored in Amazon DynamoDB tables. Users are reporting that some uploaded photos are not being processed properly. The application developers trace the logs and find that Lambda is experiencing photo processing issues when thousands of users upload photos simultaneously. The issues are the result of Lambda concurrency limits and the performance of DynamoDB when data is saved.
Which combination of actions should a solutions architect take to increase the performance and reliability of the application? (Choose two.)
A.
Evaluate and adjust the RCUs for the DynamoDB tables.
B.
Evaluate and adjust the WCUs for the DynamoDB tables.
C.
Add an Amazon ElastiCache layer to increase the performance of Lambda functions.
D.
Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions.
E.
Use S3 Transfer Acceleration to provide lower latency to users.
AnswerDiscussion
Correct Answer: B, D
To address the issues related to AWS Lambda concurrency limits and the performance of DynamoDB when data is saved, two actions should be taken. First, evaluating and adjusting the write capacity units (WCUs) for the DynamoDB tables is necessary to enhance performance during data saves, which are crucial when thousands of users are uploading photos simultaneously. Second, adding an Amazon Simple Queue Service (Amazon SQS) queue and incorporating reprocessing logic between Amazon S3 and the Lambda functions will help decouple the upload and processing stages. This setup prevents the Lambda functions from being overwhelmed, ensuring that photos are processed reliably and efficiently.
Question 258 of 529
A company runs an application in an on-premises data center. The application gives users the ability to upload media files. The files persist in a file server. The web application has many users. The application server is overutilized, which causes data uploads to fail occasionally. The company frequently adds new storage to the file server. The company wants to resolve these challenges by migrating the application to AWS.
Users from across the United States and Canada access the application. Only authenticated users should have the ability to access the application to upload files. The company will consider a solution that refactors the application, and the company needs to accelerate application development.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Modify the application to use Amazon S3 to persist the files. Use Amazon Cognito to authenticate users.
B.
Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Set up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to the application. Modify the application to use Amazon S3 to persist the files.
C.
Create a static website for uploads of media files. Store the static assets in Amazon S3. Use AWS AppSync to create an API. Use AWS Lambda resolvers to upload the media files to Amazon S3. Use Amazon Cognito to authenticate users.
D.
Use AWS Amplify to create a static website for uploads of media files. Use Amplify Hosting to serve the website through Amazon CloudFront. Use Amazon S3 to store the uploaded media files. Use Amazon Cognito to authenticate users.
AnswerDiscussion
Correct Answer: D
The solution described in option D leverages AWS Amplify to create a serverless and scalable architecture for media file uploads. AWS Amplify simplifies the development and deployment process, which accelerates application development. Amplify Hosting, combined with Amazon CloudFront, ensures low-latency content delivery, which is essential for users across the United States and Canada. Additionally, Amazon S3 provides scalable storage for media files, and Amazon Cognito handles user authentication, ensuring security. This comprehensive solution minimizes operational overhead, addressing the company's requirements efficiently.
Question 259 of 529
A company has an application that is deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are part of an Auto Scaling group. The application has unpredictable workloads and frequently scales out and in. The company’s development team wants to analyze application logs to find ways to improve the application's performance. However, the logs are no longer available after instances scale in.
Which solution will give the development team the ability to view the application logs after a scale-in event?
A.
Enable access logs for the ALB. Store the logs in an Amazon S3 bucket.
B.
Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent.
C.
Modify the Auto Scaling group to use a step scaling policy.
D.
Instrument the application with AWS X-Ray tracing.
AnswerDiscussion
Correct Answer: B
To retain application logs even after EC2 instances are scaled in, you should configure the EC2 instances to publish logs to Amazon CloudWatch Logs using the unified CloudWatch agent. This approach ensures that logs are centrally stored and accessible for analysis independent of the lifecycle of individual instances. Neither enabling access logs on the ALB, changing the Auto Scaling policies, nor using AWS X-Ray specifically addresses the requirement to retain application logs after the instances scale in.
Question 260 of 529
A company runs an unauthenticated static website (www.example.com) that includes a registration form for users. The website uses Amazon S3 for hosting and uses Amazon CloudFront as the content delivery network with AWS WAF configured. When the registration form is submitted, the website calls an Amazon API Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward the payload to an external API call.
During testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutions architect confirms that the CloudFront distribution origin has the Access-Control-Allow-Origin header set to www.example.com.
What should the solutions architect do to resolve the error?
A.
Change the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com.
B.
Enable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com.
C.
Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com.
D.
Enable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header set to www.example.com.
AnswerDiscussion
Correct Answer: C
The correct action to resolve the Cross-Origin Resource Sharing (CORS) error is to enable the CORS setting on the API Gateway API endpoint. The error occurs because the API response is lacking the Access-Control-Allow-Origin header, which specifies the allowed origin domains. By configuring the API Gateway to return responses with the Access-Control-Allow-Origin header set to www.example.com, the browser will recognize the server's approval for cross-origin requests from this domain, thus resolving the CORS issue.
Question 261 of 529
A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different departments in the company. The company has a Microsoft Azure Active Directory that is deployed.
A solutions architect needs to centralize billing and management of the company’s AWS accounts. The company wants to start using identity federation instead of manual user management. The company also wants to use temporary credentials instead of long-lived access keys.
Which combination of steps will meet these requirements? (Choose three.)
A.
Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS account to join the organization. Ensure that each account accepts the invitation.
B.
Configure each AWS account's email address to be aws+@example.com so that account management email messages and invoices are sent to the same place.
C.
Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronization of users and groups.
D.
Deploy an AWS Managed Microsoft AD directory in the management account. Share the directory with all other accounts in the organization by using AWS Resource Access Manager (AWS RAM).
E.
Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts.
F.
Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and authorization.
AnswerDiscussion
Correct Answer: A, C, E
To centralize billing and management of the company's AWS accounts and start using identity federation with temporary credentials, creating a new management account and deploying an organization in AWS Organizations is necessary. This allows all existing accounts to be brought under centralized management. Deploying AWS IAM Identity Center (AWS Single Sign-On) in the management account and connecting it to the company's Azure Active Directory enables federated access using existing Azure AD identities. Lastly, creating permission sets in AWS IAM Identity Center (AWS Single Sign-On) and attaching them to appropriate groups and AWS accounts ensures proper access controls and permissions management. Using these steps meets the requirements of centralizing billing, management, and implementing identity federation with temporary credentials.
Question 262 of 529
A company wants to manage the costs associated with a group of 20 applications that are infrequently used, but are still business-critical, by migrating to AWS. The applications are a mix of Java and Node.js spread across different instance clusters. The company wants to minimize costs while standardizing by using a single deployment methodology.
Most of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other times. Average application memory consumption is less than 1 GB. though some applications use as much as 2.5 GB of memory during peak processing. The most important application in the group is a billing report written in Java that accesses multiple data sources and often runs for several hours.
Which is the MOST cost-effective solution?
A.
Deploy a separate AWS Lambda function for each application. Use AWS CloudTrail logs and Amazon CloudWatch alarms to verify completion of critical jobs.
B.
Deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. Deploy an ECS task for each application being migrated with ECS task scaling. Monitor services and hosts by using Amazon CloudWatch.
C.
Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have sufficient resources. Monitor each AWS Elastic Beanstalk deployment by using CloudWatch alarms.
D.
Deploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling and Application Load Balancers. Scale cluster size based on a custom metric set on instance memory utilization. Purchase 3-year Reserved Instance reservations equal to the GroupMaxSize parameter of the Auto Scaling group.
AnswerDiscussion
Correct Answer: B
To manage the costs associated with infrequently used business-critical applications while minimizing costs and standardizing deployment, deploying Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75% is the most cost-effective solution. ECS allows for efficient container management, which reduces overhead and can optimize resource usage. Auto Scaling will ensure that the applications have the required resources during their peak usage times without over-provisioning. Also, containerization is more granular and cost-effective compared to provisioning dedicated instances for each application.
Question 263 of 529
A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs tasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, primary, and core nodes. The EMR tasks run each morning, starting at 1:00 AM. and take 6 hours to finish running. The amount of time to complete the processing is not a priority because the data is not referenced until late in the day.
The solutions architect must review the architecture and suggest a solution to minimize the compute costs.
Which solution should the solutions architect recommend to meet these requirements?
A.
Launch all task, primary, and core nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed.
B.
Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.
C.
Continue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.
D.
Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate only the task node instances when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.
AnswerDiscussion
Correct Answer: B
To minimize compute costs in an Amazon EMR cluster, it makes the most sense to use a mix of On-Demand and Spot Instances. On-Demand Instances should be used for primary and core nodes to ensure stability and availability, while less critical task nodes can be launched on Spot Instances to save costs. Once the processing tasks are completed, terminating the entire cluster can maximize cost savings since the data is stored in Amazon S3 and can be accessed later without needing to keep the cluster running. Additionally, purchasing Compute Savings Plans can further reduce costs for the On-Demand Instances. This approach maintains the balance between cost savings and operational efficiency.
Question 264 of 529
A company has migrated a legacy application to the AWS Cloud. The application runs on three Amazon EC2 instances that are spread across three Availability Zones. One EC2 instance is in each Availability Zone. The EC2 instances are running in three private subnets of the VPC and are set up as targets for an Application Load Balancer (ALB) that is associated with three public subnets.
The application needs to communicate with on-premises systems. Only traffic from IP addresses in the company's IP address range are allowed to access the on-premises systems. The company’s security team is bringing only one IP address from its internal IP address range to the cloud. The company has added this IP address to the allow list for the company firewall. The company also has created an Elastic IP address for this IP address.
A solutions architect needs to create a solution that gives the application the ability to communicate with the on-premises systems. The solution also must be able to mitigate failures automatically.
Which solution will meet these requirements?
A.
Deploy three NAT gateways, one in each public subnet. Assign the Elastic IP address to the NAT gateways. Turn on health checks for the NAT gateways. If a NAT gateway fails a health check, recreate the NAT gateway and assign the Elastic IP address to the new NAT gateway.
B.
Replace the ALB with a Network Load Balancer (NLB). Assign the Elastic IP address to the NLTurn on health checks for the NLIn the case of a failed health check, redeploy the NLB in different subnets.
C.
Deploy a single NAT gateway in a public subnet. Assign the Elastic IP address to the NAT gateway. Use Amazon CloudWatch with a custom metric to monitor the NAT gateway. If the NAT gateway is unhealthy, invoke an AWS Lambda function to create a new NAT gateway in a different subnet. Assign the Elastic IP address to the new NAT gateway.
D.
Assign the Elastic IP address to the ALB. Create an Amazon Route 53 simple record with the Elastic IP address as the value. Create a Route 53 health check. In the case of a failed health check, recreate the ALB in different subnets.
AnswerDiscussion
Correct Answer: C
To enable communication between the application running on EC2 instances in private subnets and the on-premises systems, a single NAT gateway should be deployed in a public subnet. The Elastic IP address will be assigned to the NAT gateway, allowing all traffic from the private subnets to appear as coming from the company's public IP address. Monitoring the health of this NAT gateway using Amazon CloudWatch ensures that if it becomes unhealthy, a failover process can be triggered. This involves an AWS Lambda function to create a new NAT gateway in a different subnet and reassign the Elastic IP address, thereby ensuring continuous communication and mitigating failures automatically. This solution meets the requirements by providing a single point of egress with automatic health checks and failover capability.
Question 265 of 529
A company uses AWS Organizations to manage more than 1,000 AWS accounts. The company has created a new developer organization. There are 540 developer member accounts that must be moved to the new developer organization. All accounts are set up with all the required information so that each account can be operated as a standalone account.
Which combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Choose three.)
A.
Call the MoveAccount operation in the Organizations API from the old organization's management account to migrate the developer accounts to the new developer organization.
B.
From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.
C.
From each developer account, remove the account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.
D.
Sign in to the new developer organization's management account and create a placeholder member account that acts as a target for the developer account migration.
E.
Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to send invitations to the developer accounts.
F.
Have each developer sign in to their account and confirm to join the new developer organization.
AnswerDiscussion
Correct Answer: B, E, F
To move the developer accounts to the new developer organization, the steps should be as follows: First, from the management account of the old organization, remove each developer account using the RemoveAccountFromOrganization operation. Next, from the new developer organization’s management account, call the InviteAccountToOrganization operation to send invitations to the developer accounts. Finally, have each developer sign in to their account and confirm the invitation to join the new developer organization. These actions collectively ensure the transfer of accounts between organizations. The options that support these actions are removing the accounts, sending invitations, and confirming the invitations, making B, E, and F the correct choices.
Question 266 of 529
A company’s interactive web application uses an Amazon CloudFront distribution to serve images from an Amazon S3 bucket. Occasionally, third-party tools ingest corrupted images into the S3 bucket. This image corruption causes a poor user experience in the application later. The company has successfully implemented and tested Python logic to detect corrupt images.
A solutions architect must recommend a solution to integrate the detection logic with minimal latency between the ingestion and serving.
Which solution will meet these requirements?
A.
Use a Lambda@Edge function that is invoked by a viewer-response event.
B.
Use a Lambda@Edge function that is invoked by an origin-response event.
C.
Use an S3 event notification that invokes an AWS Lambda function.
D.
Use an S3 event notification that invokes an AWS Step Functions state machine.
AnswerDiscussion
Correct Answer: C
To ensure minimal latency between the ingestion and serving of images while detecting corrupted images, the solution should trigger the detection logic as soon as a new image is uploaded to the S3 bucket. Using an S3 event notification to invoke an AWS Lambda function effectively integrates the detection logic right at the point of ingestion. This approach allows immediate detection and handling of corrupted images, preventing them from causing a poor user experience later. Options A and B, which involve Lambda@Edge invoked by viewer-response or origin-response events, would introduce more latency and detect the issue later in the process. Option D, involving AWS Step Functions, would add unnecessary complexity for this use case.
Question 267 of 529
A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group. The company uses AWS CodePipeline to deploy the application. The instances that run in the Auto Scaling group are constantly changing because of scaling events.
When the company deploys new application code versions, the company installs the AWS CodeDeploy agent on any new target EC2 instances and associates the instances with the CodeDeploy deployment group. The application is set to go live within the next 24 hours.
What should a solutions architect recommend to automate the application deployment process with the LEAST amount of operational overhead?
A.
Configure Amazon EventBridge to invoke an AWS Lambda function when a new EC2 instance is launched into the Auto Scaling group. Code the Lambda function to associate the EC2 instances with the CodeDeploy deployment group.
B.
Write a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new code. When the deployment is complete, create a new AMI and configure the Auto Scaling group's launch template to use the new AMI for new launches. Resume Amazon EC2 Auto Scaling operations.
C.
Create a new AWS CodeBuild project that creates a new AMI that contains the new code. Configure CodeBuild to update the Auto Scaling group’s launch template to the new AMI. Run an Amazon EC2 Auto Scaling instance refresh operation.
D.
Create a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group’s launch template to use the new AMI. Associate the CodeDeploy deployment group with the Auto Scaling group instead of the EC2 instances.
AnswerDiscussion
Correct Answer: D
To automate the application deployment process with the least operational overhead, the best recommendation is to create a new Amazon Machine Image (AMI) that includes the AWS CodeDeploy agent pre-installed. The Auto Scaling group's launch template should be updated to use this new AMI. By doing so, any new instances launched by the Auto Scaling group will already have the CodeDeploy agent installed and can be automatically associated with the CodeDeploy deployment group. This eliminates the need to manually install the CodeDeploy agent on each new instance and ensures a seamless and automated deployment process.
Question 268 of 529
A company has a website that runs on four Amazon EC2 instances that are behind an Application Load Balancer (ALB). When the ALB detects that an EC2 instance is no longer available, an Amazon CloudWatch alarm enters the ALARM state. A member of the company's operations team then manually adds a new EC2 instance behind the ALB.
A solutions architect needs to design a highly available solution that automatically handles the replacement of EC2 instances. The company needs to minimize downtime during the switch to the new solution.
Which set of steps should the solutions architect take to meet these requirements?
A.
Delete the existing ALB. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Attach the existing EC2 instances to the Auto Scaling group.
B.
Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALAttach the existing EC2 instances to the Auto Scaling group.
C.
Delete the existing ALB and the EC2 instances. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Wait for the Auto Scaling group to launch the minimum number of EC2 instances.
D.
Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Wait for the existing ALB to register the existing EC2 instances with the Auto Scaling group.
AnswerDiscussion
Correct Answer: B
To design a highly available solution that automatically handles the replacement of EC2 instances while minimizing downtime, the solutions architect should create an Auto Scaling group (ASG) configured to handle the web application traffic. Attach a new launch template to the ASG and then attach the ASG to the existing Application Load Balancer (ALB). The existing EC2 instances should be attached to the ASG to ensure they are managed by the ASG, which will automatically handle the replacement of any failing instances based on health checks. This approach leverages the existing infrastructure (the ALB and the running EC2 instances) and enhances it with the ASG’s capabilities, providing a seamless and automated scaling solution without the need to delete or recreate resources, thus ensuring minimal downtime.
Question 269 of 529
A company wants to optimize AWS data-transfer costs and compute costs across developer accounts within the company's organization in AWS Organizations. Developers can configure VPCs and launch Amazon EC2 instances in a single AWS Region. The EC2 instances retrieve approximately 1 TB of data each day from Amazon S3.
The developer activity leads to excessive monthly data-transfer charges and NAT gateway processing charges between EC2 instances and S3 buckets, along with high compute costs. The company wants to proactively enforce approved architectural patterns for any EC2 instance and VPC infrastructure that developers deploy within the AWS accounts. The company does not want this enforcement to negatively affect the speed at which the developers can perform their tasks.
Which solution will meet these requirements MOST cost-effectively?
A.
Create SCPs to prevent developers from launching unapproved EC2 instance types. Provide the developers with an AWS CloudFormation template to deploy an approved VPC configuration with S3 interface endpoints. Scope the developers' IAM permissions so that the developers can launch VPC resources only with CloudFormation.
B.
Create a daily forecasted budget with AWS Budgets to monitor EC2 compute costs and S3 data-transfer costs across the developer accounts. When the forecasted cost is 75% of the actual budget cost, send an alert to the developer teams. If the actual budget cost is 100%, create a budget action to terminate the developers' EC2 instances and VPC infrastructure.
C.
Create an AWS Service Catalog portfolio that users can use to create an approved VPC configuration with S3 gateway endpoints and approved EC2 instances. Share the portfolio with the developer accounts. Configure an AWS Service Catalog launch constraint to use an approved IAM role. Scope the developers' IAM permissions to allow access only to AWS Service Catalog.
D.
Create and deploy AWS Config rules to monitor the compliance of EC2 and VPC resources in the developer AWS accounts. If developers launch unapproved EC2 instances or if developers create VPCs without S3 gateway endpoints, perform a remediation action to terminate the unapproved resources.
AnswerDiscussion
Correct Answer: C
The best solution to meet the company's requirements is to create an AWS Service Catalog portfolio. This portfolio can be configured to allow developers to create VPC configurations that include S3 gateway endpoints and approved EC2 instances, ensuring cost-effectiveness and adherence to company architectural patterns. Sharing the portfolio with the developer accounts and setting up launch constraints using an approved IAM role allows for governance while maintaining developer productivity. By scoping developers' IAM permissions to allow access only to AWS Service Catalog, the company can enforce compliance without hindering the speed at which developers can perform their tasks.
Question 270 of 529
A company is expanding. The company plans to separate its resources into hundreds of different AWS accounts in multiple AWS Regions. A solutions architect must recommend a solution that denies access to any operations outside of specifically designated Regions.
Which solution will meet these requirements?
A.
Create IAM roles for each account. Create IAM policies with conditional allow permissions that include only approved Regions for the accounts.
B.
Create an organization in AWS Organizations. Create IAM users for each account. Attach a policy to each user to block access to Regions where an account cannot deploy infrastructure.
C.
Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions.
D.
Enable AWS Security Hub in each account. Create controls to specify the Regions where an account can deploy infrastructure.
AnswerDiscussion
Correct Answer: C
The best solution for ensuring that accounts cannot perform operations outside of designated AWS Regions is to utilize AWS Control Tower along with Service Control Policies (SCPs). By launching an AWS Control Tower landing zone and creating Organizational Units (OUs), SCPs can be applied to these OUs to enforce restrictions on running services outside of approved Regions. This method centralizes access control management and scales efficiently across multiple accounts, ensuring consistent compliance with the company's requirements.
Question 271 of 529
A company wants to refactor its retail ordering web application that currently has a load-balanced Amazon EC2 instance fleet for web hosting, database API services, and business logic. The company needs to create a decoupled, scalable architecture with a mechanism for retaining failed orders while also minimizing operational costs.
Which solution will meet these requirements?
A.
Use Amazon S3 for web hosting with Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic Container Service (Amazon ECS) for business logic with Amazon SQS long polling for retaining failed orders.
B.
Use AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API services. Use Amazon MQ for order queuing. Use AWS Step Functions for business logic with Amazon S3 Glacier Deep Archive for retaining failed orders.
C.
Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an Amazon SQS dead-letter queue for retaining failed orders.
D.
Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Amazon Simple Email Service (Amazon SES) for order queuing. Use Amazon Elastic Kubernetes Service (Amazon EKS) for business logic with Amazon OpenSearch Service for retaining failed orders.
AnswerDiscussion
Correct Answer: C
Question 272 of 529
A company hosts a web application on AWS in the us-east-1 Region. The application servers are distributed across three Availability Zones behind an Application Load Balancer. The database is hosted in a MySQL database on an Amazon EC2 instance. A solutions architect needs to design a cross-Region data recovery solution using AWS services with an RTO of less than 5 minutes and an RPO of less than 1 minute. The solutions architect is deploying application servers in us-west-2, and has configured Amazon Route 53 health checks and DNS failover to us-west-2.
Which additional step should the solutions architect take?
A.
Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica in us-west-2.
B.
Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2.
C.
Migrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment.
D.
Create a MySQL standby database on an Amazon EC2 instance in us-west-2.
AnswerDiscussion
Correct Answer: B
To achieve an RTO of less than 5 minutes and an RPO of less than 1 minute, it is essential to use a database solution that allows for rapid failover and minimal data loss. Migrating the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2 effectively addresses these requirements. Aurora's global database feature allows for sub-second replication lag and the ability to promote the secondary region to primary in less than a minute, thus meeting both RTO and RPO requirements.
Question 273 of 529
A company is using AWS Organizations to manage multiple accounts. Due to regulatory requirements, the company wants to restrict specific member accounts to certain AWS Regions, where they are permitted to deploy resources. The resources in the accounts must be tagged, enforced based on a group standard, and centrally managed with minimal configuration.
What should a solutions architect do to meet these requirements?
A.
Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy.
B.
From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and apply a tag policy on the root.
C.
Associate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions.
D.
Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions.
AnswerDiscussion
Correct Answer: D
To restrict specific member accounts to certain AWS Regions and enforce tagging policies, associating the member accounts with a new Organizational Unit (OU) and applying a Service Control Policy (SCP) with conditions to limit Regions is the most effective approach. An SCP can deny or allow actions based on conditions, and when linked to an OU, it can enforce policies consistently across all accounts within that OU. Additionally, a tag policy can be applied to ensure resources are tagged according to the group standards. This method allows for centralized management with minimal configuration.
Question 274 of 529
A company has an application that generates reports and stores them in an Amazon S3 bucket. When a user accesses their report, the application generates a signed URL to allow the user to download the report. The company's security team has discovered that the files are public and that anyone can download them without authentication. The company has suspended the generation of new reports until the problem is resolved.
Which set of actions will immediately remediate the security issue without impacting the application's normal workflow?
A.
Create an AWS Lambda function that applies a deny all policy for users who are not authenticated. Create a scheduled event to invoke the Lambda function.
B.
Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions.
C.
Run a script that puts a private ACL on all of the objects in the bucket.
D.
Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcIs option to TRUE on the bucket.
AnswerDiscussion
Correct Answer: D
Using the Block Public Access feature in Amazon S3 to set the IgnorePublicAcls option to TRUE ensures that all public ACLs are ignored, effectively blocking public access to the files in the S3 bucket. This approach will immediately address the security issue by preventing unauthorized downloads without impacting the application's normal workflow, as the application can still generate signed URLs for authenticated access.
Question 275 of 529
A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance in another AWS account. A solutions architect needs to design a migration strategy that will require no downtime and that will minimize the amount of time necessary to complete the migration. The migration strategy must replicate all existing data and any new data that is created during the migration. The target database must be identical to the source database at completion of the migration process.
All applications currently use an Amazon Route 53 CNAME record as their endpoint for communication with the RDS for Oracle DB instance. The RDS for Oracle DB instance is in a private subnet.
Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)
A.
Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database.
B.
Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema and initial data from the source database.
C.
Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.
D.
Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.
E.
Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint.
F.
Use AWS Database Migration Service (AWS DMS) in the target account to perform a change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint.
AnswerDiscussion
Correct Answer: A, C, E
To migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance with no downtime and minimal time, the first step is to create a new RDS for PostgreSQL DB instance in the target account. AWS Schema Conversion Tool (AWS SCT) should be used to migrate the database schema from the source database to the target database, ensuring compatibility between Oracle and PostgreSQL. Next, configure VPC peering between the VPCs in the two AWS accounts to provide connectivity between both databases. This ensures secure communication across accounts. Lastly, use AWS Database Migration Service (AWS DMS) to perform a full load plus change data capture (CDC) migration from the source database to the target database. This process ensures that all existing data and any new data during the migration are replicated. Once the migration is complete, update the Route 53 CNAME record to point to the new PostgreSQL DB instance endpoint to redirect applications seamlessly to the new database.
Question 276 of 529
A company has implemented an ordering system using an event-driven architecture. During initial testing, the system stopped processing orders. Further log analysis revealed that one order message in an Amazon Simple Queue Service (Amazon SQS) standard queue was causing an error on the backend and blocking all subsequent order messages. The visibility timeout of the queue is set to 30 seconds, and the backend processing timeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages and ensure that the system continues to process subsequent messages.
Which step should the solutions architect take to meet these requirements?
A.
Increase the backend processing timeout to 30 seconds to match the visibility timeout.
B.
Reduce the visibility timeout of the queue to automatically remove the faulty message.
C.
Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages.
D.
Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages.
AnswerDiscussion
Correct Answer: D
To address the issue of a faulty message blocking subsequent messages in an Amazon SQS standard queue, the best solution is to configure a new SQS standard queue as a dead-letter queue. This approach ensures that faulty messages are isolated in a separate queue, allowing the main queue to continue processing subsequent messages without interruption. Using a standard dead-letter queue is appropriate since the original queue is also a standard queue, ensuring compatibility and proper handling of message failures.
Question 277 of 529
A company has automated the nightly retraining of its machine learning models by using AWS Step Functions. The workflow consists of multiple steps that use AWS Lambda. Each step can fail for various reasons, and any failure causes a failure of the overall workflow.
A review reveals that the retraining has failed multiple nights in a row without the company noticing the failure. A solutions architect needs to improve the workflow so that notifications are sent for all types of failures in the retraining process.
Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)
A.
Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type "Email" that targets the team's mailing list.
B.
Create a task named "Email" that forwards the input arguments to the SNS topic.
C.
Add a Catch field to all Task, Map, and Parallel states that have a statement of "ErrorEquals": [ "States.ALL" ] and "Next”: "Email".
D.
Add a new email address to Amazon Simple Email Service (Amazon SES). Verify the email address.
E.
Create a task named "Email" that forwards the input arguments to the SES email address.
F.
Add a Catch field to all Task, Map, and Parallel states that have a statement of "ErrorEquals": [ "States.Runtime" ] and "Next": "Email".
AnswerDiscussion
Correct Answer: A, B, C
To ensure that notifications are sent for all types of failures in the retraining process, the solutions architect should create an Amazon Simple Notification Service (SNS) topic with a subscription of type 'Email' that targets the team's mailing list. This allows for email notifications when the retraining process fails. Additionally, a task named 'Email' should be created that forwards the input arguments to the SNS topic, which ensures that these email notifications are triggered appropriately. Finally, incorporating a Catch field to all Task, Map, and Parallel states in the AWS Step Functions workflow with a statement of 'ErrorEquals': ['States.ALL'] and 'Next': 'Email' will handle all types of errors by directing the workflow to the 'Email' task, ensuring comprehensive monitoring and alerting for any failure during the retraining process.
Question 278 of 529
A company plans to deploy a new private intranet service on Amazon EC2 instances inside a VPC. An AWS Site-to-Site VPN connects the VPC to the company's on-premises network. The new service must communicate with existing on-premises services. The on-premises services are accessible through the use of hostnames that reside in the company.example DNS zone. This DNS zone is wholly hosted on premises and is available only on the company's private network.
A solutions architect must ensure that the new service can resolve hostnames on the company.example domain to integrate with existing services.
Which solution meets these requirements?
A.
Create an empty private zone in Amazon Route 53 for company.example. Add an additional NS record to the company's on-premises company.example zone that points to the authoritative name servers for the new private zone in Route 53.
B.
Turn on DNS hostnames for the VPC. Configure a new outbound endpoint with Amazon Route 53 Resolver. Create a Resolver rule to forward requests for company.example to the on-premises name servers.
C.
Turn on DNS hostnames for the VPConfigure a new inbound resolver endpoint with Amazon Route 53 Resolver. Configur&the on-premises DNS server to forward requests for company.example to the new resolver.
D.
Use AWS Systems Manager to configure a run document that will install a hosts file that contains any required hostnames. Use an Amazon EventBridge rule to run the document when an instance is entering the running state.
AnswerDiscussion
Correct Answer: B
To ensure the new service can resolve hostnames on the company.example domain and integrate with existing on-premises services, it's necessary to leverage Amazon Route 53 Resolver. By turning on DNS hostnames for the VPC, the EC2 instances will have the capability to perform DNS resolution. Setting up a new outbound endpoint with Route 53 Resolver will allow the VPC to resolve DNS queries for domains hosted on the on-premises network. Creating a Resolver rule to forward requests specifically for the company.example domain to the on-premises name servers ensures that the new service can access the existing on-premises services via their hostnames.
Question 279 of 529
A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached to a transit gateway. Each VPC that sends traffic to the public internet must send the traffic through a shared services VPC. Each subnet within a VPC uses the default VPC route table, and the traffic is routed to the transit gateway. The transit gateway uses its default route table for any VPC attachment.
A security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate with an EC2 instance that is deployed in any of the company's other VPCs. A solutions architect needs to limit the traffic between the VPCs. Each VPC must be able to communicate only with a predefined, limited set of authorized VPCs.
What should the solutions architect do to meet these requirements?
A.
Update the network ACL of each subnet within a VPC to allow outbound traffic only to the authorized VPCs. Remove all deny rules except the default deny rule.
B.
Update all the security groups that are used within a VPC to deny outbound traffic to security groups that are used within the unauthorized VPCs.
C.
Create a dedicated transit gateway route table for each VPC attachment. Route traffic only to the authorized VPCs.
D.
Update the main route table of each VPC to route traffic only to the authorized VPCs through the transit gateway.
AnswerDiscussion
Correct Answer: C
To ensure that only authorized VPCs can communicate with each other, a solutions architect should create a dedicated transit gateway route table for each VPC attachment. This approach provides fine-grained control over the routing of traffic, allowing the architect to specify routes only to the predefined, authorized VPCs. By setting up separate route tables for each VPC attachment, traffic can be limited appropriately, thereby enforcing communication restrictions and enhancing network security.
Question 280 of 529
A company has a Windows-based desktop application that is packaged and deployed to the users' Windows machines. The company recently acquired another company that has employees who primarily use machines with a Linux operating system. The acquiring company has decided to migrate and rehost the Windows-based desktop application to AWS.
All employees must be authenticated before they use the application. The acquiring company uses Active Directory on premises but wants a simplified way to manage access to the application on AWS for all the employees.
Which solution will rehost the application on AWS with the LEAST development effort?
A.
Set up and provision an Amazon Workspaces virtual desktop for every employee. Implement authentication by using Amazon Cognito identity pools. Instruct employees to run the application from their provisioned Workspaces virtual desktops.
B.
Create an Auto Scaling group of Windows-based Amazon EC2 instances. Join each EC2 instance to the company’s Active Directory domain. Implement authentication by using the Active Directory that is running on premises. Instruct employees to run the application by using a Windows remote desktop.
C.
Use an Amazon AppStream 2.0 image builder to create an image that includes the application and the required configurations. Provision an AppStream 2.0 On-Demand fleet with dynamic Fleet Auto Scaling policies for running the image. Implement authentication by using AppStream 2.0 user pools. Instruct the employees to access the application by starting browser-based AppStream 2.0 streaming sessions.
D.
Refactor and containerize the application to run as a web-based application. Run the application in Amazon Elastic Container Service (Amazon ECS) on AWS Fargate with step scaling policies. Implement authentication by using Amazon Cognito user pools. Instruct the employees to run the application from their browsers.
AnswerDiscussion
Correct Answer: C
The most straightforward solution with the least development effort is to use Amazon AppStream 2.0. This service allows you to create an image that includes the Windows-based desktop application and necessary configurations. AppStream 2.0 supports dynamic scaling and user authentication via its user pools. Employees can easily access the application through browser-based streaming sessions, making it cross-platform compatible without significant refactoring or additional complexity. This effectively bridges the gap between different operating systems used by the employees while simplifying access management.
Question 281 of 529
A company is collecting a large amount of data from a fleet of IoT devices. Data is stored as Optimized Row Columnar (ORC) files in the Hadoop Distributed File System (HDFS) on a persistent Amazon EMR cluster. The company's data analytics team queries the data by using SQL in Apache Presto deployed on the same EMR cluster. Queries scan large amounts of data, always run for less than 15 minutes, and run only between 5 PM and 10 PM.
The company is concerned about the high cost associated with the current solution. A solutions architect must propose the most cost-effective solution that will allow SQL data queries.
Which solution will meet these requirements?
A.
Store data in Amazon S3. Use Amazon Redshift Spectrum to query data.
B.
Store data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data.
C.
Store data in EMR File System (EMRFS). Use Presto in Amazon EMR to query data.
D.
Store data in Amazon Redshift. Use Amazon Redshift to query data.
AnswerDiscussion
Correct Answer: B
Store data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data. Amazon S3 is a highly cost-effective storage solution compared to running a persistent EMR cluster. The AWS Glue Data Catalog provides a centralized repository for managing metadata, which facilitates easier data organization and improves query performance. Amazon Athena, a serverless query service, allows you to run SQL queries directly against data in S3 without requiring dedicated infrastructure. This approach is well-suited to the company's needs since queries are run only for a few hours daily and only pay for the queries executed, enhancing cost-effectiveness.
Question 282 of 529
A large company recently experienced an unexpected increase in Amazon RDS and Amazon DynamoDB costs. The company needs to increase visibility into details of AWS Billing and Cost Management. There are various accounts associated with AWS Organizations, including many development and production accounts. There is no consistent tagging strategy across the organization, but there are guidelines in place that require all infrastructure to be deployed using AWS CloudFormation with consistent tagging. Management requires cost center numbers and project ID numbers for all existing and future DynamoDB tables and RDS instances.
Which strategy should the solutions architect provide to meet these requirements?
A.
Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources.
B.
Use an AWS Config rule to alert the finance team of untagged resources. Create a centralized AWS Lambda based solution to tag untagged RDS databases and DynamoDB resources every hour using a cross-account role.
C.
Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID. Use SCPs to restrict resource creation that do not have the cost center and project ID on the resource.
D.
Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources. Update existing federated roles to restrict privileges to provision resources that do not include the cost center and project ID on the resource.
AnswerDiscussion
Correct Answer: C
To meet the company's requirement for both existing and future resources, the correct strategy is to use the Tag Editor to tag existing resources and create cost allocation tags to define the cost center and project ID. Additionally, using Service Control Policies (SCPs) can enforce tagging requirements on new resource creation, ensuring that all future DynamoDB tables and RDS instances include the required cost center and project ID tags. This approach provides a comprehensive solution for managing tagging both retrospectively and prospectively.
Question 283 of 529
A company wants to send data from its on-premises systems to Amazon S3 buckets. The company created the S3 buckets in three different accounts. The company must send the data privately without the data traveling across the internet. The company has no existing dedicated connectivity to AWS.
Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)
A.
Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a private VIF between the on-premises environment and the private VPC.
B.
Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a public VIF between the on-premises environment and the private VPC.
C.
Create an Amazon S3 interface endpoint in the networking account.
D.
Create an Amazon S3 gateway endpoint in the networking account.
E.
Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Peer VPCs from the accounts that host the S3 buckets with the VPC in the network account.
AnswerDiscussion
Correct Answer: A, C
To meet the requirements of sending data from on-premises systems to Amazon S3 buckets in different accounts without the data traveling across the internet, the company should establish a networking account in AWS and create a private VPC in that account. Setting up an AWS Direct Connect connection with a private virtual interface (VIF) between the on-premises environment and the private VPC ensures a dedicated, private connection that does not traverse the public internet. Additionally, creating an Amazon S3 interface endpoint in the networking account allows the on-premises systems to access the S3 buckets using private IP addresses within the VPC. Gateway endpoints do not allow access from on-premises environments, hence leveraging an interface endpoint is the appropriate approach for secure, private transfer of data.
Question 284 of 529
A company operates quick-service restaurants. The restaurants follow a predictable model with high sales traffic for 4 hours daily. Sales traffic is lower outside of those peak hours.
The point of sale and management platform is deployed in the AWS Cloud and has a backend that is based on Amazon DynamoDB. The database table uses provisioned throughput mode with 100,000 RCUs and 80,000 WCUs to match known peak resource consumption.
The company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.
Which solution meets these requirements MOST cost-effectively?
A.
Reduce the provisioned RCUs and WCUs.
B.
Change the DynamoDB table to use on-demand capacity.
C.
Enable Dynamo DB auto scaling for the table.
D.
Purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day.
AnswerDiscussion
Correct Answer: C
The stated scenario describes a restaurant with predictable high sales traffic for 4 hours each day, while the remaining hours have lower traffic. The current configuration of the DynamoDB table with provisioned throughput mode is set to handle the peak load, resulting in high costs during non-peak hours. Enabling DynamoDB auto-scaling is the most cost-effective solution for this predictable pattern. Auto-scaling adjusts the table's capacity automatically based on actual traffic, which reduces costs during non-peak hours by scaling down resources and ensures sufficient capacity during peak hours by scaling up as needed. This approach also minimizes operational overhead by eliminating the need for manual adjustments, making it the most suitable option for predictable traffic patterns.
Question 285 of 529
A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application currently does not use API keys to authorize requests. The API model is as follows:
GET /posts/{postId}: to get post details
GET /users/{userId}: to get user details
GET /comments/{commentId}: to get comments details
The company has noticed users are actively discussing topics in the comments section, and the company wants to increase user engagement by making the comments appear in real time.
Which design should be used to reduce comment latency and improve user experience?
A.
Use edge-optimized API with Amazon CloudFront to cache API responses.
B.
Modify the blog application code to request GET/comments/{commentId} every 10 seconds.
C.
Use AWS AppSync and leverage WebSockets to deliver comments.
D.
Change the concurrency limit of the Lambda functions to lower the API response time.
AnswerDiscussion
Correct Answer: C
To achieve real-time updates for the comments in the blog post application, AWS AppSync with WebSockets provides the most effective solution. AWS AppSync is a fully managed service that facilitates real-time data synchronization using GraphQL, and WebSockets are ideal for enabling real-time communication between clients and the server. This setup ensures that comments appear instantly as they are posted, thus reducing latency and enhancing user engagement. Other options provided either do not support real-time communication or do not adequately address the need for real-time comment updates.
Question 286 of 529
A company manages hundreds of AWS accounts centrally in an organization in AWS Organizations. The company recently started to allow product teams to create and manage their own S3 access points in their accounts. The S3 access points can be accessed only within VPCs, not on the internet.
What is the MOST operationally efficient way to enforce this requirement?
A.
Set the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.
B.
Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.
C.
Use AWS CloudFormation StackSets to create a new IAM policy in each AWS account that allows the s3:CreateAccessPoint action only if the s3:AccessPointNetworkOrigin condition key evaluates to VPC.
D.
Set the S3 bucket policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.
AnswerDiscussion
Correct Answer: B
The most operationally efficient way to enforce the requirement that S3 access points can be accessed only within VPCs in an environment with hundreds of AWS accounts managed by AWS Organizations is to create a Service Control Policy (SCP) at the root level in the organization. This ensures that the policy is applied uniformly across all accounts and prevents the need to manually configure policies in each individual account. SCPs are specifically designed to control AWS service actions across multiple accounts within an organization, making them ideal for this scenario.
Question 287 of 529
A solutions architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The solutions architect creates an environment that is identical to the existing application environment and deploys the application to the new environment.
What should be done next to complete the update?
A.
Redirect to the new environment using Amazon Route 53.
B.
Select the Swap Environment URLs option.
C.
Replace the Auto Scaling launch configuration.
D.
Update the DNS records to point to the green environment.
AnswerDiscussion
Correct Answer: B
To complete the update in an AWS Elastic Beanstalk environment using a blue/green deployment methodology, you should use the Swap Environment URLs option. This option seamlessly swaps the CNAME records of the two environments, effectively rerouting the traffic from the old environment to the new one with minimal downtime.
Question 288 of 529
A company is building an image service on the web that will allow users to upload and search random photos. At peak usage, up to 10,000 users worldwide will upload their images. The will then overlay text on the uploaded images, which will then be published on the company website.
Which design should a solutions architect implement?
A.
Store the uploaded images in Amazon Elastic File System (Amazon EFS). Send application log information about each image to Amazon CloudWatch Logs. Create a fleet of Amazon EC2 instances that use CloudWatch Logs to determine which images need to be processed. Place processed images in another directory in Amazon EFS. Enable Amazon CloudFront and configure the origin to be the one of the EC2 instances in the fleet.
B.
Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to Amazon Simple Notification Service (Amazon SNS). Create a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) to pull messages from Amazon SNS to process the images and place them in Amazon Elastic File System (Amazon EFS). Use Amazon CloudWatch metrics for the SNS message volume to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the ALB in front of the EC2 instances.
C.
Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to the Amazon Simple Queue Service (Amazon SQS) queue. Create a fleet of Amazon EC2 instances to pull messages from the SQS queue to process the images and place them in another S3 bucket. Use Amazon CloudWatch metrics for queue depth to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the S3 bucket that contains the processed images.
D.
Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume mounted to a fleet of Amazon EC2 Spot instances. Create an Amazon DynamoDB table that contains information about each uploaded image and whether it has been processed. Use an Amazon EventBridge rule to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to reference an Elastic Load Balancer in front of the fleet of EC2 instances.
AnswerDiscussion
Correct Answer: C
To handle the upload and processing of images efficiently, storing the uploaded images in an Amazon S3 bucket provides highly scalable and durable object storage. Configuring an S3 bucket event notification to send a message to an Amazon Simple Queue Service (SQS) queue ensures decoupling between the upload and processing steps. A fleet of Amazon EC2 instances can then pull messages from the SQS queue to process the images, ensuring that the system can scale out based on demand by using Amazon CloudWatch metrics to monitor the SQS queue depth. Finally, placing the processed images in another S3 bucket and enabling Amazon CloudFront with the origin set to the S3 bucket containing the processed images improves the global availability and performance of image delivery. This design provides a scalable, decoupled, and efficient solution for the given requirements.
Question 289 of 529
A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data available to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not tolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of customers need to see updates from the other group in real time.
Which solution will meet these requirements?
A.
Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure the application to use the Aurora database and resume writes. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.
B.
Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the primary DB instance. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.
C.
Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1 from the snapshot. Configure MySQL logical replication from us-east-1 to eu-west-1. Enable write forwarding on the DB cluster. Deploy the application in eu-wes&1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.
D.
Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.
AnswerDiscussion
Correct Answer: D
To meet the requirements of providing real-time data availability and low latency for both US and European customers, the best solution is to use Amazon Aurora global database with write forwarding enabled. Aurora global databases allow for low-latency global reads and writes, with synchronization across multiple regions. Converting the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster and adding Eu-west-1 as a secondary region ensures that European customers can write to the database and see updates from US customers in real time. Therefore, converting RDS to Aurora and enabling write forwarding offers the most suitable solution for global low-latency, high-availability data access.
Question 290 of 529
A company is serving files to its customers through an SFTP server that is accessible over the internet. The SFTP server is running on a single Amazon EC2 instance with an Elastic IP address attached. Customers connect to the SFTP server through its Elastic IP address and use SSH for authentication. The EC2 instance also has an attached security group that allows access from all customer IP addresses.
A solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the disruption to customers who access files. The solution must not change the way customers connect.
Which solution will meet these requirements?
A.
Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a publicly accessible endpoint. Associate the SFTP Elastic IP address with the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.
B.
Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a VPC-hosted, internet-facing endpoint. Associate the SFTP Elastic IP address with the new endpoint. Attach the security group with customer IP addresses to the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.
C.
Disassociate the Elastic IP address from the EC2 instance. Create a new Amazon Elastic File System (Amazon EFS) file system to be used for SFTP file hosting. Create an AWS Fargate task definition to run an SFTP server. Specify the EFS file system as a mount in the task definition. Create a Fargate service by using the task definition, and place a Network Load Balancer (NLB) in front of the service. When configuring the service, attach the security group with customer IP addresses to the tasks that run the SFTP server. Associate the Elastic IP address with the NLB. Sync all files from the SFTP server to the S3 bucket.
D.
Disassociate the Elastic IP address from the EC2 instance. Create a multi-attach Amazon Elastic Block Store (Amazon EBS) volume to be used for SFTP file hosting. Create a Network Load Balancer (NLB) with the Elastic IP address attached. Create an Auto Scaling group with EC2 instances that run an SFTP server. Define in the Auto Scaling group that instances that are launched should attach the new multi-attach EBS volume. Configure the Auto Scaling group to automatically add instances behind the NLB. Configure the Auto Scaling group to use the security group that allows customer IP addresses for the EC2 instances that the Auto Scaling group launches. Sync all files from the SFTP server to the new multi-attach EBS volume.
AnswerDiscussion
Correct Answer: B
The solution needs to improve availability, minimize infrastructure management complexity, and avoid disrupting customer access, while not changing the way customers connect to the SFTP server. The best approach to achieve this is to use AWS Transfer Family with an Amazon S3 bucket as the backend for SFTP file hosting. By creating a Transfer Family server with a VPC-hosted, internet-facing endpoint and associating the existing Elastic IP address to this endpoint, the transition becomes seamless for customers. Additionally, attaching the existing security group that contains customer IP addresses ensures that only authorized users have access, maintaining security without altering the connection method for the customers. Syncing the existing files to the S3 bucket achieves the necessary data migration without disruption. This setup meets all the requirements effectively.
Question 291 of 529
A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics takes 4 hours to complete. The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run fails.
The current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations. These EC2 instances run full time to ingest and store the streaming data in attached Amazon Elastic Block Store (Amazon EBS) volumes. A scheduled script launches EC2 On-Demand Instances each night to perform the nightly processing. The instances access the stored data from NFS shares on the ingestion servers. The script terminates the instances when the processing is complete.
The Reserved Instance reservations are expiring. The company needs to determine whether to purchase new reservations or implement a new design.
Which solution will meet these requirements MOST cost-effectively?
A.
Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a scheduled script to launch a fleet of EC2 On-Demand Instances each night to perform the batch processing of the S3 data. Configure the script to terminate the instances when the processing is complete.
B.
Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price.
C.
Update the ingestion process to use a fleet of EC2 Reserved Instances with 3-year reservations behind a Network LoadBalancer. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price.
D.
Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use Amazon EventBridge to schedule an AWS Lambda function to run nightly to query Amazon Redshift to generate the daily statistics.
AnswerDiscussion
Correct Answer: B
Using Amazon Kinesis Data Firehose to save data to Amazon S3 is a reliable and cost-effective way to ingest data. AWS Batch with Spot Instances provides a highly cost-effective method for performing batch processing, as Spot Instances can be significantly cheaper than On-Demand Instances. Since the statistical analysis is not critical and can tolerate occasional failures, the use of Spot Instances, which can be interrupted, is a suitable and cost-saving choice. This approach aligns well with the requirement to minimize costs while ensuring that the nightly processing is completed most of the time.
Question 292 of 529
A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a Linux VM. Uploaded files are made available to downstream applications through an NFS share.
As part of the migration to AWS, a solutions architect must implement high availability. The solution must provide external vendors with a set of static public IP addresses that the vendors can allow. The company has set up an AWS Direct Connect connection between its on-premises data center and its VPC.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create an AWS Transfer Family server. Configure an internet-facing VPC endpoint for the Transfer Family server. Specify an Elastic IP address for each subnet. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.
B.
Create an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.
C.
Use AWS Application Migration Service to migrate the existing Linux VM to an Amazon EC2 instance. Assign an Elastic IP address to the EC2 instance. Mount an Amazon Elastic File System (Amazon EFS) file system to the EC2 instance. Configure the SFTP server to place files in the EFS file system. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.
D.
Use AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon FSx for Lustre file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the FSx for Lustre endpoint instead.
AnswerDiscussion
Correct Answer: A
To meet the company's requirement for providing static public IP addresses, the solution involves creating an AWS Transfer Family server and configuring an internet-facing VPC endpoint for it. By specifying an Elastic IP address for each subnet, the company ensures that external vendors have a consistent set of static IPs to allow. The Transfer Family server will handle the SFTP connections, while Amazon EFS offers a scalable and highly available storage solution, integrating seamlessly with the existing NFS-based systems. This setup minimizes operational overhead by leveraging managed services and ensures high availability.
Question 293 of 529
A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and connectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as follows:
VPC CIDR: 10.0.0.0/23 -
AZ1 subnet CIDR: 10.0.0.0/24 -
AZ2 subnet CIDR: 10.0.1.0/24 -
Since deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional IPv4 address space and without service downtime. Which solution will meet these requirements?
A.
Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.
B.
Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets.
C.
Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC.
D.
Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have half the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.
AnswerDiscussion
Correct Answer: A
The correct approach to add a new AZ without service downtime, while working within the existing IPv4 address space, involves progressively transitioning to new subnets. This ensures high availability and minimal disruption. Here’s why: You first update the Auto Scaling group to use the existing AZ2 subnet only, ensuring continuity. Then, by deleting and recreating the AZ1 subnet with a smaller address space and gradually shifting the Auto Scaling group to this new subnet, you maintain healthy instances. Once done, delete the old AZ2 subnet, recreate it with a smaller address space, and finally create a new subnet in AZ3. This method ensures the instances are always within a subnet, maintaining service availability. Therefore, option A is the most suitable choice.
Question 294 of 529
A company uses an organization in AWS Organizations to manage the company's AWS accounts. The company uses AWS CloudFormation to deploy all infrastructure. A finance team wants to build a chargeback model. The finance team asked each business unit to tag resources by using a predefined list of project values.
When the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based on project, the team noticed noncompliant project values. The company wants to enforce the use of project tags for new resources.
Which solution will meet these requirements with the LEAST effort?
A.
Create a tag policy that contains the allowed project tag values in the organization's management account. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.
B.
Create a tag policy that contains the allowed project tag values in each OU. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.
C.
Create a tag policy that contains the allowed project tag values in the AWS management account. Create an IAM policy that denies the cloudformation:CreateStack API operation unless a project tag is added. Assign the policy to each user.
D.
Use AWS Service Catalog to manage the CloudFormation stacks as products. Use a TagOptions library to control project tag values. Share the portfolio with all OUs that are in the organization.
AnswerDiscussion
Correct Answer: A
The company should create a tag policy in the organization's management account with the allowed project tag values. This ensures consistency across the organization. Additionally, they should create a Service Control Policy (SCP) that denies the cloudformation:CreateStack API operation unless a project tag is added and attach this SCP to each Organizational Unit (OU). This approach enforces the correct tags with minimal effort, as the tag policy in the management account is inherited by all child OUs. Using IAM policies for individual users or managing policies at the OU level would create unnecessary overhead. AWS Service Catalog is an overcomplication and does not directly enforce tags for all resources.
Question 295 of 529
An application is deployed on Amazon EC2 instances that run in an Auto Scaling group. The Auto Scaling group configuration uses only one type of instance.
CPU and memory utilization metrics show that the instances are underutilized. A solutions architect needs to implement a solution to permanently reduce the EC2 cost and increase the utilization.
Which solution will meet these requirements with the LEAST number of configuration changes in the future?
A.
List instance types that have properties that are similar to the properties that the current instances have. Modify the Auto Scaling group's launch template configuration to use multiple instance types from the list.
B.
Use the information about the application's CPU and memory utilization to select an instance type that matches the requirements. Modify the Auto Scaling group's configuration by adding the new instance type. Remove the current instance type from the configuration.
C.
Use the information about the application's CPU and memory utilization to specify CPU and memory requirements in a new revision of the Auto Scaling group's launch template. Remove the current instance type from the configuration.
D.
Create a script that selects the appropriate instance types from the AWS Price List Bulk API. Use the selected instance types to create a new revision of the Auto Scaling group's launch template.
AnswerDiscussion
Correct Answer: B
To permanently reduce EC2 costs and increase utilization, it is effective to select an instance type based on the application's CPU and memory utilization requirements. This ensures that the instance type aligns with the usage needs, optimizing both performance and cost. By modifying the Auto Scaling group's configuration to use the new instance type and removing the old one, you maintain a single configuration that is tailored to the application’s needs. This approach minimizes the need for future adjustments, as it directly addresses the issue of overallocation by selecting an appropriate instance size from the start.
Question 296 of 529
A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway The application data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning by using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.
A solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.
Which solution will meet these requirements MOST cost-effectively?
A.
Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront with origin failover to route traffic to the secondary Region during a DR scenario.
B.
Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB Streams, EventBridge. and Lambda to replicate the DynamoDB databases to the secondary Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.
C.
Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.
D.
Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.
AnswerDiscussion
Correct Answer: D
To ensure an RPO of 2 hours and an RTO of 4 hours, the best choice involves minimal complexity while still meeting the recovery requirements. Setting up an Aurora global database and using DynamoDB global tables for replication ensures that the data is continuously synchronized between the primary and secondary regions, effectively meeting the RPO. Configuring API Gateway with Regional endpoints in both regions allows for a quick switch in the event of a disaster, contributing to meeting the RTO. Utilizing Amazon Route 53 failover routing provides a reliable and cost-effective way to automatically redirect traffic to the secondary region during a disaster recovery scenario, ensuring that the transition happens smoothly without additional complex configurations or high costs associated with services like CloudFront.
Question 297 of 529
A company has a complex web application that leverages Amazon CloudFront for global scalability and performance. Over time, users report that the web application is slowing down.
The company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. The cache metrics report indicates that query strings on some URLs are inconsistently ordered and are specified sometimes in mixed-case letters and sometimes in lowercase letters.
Which set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?
A.
Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger to invoke the function.
B.
Update the CloudFront distribution to disable caching based on query string parameters.
C.
Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase.
D.
Update the CloudFront distribution to specify casing-insensitive query string processing.
AnswerDiscussion
Correct Answer: A
Using a Lambda@Edge function to sort parameters by name and force them to be lowercase is the most effective solution. This method addresses the issue of inconsistent and mixed-case query strings, which are causing cache misses. By normalizing these parameters at the CloudFront edge, the cache hit ratio will improve as requests for the same resources can be properly matched.
Question 298 of 529
A company runs an ecommerce application in a single AWS Region. The application uses a five-node Amazon Aurora MySQL DB cluster to store information about customers and their recent orders. The DB cluster experiences a large number of write transactions throughout the day.
The company needs to replicate the data in the Aurora database to another Region to meet disaster recovery requirements. The company has an RPO of 1 hour.
Which solution will meet these requirements with the LOWEST cost?
A.
Modify the Aurora database to be an Aurora global database. Create a second Aurora database in another Region.
B.
Enable the Backtrack feature for the Aurora database. Create an AWS Lambda function that runs daily to copy the snapshots of the database to a backup Region.
C.
Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region.
D.
Turn off automated Aurora backups. Configure Aurora backups with a backup frequency of 1 hour. Specify another Region as the destination Region. Select the Aurora database as the resource assignment.
AnswerDiscussion
Correct Answer: A
The company needs to replicate the data in the Aurora database to another region to meet disaster recovery requirements. The solution must provide a low recovery point objective (RPO) of 1 hour and be cost-effective. Modifying the Aurora database to be an Aurora global database and creating a second Aurora database in another region leverages Aurora's built-in global database feature, which allows continuous replication with low-latency and minimal data transfer costs. This method directly addresses the disaster recovery needs and assures compliance with the RPO requirement.
Question 299 of 529
A company's solutions architect is evaluating an AWS workload that was deployed several years ago. The application tier is stateless and runs on a single large Amazon EC2 instance that was launched from an AMI. The application stores data in a MySQL database that runs on a single EC2 instance.
The CPU utilization on the application server EC2 instance often reaches 100% and causes the application to stop responding. The company manually installs patches on the instances. Patching has caused downtime in the past. The company needs to make the application highly available.
Which solution will meet these requirements with the LEAST development me?
A.
Move the application tier to AWS Lambda functions in the existing VPC. Create an Application Load Balancer to distribute traffic across the Lambda functions. Use Amazon GuardDuty to scan the Lambda functions. Migrate the database to Amazon DocumentDB (with MongoDB compatibility.
B.
Change the EC2 instance type to a smaller Graviton powered instance type. Use the existing AMI to create a launch template for an Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon DynamoDB.
C.
Move the application tier to containers by using Docker. Run the containers on Amazon Elastic Container Service (Amazon ECS) with EC2 instances. Create an Application Load Balancer to distribute traffic across the ECS cluster. Configure the ECS cluster to scale based on CPU utilization. Migrate the database to Amazon Neptune.
D.
Create a now AMI that is configured with AWS Systems Manager Agent (SSM Agent). Use the new AMI to create a launch template for an Auto Scaling group. Use smaller instances in the Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon Aurora MySQL.
AnswerDiscussion
Correct Answer: D
The best approach to make the application highly available with the least development effort involves maintaining compatibility with the existing architecture. Creating a new AMI configured with AWS Systems Manager Agent (SSM Agent) and using it to create a launch template for an Auto Scaling group ensures scaling based on CPU utilization. Smaller instances in the Auto Scaling group would improve resource allocation efficiency, and an Application Load Balancer distributes traffic effectively. Migrating the MySQL database to Amazon Aurora MySQL minimizes changes needed while leveraging the benefits of a managed service that's compatible with the existing MySQL database.
Question 300 of 529
A company is planning to migrate several applications to AWS. The company does not have a good understanding of its entire application estate. The estate consists of a mixture of physical machines and VMs.
One application that the company will migrate has many dependencies that are sensitive to latency. The company is unsure what all the dependencies are. However the company knows that the low-latency communications use a custom IP-based protocol that runs on port 1000. The company wants to migrate the application and these dependencies together to move all the low-latency interfaces to AWS at the same time.
The company has installed the AWS Application Discovery Agent and has been collecting data for several months.
What should the company do to identify the dependencies that need to be migrated in the same phase as the application?
A.
Use AWS Migration Hub and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Turn on data exploration in Amazon Athena. Query the data that is transferred between the servers to identify the servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries.
B.
Use AWS Application Migration Service and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Configure Application Migration Service to launch test instances for all the servers that interact with the application. Perform acceptance tests on the test instances. If no issues are identified, create a move group that is based on the tested servers.
C.
Use AWS Migration Hub and select the servers that host the application. Turn on data exploration in Network Access Analyzer. Use the Network Access Analyzer console to select the servers that host the application. Select a Network Access Scope of port 1000 and note the matching servers. Return to Migration Hub. Create a move group that is based on the findings from Network Access Analyzer.
D.
Use AWS Migration Hub and select the servers that host the application. Push the Amazon CloudWalch agent to the identified servers by using the AWS Application Discovery Agent. Export the CloudWatch logs that the agents collect to Amazon S3. Use Amazon Athena to query the logs to find servers that communicate on port 1000. Return to Migration Hub Create a move group that is based on the findings from the Athena queries.
AnswerDiscussion
Correct Answer: A
To identify dependencies sensitive to latency that communicate using a custom IP-based protocol on port 1000, the company should use AWS Migration Hub to visualize the network graph. This allows the identification of servers interacting with the application. Turning on data exploration in Amazon Athena and querying server data specifically for port 1000 communication will highlight the necessary dependencies. Based on these findings, a move group can be created in Migration Hub, ensuring that all dependencies are migrated together, maintaining low-latency communication.
Question 301 of 529
A company is building an application that will run on an AWS Lambda function. Hundreds of customers will use the application. The company wants to give each customer a quota of requests for a specific time period. The quotas must match customer usage patterns. Some customers must receive a higher quota for a shorter time period.
Which solution will meet these requirements?
A.
Create an Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. For each customer, configure an API Gateway usage plan that includes an appropriate request quota. Create an API key from the usage plan for each user that the customer needs.
B.
Create an Amazon API Gateway HTTP API with a proxy integration to invoke the Lambda function. For each customer configure an API Gateway usage plan that includes an appropriate request quota Configure route-level throttling for each usage plan. Create an API Key from the usage plan for each user that the customer needs.
C.
Create a Lambda function alias for each customer. Include a concurrency limit with an appropriate request quota. Create a Lambda function URL for each function alias. Share the Lambda function URL for each alias with the relevant customer.
D.
Create an Application Load Balancer (ALB) in a VPC. Configure the Lambda function as a target for the ALB. Configure an AWS WAF web ACL for the ALB. For each customer configure a rale-based rule that includes an appropriate request quota.
AnswerDiscussion
Correct Answer: A
The best solution for creating request quotas for each customer in an AWS Lambda application is to use Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. By configuring an API Gateway usage plan for each customer, you can manage specific request quotas, accommodating different customer usage patterns. This approach allows the creation of an API key from the usage plan for each user that the customer needs, helping to effectively limit and track the usage based on the predefined quotas.
Question 302 of 529
A company is planning to migrate its on-premises VMware cluster of 120 VMs to AWS. The VMs have many different operating systems and many custom software packages installed. The company also has an on-premises NFS server that is 10 TB in size. The company has set up a 10 Gbps AWS Direct Connect connection to AWS for the migration.
Which solution will complete the migration to AWS in the LEAST amount of time?
A.
Export the on-premises VMs and copy them to an Amazon S3 bucket. Use VM Import/Export to create AMIs from the VM images that are stored in Amazon S3. Order an AWS Snowball Edge device. Copy the NFS server data to the device. Restore the NFS server data to an Amazon EC2 instance that has NFS configured.
B.
Configure AWS Application Migration Service with a connection to the VMware cluster. Create a replication job for the VMS. Create an Amazon Elastic File System (Amazon EFS) file system. Configure AWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection.
C.
Recreate the VMs on AWS as Amazon EC2 instances. Install all the required software packages. Create an Amazon FSx for Lustre file system. Configure AWS DataSync to copy the NFS server data to the FSx for Lustre file system over the Direct Connect connection.
D.
Order two AWS Snowball Edge devices. Copy the VMs and the NFS server data to the devices. Run VM Import/Export after the data from the devices is loaded to an Amazon S3 bucket. Create an Amazon Elastic File System (Amazon EFS) file system. Copy the NFS server data from Amazon S3 to the EFS file system.
AnswerDiscussion
Correct Answer: B
The most efficient approach to minimize the migration time of on-premises VMware cluster and NFS server data to AWS is to utilize AWS Application Migration Service for the VMs and AWS DataSync for transferring the NFS data using the existing 10 Gbps AWS Direct Connect connection. AWS Application Migration Service is designed to handle the migration of virtual machines efficiently, and AWS DataSync can move large amounts of data rapidly. This method leverages the high-speed Direct Connect connection, ensuring fast data transfer without the need for physical devices like AWS Snowball. Creating the Amazon Elastic File System (EFS) allows for scalable storage compatible with NFS, providing a seamless transition for the NFS server data.
Question 303 of 529
An online survey company runs its application in the AWS Cloud. The application is distributed and consists of microservices that run in an automatically scaled Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster is a target for an Application Load Balancer (ALB). The ALB is a custom origin for an Amazon CloudFront distribution.
The company has a survey that contains sensitive data. The sensitive data must be encrypted when it moves through the application. The application's data-handling microservice is the only microservice that should be able to decrypt the data
Which solution will meet these requirements?
A.
Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a field-level encryption profile and a configuration. Associate the KMS key and the configuration with the CloudFront cache behavior.
B.
Create an RSA key pair that is dedicated to the data-handing microservice. Upload the public key to the CloudFront distribution. Create a field-level encryption profile and a configuration. Add the configuration to the CloudFront cache behavior.
C.
Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the KMS key to encrypt the sensitive data.
D.
Create an RSA key pair that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the private key of the RSA key pair to encrypt the sensitive data.
AnswerDiscussion
Correct Answer: B
To meet the requirement of encrypting sensitive data while it moves through the application and ensuring that only the data-handling microservice can decrypt it, using an RSA key pair for field-level encryption in Amazon CloudFront is the correct solution. Field-level encryption in CloudFront requires asymmetric encryption, specifically utilizing an RSA public-private key pair. By uploading the public key to the CloudFront distribution and creating a field-level encryption profile and configuration, you can ensure that the data is encrypted before it reaches the backend and only the data-handling microservice, which holds the private key, can decrypt it.
Question 304 of 529
A solutions architect is determining the DNS strategy for an existing VPC. The VPC is provisioned to use the 10.24.34.0/24 CIDR block. The VPC also uses Amazon Route 53 Resolver for DNS. New requirements mandate that DNS queries must use private hosted zones. Additionally instances that have public IP addresses must receive corresponding public hostnames
Which solution will meet these requirements to ensure that the domain names are correctly resolved within the VPC?
A.
Create a private hosted zone. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=10.24.34.2.
B.
Create a private hosted zone Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Create a new VPC DHCP options set, and configure domain-name-servers=AmazonProvidedDNS. Associate the new DHCP options set with the VPC.
C.
Deactivate the enableDnsSupport attribute for the VPActivate the enableDnsHostnames attribute for the VPCreate a new VPC DHCP options set, and configure doman-name-servers=10.24.34.2. Associate the new DHCP options set with the VPC.
D.
Create a private hosted zone. Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute for the VPC. Deactivate the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=AmazonProvidedDNS.
AnswerDiscussion
Correct Answer: B
To meet the requirements that DNS queries must use private hosted zones and that instances with public IP addresses must receive corresponding public hostnames, we need to create a private hosted zone and associate it with the VPC. Additionally, Amazon Route 53 Resolver will require both the enableDnsSupport and enableDnsHostnames attributes to be activated. The VPC DHCP options set must be configured to use AmazonProvidedDNS to ensure proper resolution of domain names within the VPC.
Question 305 of 529
A data analytics company has an Amazon Redshift cluster that consists of several reserved nodes. The cluster is experiencing unexpected bursts of usage because a team of employees is compiling a deep audit analysis report. The queries to generate the report are complex read queries and are CPU intensive.
Business requirements dictate that the cluster must be able to service read and write queries at all times. A solutions architect must devise a solution that accommodates the bursts of usage.
Which solution meets these requirements MOST cost-effectively?
A.
Provision an Amazon EMR cluster Offload the complex data processing tasks.
B.
Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using a classic resize operation when the cluster’s CPU metrics in Amazon CloudWatch reach 80%.
C.
Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using an elastic resize operation when the cluster’s CPU metrics in Amazon CloudWatch reach 80%.
D.
Turn on the Concurrency Scaling feature for the Amazon Redshift cluster.
AnswerDiscussion
Correct Answer: D
Turning on the Concurrency Scaling feature for the Amazon Redshift cluster allows the system to automatically add additional capacity to handle increased read and write queries during unexpected bursts of usage. It ensures that the workload is managed dynamically without manual intervention and is the most cost-effective solution to maintain performance for both read and write operations.
Question 306 of 529
A research center is migrating to the AWS Cloud and has moved its on-premises 1 PB object storage to an Amazon S3 bucket. One hundred scientists are using this object storage to store their work-related documents. Each scientist has a personal folder on the object store. All the scientists are members of a single IAM user group.
The research center's compliance officer is worried that scientists will be able to access each other's work. The research center has a strict obligation to report on which scientist accesses which documents. The team that is responsible for these reports has little AWS experience and wants a ready-to-use solution that minimizes operational overhead.
Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)
A.
Create an identity policy that grants the user read and write access. Add a condition that specifies that the S3 paths must be prefixed with $(aws:username). Apply the policy on the scientists’ IAM user group.
B.
Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket. Store the trail output in another S3 bucket. Use Amazon Athena to query the logs and generate reports.
C.
Enable S3 server access logging. Configure another S3 bucket as the target for log delivery. Use Amazon Athena to query the logs and generate reports.
D.
Create an S3 bucket policy that grants read and write access to users in the scientists’ IAM user group.
E.
Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket and write the events to Amazon CloudWatch. Use the Amazon Athena CloudWatch connector to query the logs and generate reports.
AnswerDiscussion
Correct Answer: A, B
To ensure that each scientist can only access their own work-related documents and to meet the strict compliance reporting requirements, a two-part strategy is necessary. First, create an identity policy granting users read and write access specifically to paths prefixed with their username. This will ensure that scientists cannot access each other's folders and keeps their data isolated in accordance with the compliance officer's concerns. Second, configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket and store the logs in another S3 bucket. This will provide a detailed and accurate log of which scientist accesses which documents, which can be queried using Amazon Athena to generate the necessary compliance reports with minimal operational overhead. This combination addresses both the access control and the auditing requirements effectively.
Question 307 of 529
A company uses AWS Organizations to manage a multi-account structure. The company has hundreds of AWS accounts and expects the number of accounts to increase. The company is building a new application that uses Docker images. The company will push the Docker images to Amazon Elastic Container Registry (Amazon ECR). Only accounts that are within the company’s organization should have access to the images.
The company has a CI/CD process that runs frequently. The company wants to retain all the tagged images. However, the company wants to retain only the five most recent untagged images.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create a private repository in Amazon ECR. Create a permissions policy for the repository that allows only required ECR operations. Include a condition to allow the ECR operations if the value of the aws:PrincipalOrglD condition key is equal to the ID of the company’s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five
B.
Create a public repository in Amazon ECR. Create an IAM role in the ECR account. Set permissions so that any account can assume the role if the value of the aws:PrincipalOrglD condition key is equal to the ID of the company’s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five.
C.
Create a private repository in Amazon ECR. Create a permissions policy for the repository that includes only required ECR operations. Include a condition to allow the ECR operations for all account IDs in the organization Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five.
D.
Create a public repository in Amazon ECR. Configure Amazon ECR to use an interface VPC endpoint with an endpoint policy that includes the required permissions for images that the company needs to pull. Include a condition to allow the ECR operations for all account IDs in the company’s organization. Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five.
AnswerDiscussion
Correct Answer: A
Creating a private repository in Amazon ECR allows for controlled access and ensures that only accounts within the company’s organization can access the Docker images. By setting a permissions policy that includes a condition based on the aws:PrincipalOrglD condition key, access is restricted to the organization's accounts. Adding a lifecycle rule to the ECR repository to delete untagged images over the count of five automates the cleanup process, reducing operational overhead. This solution meets the requirements for scalability, security, and operational efficiency.
Question 308 of 529
A solutions architect is reviewing a company's process for taking snapshots of Amazon RDS DB instances. The company takes automatic snapshots every day and retains the snapshots for 7 days.
The solutions architect needs to recommend a solution that takes snapshots every 6 hours and retains the snapshots for 30 days. The company uses AWS Organizations to manage all of its AWS accounts. The company needs a consolidated view of the health of the RDS snapshots.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Turn on the cross-account management feature in AWS Backup. Create a backup plan that specifies the frequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags. Use AWS Backup to monitor the status of the backups.
B.
Turn on the cross-account management feature in Amazon RDS. Create a snapshot global policy that specifies the frequency and retention requirements. Use the RDS console in the management account to monitor the status of the backups.
C.
Turn on the cross-account management feature in AWS CloudFormation. From the management account, deploy a CloudFormation stack set that contains a backup plan from AWS Backup that specifies the frequency and retention requirements. Create an AWS Lambda function in the management account to monitor the status of the backups. Create an Amazon EventBridge rule in each account to run the Lambda function on a schedule.
D.
Configure AWS Backup in each account. Create an Amazon Data Lifecycle Manager lifecycle policy that specifies the frequency and retention requirements. Specify the DB instances as the target resource Use the Amazon Data Lifecycle Manager console in each member account to monitor the status of the backups.
AnswerDiscussion
Correct Answer: A
The optimal solution to handle snapshots every 6 hours and retain them for 30 days, with consolidated health monitoring and minimal operational overhead, is to use AWS Backup. By turning on the cross-account management feature in AWS Backup, creating a backup plan that specifies the required frequency and retention, and applying the backup plan using tags, the company can efficiently manage and monitor the backup status across all accounts. This approach leverages AWS Backup's built-in features for cross-account management and health monitoring, meeting the requirements with the least operational overhead.
Question 309 of 529
A company is using AWS Organizations with a multi-account architecture. The company's current security configuration for the account architecture includes SCPs, resource-based policies, identity-based policies, trust policies, and session policies.
A solutions architect needs to allow an IAM user in Account A to assume a role in Account B.
Which combination of steps must the solutions architect take to meet this requirement? (Choose three.)
A.
Configure the SCP for Account A to allow the action.
B.
Configure the resource-based policies to allow the action.
C.
Configure the identity-based policy on the user in Account A to allow the action.
D.
Configure the identity-based policy on the user in Account B to allow the action.
E.
Configure the trust policy on the target role in Account B to allow the action.
F.
Configure the session policy to allow the action and to be passed programmatically by the GetSessionToken API operation.
AnswerDiscussion
Correct Answer: A, C, E
To allow an IAM user in Account A to assume a role in Account B, several steps are necessary: First, the identity-based policy on the user in Account A must be configured to allow the action, as this policy controls what the users in Account A can do. Second, the trust policy on the role in Account B must be configured to trust the IAM user from Account A to assume the role. This configuration establishes the relationship that allows the user to assume the role. Lastly, as the company uses AWS Organizations with SCPs, the SCP for Account A must be configured to allow this action. SCPs act at the organizational unit and account level to ensure that nothing in the hierarchy explicitly denies the required action.
Question 310 of 529
A company wants to use Amazon S3 to back up its on-premises file storage solution. The company’s on-premises file storage solution supports NFS, and the company wants its new solution to support NFS. The company wants to archive the backup files after 5 days. If the company needs archived files for disaster recovery, the company is willing to wait a few days for the retrieval of those files.
Which solution meets these requirements MOST cost-effectively?
A.
Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.
B.
Deploy an AWS Storage Gateway volume gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the volume gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.
C.
Deploy an AWS Storage Gateway tape gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the tape gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.
D.
Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.
AnswerDiscussion
Correct Answer: D
To back up the on-premises file storage solution that supports NFS and to archive files after 5 days, the most cost-effective solution is to use an AWS Storage Gateway file gateway associated with an S3 bucket. The S3 Lifecycle rule should move the files to S3 Glacier Deep Archive after 5 days, as this storage class is the most cost-efficient for long-term storage despite its longer retrieval time, which the company is willing to accommodate for disaster recovery purposes.
Question 311 of 529
A company runs its application on Amazon EC2 instances and AWS Lambda functions. The EC2 instances experience a continuous and stable load. The Lambda functions experience a varied and unpredictable load. The application includes a caching layer that uses an Amazon MemoryDB for Redis cluster.
A solutions architect must recommend a solution to minimize the company's overall monthly costs.
Which solution will meet these requirements?
A.
Purchase an EC2 instance Savings Plan to cover the EC2 instances. Purchase a Compute Savings Plan for Lambda to cover the minimum expected consumption of the Lambda functions. Purchase reserved nodes to cover the MemoryDB cache nodes.
B.
Purchase a Compute Savings Plan to cover the EC2 instances. Purchase Lambda reserved concurrency to cover the expected Lambda usage. Purchase reserved nodes to cover the MemoryDB cache nodes.
C.
Purchase a Compute Savings Plan to cover the entire expected cost of the EC2 instances, Lambda functions, and MemoryDB cache nodes.
D.
Purchase a Compute Savings Plan to cover the EC2 instances and the MemoryDB cache nodes. Purchase Lambda reserved concurrency to cover the expected Lambda usage.
AnswerDiscussion
Correct Answer: A
To minimize the company's overall monthly costs while addressing the specific characteristics of the application's load, the best solution is to purchase an EC2 instance Savings Plan to cover the EC2 instances' continuous and stable load. For the varied and unpredictable load experienced by the Lambda functions, a Compute Savings Plan is appropriate to cover the minimum expected consumption. Additionally, purchasing reserved nodes for the MemoryDB cache nodes will provide cost savings for the caching layer. This approach ensures that each component's cost-saving mechanism aligns with its usage pattern.
Question 312 of 529
A company is launching a new online game on Amazon EC2 instances. The game must be available globally. The company plans to run the game in three AWS Regions us-east-1, eu-west-1, and ap-southeast-1. The game's leaderboards, player inventory and event status must be available across Regions.
A solutions architect must design a solution that will give any Region the ability to scale to handle the load of all Regions. Additionally, users must automatically connect to the Region that provides the least latency.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create an EC2 Spot Fleet. Attach the Spot Fleet to a Network Load Balancer (NLB) in each Region. Create an AWS Global Accelerator IP address that points to the NLB. Create an Amazon Route 53 latency-based routing entry for the Global Accelerator IP address. Save the game metadata to an Amazon RDS for MySQL DB instance in each Region. Set up a read replica in the other Regions.
B.
Create an Auto Scaling group for the EC2 instances Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses geoproximity routing and points to the NLB in that Region. Save the game metadata to MySQL databases on EC2 instances in each Region. Set up replication between the database EC2 instances in each Region.
C.
Create an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses latency-based routing and points to the NLB in that Region. Save the game metadata to an Amazon DynamoDB global table.
D.
Use EC2 Global View. Deploy the EC2 instances to each Region. Attach the instances to a Network Load Balancer (NLB). Deploy a DNS server on an EC2 instance in each Region. Set up custom logic on each DNS server to redirect the user to the Region that provides the lowest latency. Save the game metadata to an Amazon Aurora global database.
AnswerDiscussion
Correct Answer: C
The best solution to meet the requirements with the least operational overhead involves creating an Auto Scaling group for the EC2 instances and attaching it to a Network Load Balancer (NLB) in each Region. Using latency-based routing in Amazon Route 53 ensures users automatically connect to the Region with the least latency. Storing the game metadata in an Amazon DynamoDB global table allows for seamless, multi-Region replication of the game's leaderboards, player inventory, and event status. This setup provides high availability and scalability while minimizing operational complexity.
Question 313 of 529
A company is deploying a third-party firewall appliance solution from AWS Marketplace to monitor and protect traffic that leaves the company's AWS environments. The company wants to deploy this appliance into a shared services VPC and route all outbound internet-bound traffic through the appliances.
A solutions architect needs to recommend a deployment method that prioritizes reliability and minimizes failover time between firewall appliances within a single AWS Region. The company has set up routing from the shared services VPC to other VPCs.
Which steps should the solutions architect recommend to meet these requirements? (Choose three.)
A.
Deploy two firewall appliances into the shared services VPC, each in a separate Availability Zone.
B.
Create a new Network Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Network Load Balancer. Add each of the firewall appliance instances to the target group.
C.
Create a new Gateway Load Balancer in the shared services VPCreate a new target group, and attach it to the new Gateway Load Balancer Add each of the firewall appliance instances to the target group.
D.
Create a VPC interface endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.
E.
Deploy two firewall appliances into the shared services VPC, each in the same Availability Zone.
F.
Create a VPC Gateway Load Balancer endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.
AnswerDiscussion
Correct Answer: A, C, F
To ensure high availability and reliability while minimizing failover times, the deployment should involve multiple Availability Zones and use specialized AWS services. Deploying two firewall appliances in separate Availability Zones increases redundancy and fault tolerance. A Gateway Load Balancer (GWLB) is designed to handle traffic routing and balancing for virtual appliances like firewalls. A GWLB endpoint ensures that traffic from other VPCs can be routed to the firewall appliances effectively. These steps ensure that failover times are minimized and reliability is prioritized.
Question 314 of 529
A solutions architect needs to migrate an on-premises legacy application to AWS. The application runs on two servers behind a load balancer. The application requires a license file that is associated with the MAC address of the server's network adapter It takes the software vendor 12 hours to send new license files. The application also uses configuration files with a static IP address to access a database server, host names are not supported.
Given these requirements, which combination of steps should be taken to implement highly available architecture for the application servers in AWS? (Choose two.)
A.
Create a pool of ENIs. Request license files from the vendor for the pool, and store the license files in Amazon S3. Create a bootstrap automation script to download a license file and attach the corresponding ENI to an Amazon EC2 instance.
B.
Create a pool of ENIs. Request license files from the vendor for the pool, store the license files on an Amazon EC2 instance. Create an AMI from the instance and use this AMI for all future EC2 instances.
C.
Create a bootstrap automation script to request a new license file from the vendor .When the response is received, apply the license file to an Amazon EC2 instance.
D.
Edit the bootstrap automation script to read the database server IP address from the AWS Systems Manager Parameter Store, and inject the value into the local configuration files.
E.
Edit an Amazon EC2 instance to include the database server IP address in the configuration files and re-create the AMI to use for all future EC2 stances.
AnswerDiscussion
Correct Answer: A, D
To implement a highly available architecture for the application servers in AWS given the application's licensing and static IP requirements, two steps should be taken. First, create a pool of Elastic Network Interfaces (ENIs). Request license files from the vendor for each ENI in the pool and store these license files in Amazon S3. Then, create a bootstrap automation script to download a license file from S3 and attach the corresponding ENI to an Amazon EC2 instance upon instance startup. This approach ensures the license file associated with the MAC address is effectively managed. Second, edit the bootstrap automation script to read the static IP address of the database server from AWS Systems Manager Parameter Store and inject this IP address into the local configuration files of the application. This ensures the configuration files always have the correct and up-to-date IP address for the database server, maintaining the application's ability to connect to its database.
Question 315 of 529
A company runs its sales reporting application in an AWS Region in the United States. The application uses an Amazon API Gateway Regional API and AWS Lambda functions to generate on-demand reports from data in an Amazon RDS for MySQL database. The frontend of the application is hosted on Amazon S3 and is accessed by users through an Amazon CloudFront distribution. The company is using Amazon Route 53 as the DNS service for the domain. Route 53 is configured with a simple routing policy to route traffic to the API Gateway API.
In the next 6 months, the company plans to expand operations to Europe. More than 90% of the database traffic is read-only traffic. The company has already deployed an API Gateway API and Lambda functions in the new Region.
A solutions architect must design a solution that minimizes latency for users who download reports.
Which solution will meet these requirements?
A.
Use an AWS Database Migration Service (AWS DMS) task with full load to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to latency-based routing to connect to the API Gateway API.
B.
Use an AWS Database Migration Service (AWS DMS) task with full load plus change data capture (CDC) to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API.
C.
Configure a cross-Region read replica for the RDS database in the new Region Change the Route 53 record to latency-based routing to connect to the API Gateway API.
D.
Configure a cross-Region read replica for the RDS database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API.
AnswerDiscussion
Correct Answer: C
To minimize latency for users downloading reports, the optimal solution must ensure data is readily accessible from the closest region. Configuring a cross-Region read replica for the RDS database in the new region ensures that read-only traffic can be handled locally, reducing latency. Additionally, using latency-based routing for the Route 53 record will direct users to the nearest API Gateway API, further minimizing latency. Therefore, configuring a cross-Region read replica and utilizing latency-based routing best meets the requirement.
Question 316 of 529
A software company needs to create short-lived test environments to test pull requests as part of its development process. Each test environment consists of a single Amazon EC2 instance that is in an Auto Scaling group.
The test environments must be able to communicate with a central server to report test results. The central server is located in an on-premises data center. A solutions architect must implement a solution so that the company can create and delete test environments without any manual intervention. The company has created a transit gateway with a VPN attachment to the on-premises network.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create an AWS CloudFormation template that contains a transit gateway attachment and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets to deploy a new stack for each VPC in the account. Deploy a new VPC for each test environment.
B.
Create a single VPC for the test environments. Include a transit gateway attachment and related routing configurations. Use AWS CloudFormation to deploy all test environments into the VPC.
C.
Create a new OU in AWS Organizations for testing. Create an AWS CioudFormation template that contains a VPC, necessary networking resources, a transit gateway attachment, and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets for deployments into each account under the testing OU. Create a new account for each test environment.
D.
Convert the test environment EC2 instances into Docker images. Use AWS CloudFormation to configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in a new VPC, create a transit gateway attachment, and create related routing configurations. Use Kubernetes to manage the deployment and lifecycle of the test environments.
AnswerDiscussion
Correct Answer: B
The solution that involves creating a single VPC for the test environments, including a transit gateway attachment and related routing configurations, and using AWS CloudFormation to deploy all test environments into the VPC, is the one that meets the requirements with the least operational overhead. This approach avoids the complexity of managing multiple VPCs or AWS Organizations accounts and ensures that new test environments can communicate with the on-premises central server efficiently. Additionally, using a single VPC simplifies network management and reduces the overhead associated with creating and deleting test environments.
Question 317 of 529
A company is deploying a new API to AWS. The API uses Amazon API Gateway with a Regional API endpoint and an AWS Lambda function for hosting. The API retrieves data from an external vendor API, stores data in an Amazon DynamoDB global table, and retrieves data from the DynamoDB global table The API key for the vendor's API is stored in AWS Secrets Manager and is encrypted with a customer managed key in AWS Key Management Service (AWS KMS). The company has deployed its own API into a single AWS Region.
A solutions architect needs to change the API components of the company’s API to ensure that the components can run across multiple Regions in an active-active configuration.
Which combination of changes will meet this requirement with the LEAST operational overhead? (Choose three.)
A.
Deploy the API to multiple Regions. Configure Amazon Route 53 with custom domain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy.
B.
Create a new KMS multi-Region customer managed key. Create a new KMS customer managed replica key in each in-scope Region.
C.
Replicate the existing Secrets Manager secret to other Regions. For each in-scope Region's replicated secret, select the appropriate KMS key.
D.
Create a new AWS managed KMS key in each in-scope Region. Convert an existing key to a multiRegion key. Use the multi-Region key in other Regions.
E.
Create a new Secrets Manager secret in each in-scope Region. Copy the secret value from the existing Region to the new secret in each in-scope Region.
F.
Modify the deployment process for the Lambda function to repeat the deployment across in-scope Regions. Turn on the multi-Region option for the existing API. Select the Lambda function that is deployed in each Region as the backend for the multi-Region API.
AnswerDiscussion
Correct Answer: A, B, F
To achieve an active-active multi-region setup for the API with the least operational overhead, the following steps are necessary. Firstly, the API must be deployed to multiple regions, and Amazon Route 53 should be configured with custom domain names that route traffic to each Regional API endpoint using a multivalue answer routing policy. Secondly, a new KMS multi-region customer managed key should be created, along with a customer managed replica key in each in-scope region, to handle encryption needs across regions efficiently. Finally, the deployment process for the Lambda function should be modified to repeat the deployment across all in-scope regions, and the multi-region option should be turned on for the existing API, selecting the respective Lambda function deployed in each region as the backend. This combination ensures regional redundancy and fault tolerance while maintaining low operational complexity.
Question 318 of 529
An online retail company hosts its stateful web-based application and MySQL database in an on-premises data center on a single server. The company wants to increase its customer base by conducting more marketing campaigns and promotions. In preparation, the company wants to migrate its application and database to AWS to increase the reliability of its architecture.
Which solution should provide the HIGHEST level of reliability?
A.
Migrate the database to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon Neptune
B.
Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis replication group.
C.
Migrate the database to Amazon DocumentDB (with MongoDB compatibility). Deploy the application in an Auto Scaling group on Amazon EC2 instances behind a Network Load Balancer Store sessions in Amazon Kinesis Data Firehose.
D.
Migrate the database to an Amazon RDS MariaDB Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon ElastiCache for Memcached.
AnswerDiscussion
Correct Answer: B
Migrating the database to Amazon Aurora MySQL and deploying the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer is a robust solution. Amazon Aurora is highly reliable due to its built-in security, continuous backups, access to read replicas, and automated multi-region replication capabilities. Using Amazon ElastiCache for Redis to store sessions additionally enhances the architecture’s reliability because Redis supports replication and provides high availability and fault tolerance.
Question 319 of 529
A company’s solutions architect needs to provide secure Remote Desktop connectivity to users for Amazon EC2 Windows instances that are hosted in a VPC. The solution must integrate centralized user management with the company's on-premises Active Directory. Connectivity to the VPC is through the internet. The company has hardware that can be used to establish an AWS Site-to-Site VPN connection.
Which solution will meet these requirements MOST cost-effectively?
A.
Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy an EC2 instance as a bastion host in the VPC. Ensure that the EC2 instance is joined to the domain. Use the bastion host to access the target instances through RDP.
B.
Configure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory by using the AWS Directory Service for Microsoft Active Directory AD Connector. Configure permission sets against user groups for access to AWS Systems Manager. Use Systems Manager Fleet Manager to access the target instances through RDP.
C.
Implement a VPN between the on-premises environment and the target VPEnsure that the target instances are joined to the on-premises Active Directory domain over the VPN connection. Configure RDP access through the VPN. Connect from the company’s network to the target instances.
D.
Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy a Remote Desktop Gateway on AWS by using an AWS Quick Start. Ensure that the Remote Desktop Gateway is joined to the domain. Use the Remote Desktop Gateway to access the target instances through RDP.
AnswerDiscussion
Correct Answer: B
The most cost-effective solution is to configure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory by using the AWS Directory Service for Microsoft Active Directory AD Connector. This approach leverages centralized user management integrated with the company's on-premises Active Directory and provides secure Remote Desktop connectivity through AWS Systems Manager Fleet Manager, avoiding the need for additional infrastructure like bastion hosts or VPN-managed connections.
Question 320 of 529
A company's compliance audit reveals that some Amazon Elastic Block Store (Amazon EBS) volumes that were created in an AWS account were not encrypted. A solutions architect must implement a solution to encrypt all new EBS volumes at rest.
Which solution will meet this requirement with the LEAST effort?
A.
Create an Amazon EventBridge rule to detect the creation of unencrypted EBS volumes. Invoke an AWS Lambda function to delete noncompliant volumes.
B.
Use AWS Audit Manager with data encryption.
C.
Create an AWS Config rule to detect the creation of a new EBS volume. Encrypt the volume by using AWS Systems Manager Automation.
D.
Turn on EBS encryption by default in all AWS Regions.
AnswerDiscussion
Correct Answer: D
Turning on EBS encryption by default in all AWS Regions will ensure that all new EBS volumes are automatically encrypted at rest, providing a solution that requires the least effort to manage and implement. This approach eliminates the need for continuous monitoring and correcting noncompliant volumes, thereby simplifying compliance management.
Question 321 of 529
A research company is running daily simulations in the AWS Cloud to meet high demand. The simulations run on several hundred Amazon EC2 instances that are based on Amazon Linux 2. Occasionally, a simulation gets stuck and requires a cloud operations engineer to solve the problem by connecting to an EC2 instance through SSH.
Company policy states that no EC2 instance can use the same SSH key and that all connections must be logged in AWS CloudTrail.
How can a solutions architect meet these requirements?
A.
Launch new EC2 instances, and generate an individual SSH key for each instance. Store the SSH key in AWS Secrets Manager. Create a new IAM policy, and attach it to the engineers’ IAM role with an Allow statement for the GetSecretValue action. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client.
B.
Create an AWS Systems Manager document to run commands on EC2 instances to set a new unique SSH key. Create a new IAM policy, and attach it to the engineers’ IAM role with an Allow statement to run Systems Manager documents. Instruct the engineers to run the document to set an SSH key and to connect through any SSH client.
C.
Launch new EC2 instances without setting up any SSH key for the instances. Set up EC2 Instance Connect on each instance. Create a new IAM policy, and attach it to the engineers’ IAM role with an Allow statement for the SendSSHPublicKey action. Instruct the engineers to connect to the instance by using a browser-based SSH client from the EC2 console.
D.
Set up AWS Secrets Manager to store the EC2 SSH key. Create a new AWS Lambda function to create a new SSH key and to call AWS Systems Manager Session Manager to set the SSH key on the EC2 instance. Configure Secrets Manager to use the Lambda function for automatic rotation once daily. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client.
AnswerDiscussion
Correct Answer: C
To meet the requirements, launch new EC2 instances without setting up any SSH key for the instances. Set up EC2 Instance Connect on each instance, which facilitates using one-time temporary SSH keys that are unique for each connection. By creating a new IAM policy and attaching it to the engineers’ IAM role with an Allow statement for the SendSSHPublicKey action, engineers can connect to the instances using a browser-based SSH client from the EC2 console. This ensures compliance with the policy of not reusing SSH keys and allows all connections to be logged in AWS CloudTrail.
Question 322 of 529
A company is migrating mobile banking applications to run on Amazon EC2 instances in a VPC. Backend service applications run in an on-premises data center. The data center has an AWS Direct Connect connection into AWS. The applications that run in the VPC need to resolve DNS requests to an on-premises Active Directory domain that runs in the data center.
Which solution will meet these requirements with the LEAST administrative overhead?
A.
Provision a set of EC2 instances across two Availability Zones in the VPC as caching DNS servers to resolve DNS queries from the application servers within the VPC.
B.
Provision an Amazon Route 53 private hosted zone. Configure NS records that point to on-premises DNS servers.
C.
Create DNS endpoints by using Amazon Route 53 Resolver. Add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC.
D.
Provision a new Active Directory domain controller in the VPC with a bidirectional trust between this new domain and the on-premises Active Directory domain.
AnswerDiscussion
Correct Answer: C
The solution that meets the requirements with the least administrative overhead is to create DNS endpoints by using Amazon Route 53 Resolver and add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC. This method leverages AWS-managed services, reducing the need for manual infrastructure management and complex configurations. It allows for seamless forwarding of DNS queries between the on-premises Active Directory domain and the VPC applications, minimizing administrative overhead compared to other options such as provisioning EC2 instances or a new Active Directory domain controller.
Question 323 of 529
A company processes environmental data. The company has set up sensors to provide a continuous stream of data from different areas in a city. The data is available in JSON format.
The company wants to use an AWS solution to send the data to a database that does not require fixed schemas for storage. The data must be sent in real time.
Which solution will meet these requirements?
A.
Use Amazon Kinesis Data Firehose to send the data to Amazon Redshift.
B.
Use Amazon Kinesis Data Streams to send the data to Amazon DynamoDB.
C.
Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to send the data to Amazon Aurora.
D.
Use Amazon Kinesis Data Firehose to send the data to Amazon Keyspaces (for Apache Cassandra).
AnswerDiscussion
Correct Answer: B
To meet the requirement of sending real-time environmental data to a database that does not require fixed schemas, the best solution is to use Amazon Kinesis Data Streams to send the data to Amazon DynamoDB. Amazon Kinesis Data Streams is designed to capture and process data in real-time, and Amazon DynamoDB is a NoSQL database that allows for flexibility in the data schema, making it ideal for storing JSON-format data without fixed schemas.
Question 324 of 529
A company is migrating a legacy application from an on-premises data center to AWS. The application uses MongoDB as a key-value database. According to the company's technical guidelines, all Amazon EC2 instances must be hosted in a private subnet without an internet connection. In addition, all connectivity between applications and databases must be encrypted. The database must be able to scale based on demand.
Which solution will meet these requirements?
A.
Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the instance endpoint to connect to Amazon DocumentDB.
B.
Create new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables.
C.
Create new Amazon DynamoDB tables for the application with on-demand capacity. Use an interface VPC endpoint for DynamoDB to connect to the DynamoDB tables.
D.
Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the cluster endpoint to connect to Amazon DocumentDB.
AnswerDiscussion
Correct Answer: B
The solution needs to involve a database that can scale based on demand and ensure that all connectivity between applications and databases is encrypted while being accessible without an internet connection. Amazon DynamoDB is a scalable key-value database suitable for this use and offers on-demand capacity mode, which automatically adjusts to accommodate workload fluctuations. By using a gateway VPC endpoint for DynamoDB, we can ensure that EC2 instances in a private subnet can securely connect to the DynamoDB tables without requiring internet access. This setup aligns well with the company's technical guidelines.
Question 325 of 529
A company is running an application on Amazon EC2 instances in the AWS Cloud. The application is using a MongoDB database with a replica set as its data tier. The MongoDB database is installed on systems in the company’s on-premises data center and is accessible through an AWS Direct Connect connection to the data center environment.
A solutions architect must migrate the on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility).
Which strategy should the solutions architect choose to perform this migration?
A.
Create a fleet of EC2 instances. Install MongoDB Community Edition on the EC2 instances, and create a database. Configure continuous synchronous replication with the database that is running in the on-premises data center.
B.
Create an AWS Database Migration Service (AWS DMS) replication instance. Create a source endpoint for the on-premises MongoDB database by using change data capture (CDC). Create a target endpoint for the Amazon DocumentDB database. Create and run a DMS migration task.
C.
Create a data migration pipeline by using AWS Data Pipeline. Define data nodes for the on-premises MongoDB database and the Amazon DocumentDB database. Create a scheduled task to run the data pipeline.
D.
Create a source endpoint for the on-premises MongoDB database by using AWS Glue crawlers. Configure continuous asynchronous replication between the MongoDB database and the Amazon DocumentDB database.
AnswerDiscussion
Correct Answer: B
The most appropriate strategy for migrating an on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility) is to use AWS Database Migration Service (AWS DMS). By creating a DMS replication instance and using change data capture (CDC) technology, one can accurately capture and replicate ongoing changes from the source MongoDB database to the target Amazon DocumentDB database. This approach ensures a smooth transition with minimal downtime and data integrity during the migration process.
Question 326 of 529
A company is rearchitecting its applications to run on AWS. The company’s infrastructure includes multiple Amazon EC2 instances. The company's development team needs different levels of access. The company wants to implement a policy that requires all Windows EC2 instances to be joined to an Active Directory domain on AWS. The company also wants to implement enhanced security processes such as multi-factor authentication (MFA). The company wants to use managed AWS services wherever possible.
Which solution will meet these requirements?
A.
Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks.
B.
Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.
C.
Create an AWS Directory Service Simple AD implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.
D.
Create an AWS Directory Service Simple AD implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks.
AnswerDiscussion
Correct Answer: A
To meet the requirement of having all Windows EC2 instances joined to an Active Directory domain on AWS, as well as implementing multi-factor authentication and using managed AWS services wherever possible, the best solution is to create an AWS Directory Service for Microsoft Active Directory implementation. Using Amazon WorkSpaces for domain security configuration tasks is a fully managed service that provides a Windows desktop environment in the AWS Cloud, making it easier to manage and secure compared to setting up and configuring an EC2 instance manually.
Question 327 of 529
A company wants to migrate its on-premises application to AWS. The database for the application stores structured product data and temporary user session data. The company needs to decouple the product data from the user session data. The company also needs to implement replication in another AWS Region for disaster recovery.
Which solution will meet these requirements with the HIGHEST performance?
A.
Create an Amazon RDS DB instance with separate schemas to host the product data and the user session data. Configure a read replica for the DB instance in another Region.
B.
Create an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create a global datastore in Amazon ElastiCache for Memcached to host the user session data.
C.
Create two Amazon DynamoDB global tables. Use one global table to host the product data. Use the other global table to host the user session data. Use DynamoDB Accelerator (DAX) for caching.
D.
Create an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create an Amazon DynamoDB global table to host the user session data.
AnswerDiscussion
Correct Answer: B
To achieve high performance while meeting the need to decouple product data from user session data, Amazon RDS for the structured product data combined with Amazon ElastiCache for Memcached for the user session data provides a compelling solution. RDS is well-suited for structured data due to its relational model capabilities, and ElastiCache excels at managing temporary session data with high-speed access. Furthermore, configuring RDS with a read replica in another Region ensures cross-region replication and thus disaster recovery. This setup ensures high performance and fault tolerance. Although option D provides a similar setup, it does not leverage the high-performance caching that ElastiCache offers for session data. Therefore, option B better satisfies the requirements. While there are claims about global datastore in ElastiCache, it should be treated with caution, but generally, ElastiCache remains efficient for caching purposes.
Question 328 of 529
A company orchestrates a multi-account structure on AWS by using AWS Control Tower. The company is using AWS Organizations, AWS Config, and AWS Trusted Advisor. The company has a specific OU for development accounts that developers use to experiment on AWS. The company has hundreds of developers, and each developer has an individual development account.
The company wants to optimize costs in these development accounts. Amazon EC2 instances and Amazon RDS instances in these accounts must be burstable. The company wants to disallow the use of other services that are not relevant.
What should a solutions architect recommend to meet these requirements?
A.
Create a custom SCP in AWS Organizations to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the SCP to the development OU.
B.
Create a custom detective control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU.
C.
Create a custom preventive control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU.
D.
Create an AWS Config rule in the AWS Control Tower account. Configure the AWS Config rule to allow the deployment of only burstable instances and to disallow services that are not relevant. Deploy the AWS Config rule to the development OU by using AWS CloudFormation StackSets.
AnswerDiscussion
Correct Answer: C
To meet the requirement of allowing only burstable EC2 and RDS instances while disallowing irrelevant services in development accounts, a custom preventive control (guardrail) in AWS Control Tower is the best option. Preventive controls are meant to enforce compliance by preventing actions that do not meet specified criteria. By configuring this control to allow only burstable instances and applying it to the development OU, the company can ensure that the desired constraints are enforced in all development accounts.
Question 329 of 529
A financial services company runs a complex, multi-tier application on Amazon EC2 instances and AWS Lambda functions. The application stores temporary data in Amazon S3. The S3 objects are valid for only 45 minutes and are deleted after 24 hours.
The company deploys each version of the application by launching an AWS CloudFormation stack. The stack creates all resources that are required to run the application. When the company deploys and validates a new application version, the company deletes the CloudFormation stack of the old version.
The company recently tried to delete the CloudFormation stack of an old application version, but the operation failed. An analysis shows that CloudFormation failed to delete an existing S3 bucket. A solutions architect needs to resolve this issue without making major changes to the application's architecture.
Which solution meets these requirements?
A.
Implement a Lambda function that deletes all files from a given S3 bucket. Integrate this Lambda function as a custom resource into the CloudFormation stack. Ensure that the custom resource has a DependsOn attribute that points to the S3 bucket's resource.
B.
Modify the CloudFormation template to provision an Amazon Elastic File System (Amazon EFS) file system to store the temporary files there instead of in Amazon S3. Configure the Lambda functions to run in the same VPC as the file system. Mount the file system to the EC2 instances and Lambda functions.
C.
Modify the CloudF ormation stack to create an S3 Lifecycle rule that expires all objects 45 minutes after creation. Add a DependsOn attribute that points to the S3 bucket’s resource.
D.
Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket.
AnswerDiscussion
Correct Answer: A
To delete an S3 bucket successfully in a CloudFormation stack, the bucket must be empty. The most effective solution is to use a Lambda function that will clean up the bucket's contents before deletion. This approach ensures that the S3 bucket can be deleted without making significant changes to the application's architecture. Integrating this Lambda function as a custom resource into the CloudFormation stack and setting the DependsOn attribute to point to the S3 bucket's resource ensures proper sequencing of the deletion steps, allowing the function to clear the bucket before the stack attempts to delete it.
Question 330 of 529
A company has developed a mobile game. The backend for the game runs on several virtual machines located in an on-premises data center. The business logic is exposed using a REST API with multiple functions. Player session data is stored in central file storage. Backend services use different API keys for throttling and to distinguish between live and test traffic.
The load on the game backend varies throughout the day. During peak hours, the server capacity is not sufficient. There are also latency issues when fetching player session data. Management has asked a solutions architect to present a cloud architecture that can handle the game’s varying load and provide low-latency data access. The API model should not be changed.
Which solution meets these requirements?
A.
Implement the REST API using a Network Load Balancer (NLB). Run the business logic on an Amazon EC2 instance behind the NLB. Store player session data in Amazon Aurora Serverless.
B.
Implement the REST API using an Application Load Balancer (ALB). Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.
C.
Implement the REST API using Amazon API Gateway. Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.
D.
Implement the REST API using AWS AppSync. Run the business logic in AWS Lambda. Store player session data in Amazon Aurora Serverless.
AnswerDiscussion
Correct Answer: C
To handle the game's varying load and ensure low-latency data access, using Amazon API Gateway to implement the REST API is an optimal choice as it can efficiently manage fluctuating loads and scale automatically. Running the business logic in AWS Lambda is suitable because Lambda scales automatically with the load and removes the need for server management. Storing player session data in Amazon DynamoDB with on-demand capacity ensures low-latency access and can handle high request rates without manual intervention, making the architecture more resilient and responsive to traffic spikes.
Question 331 of 529
A company is migrating an application to the AWS Cloud. The application runs in an on-premises data center and writes thousands of images into a mounted NFS file system each night. After the company migrates the application, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system.
The company has established an AWS Direct Connect connection to AWS. Before the migration cutover, a solutions architect must build a process that will replicate the newly created on-premises images to the EFS file system.
What is the MOST operationally efficient way to replicate the images?
A.
Configure a periodic process to run the aws s3 sync command from the on-premises file system to Amazon S3. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.
B.
Deploy an AWS Storage Gateway file gateway with an NFS mount point. Mount the file gateway file system on the on-premises server. Configure a process to periodically copy the images to the mount point.
C.
Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an S3 bucket by using a public VIF. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.
D.
Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Configure a DataSync scheduled task to send the images to the EFS file system every 24 hours.
AnswerDiscussion
Correct Answer: D
The most operationally efficient way to replicate images from an on-premises NFS file system to an Amazon EFS file system is to deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. DataSync is specifically designed to automate transferring large amounts of data between on-premises storage and AWS storage services, such as EFS. By using DataSync over the AWS Direct Connect connection, you can leverage a private and high-bandwidth network link, ensuring secure and efficient data transfer. Configuring a DataSync scheduled task to send the images directly to the EFS file system every 24 hours simplifies the process and reduces the need for intermediate storage or additional processing steps. This method provides a straightforward, efficient, and reliable solution for the data replication task.
Question 332 of 529
A company recently migrated a web application from an on-premises data center to the AWS Cloud. The web application infrastructure consists of an Amazon CloudFront distribution that routes to an Application Load Balancer (ALB), with Amazon Elastic Container Service (Amazon ECS) to process requests. A recent security audit revealed that the web application is accessible by using both CloudFront and ALB endpoints. However, the company requires that the web application must be accessible only by using the CloudFront endpoint.
Which solution will meet this requirement with the LEAST amount of effort?
A.
Create a new security group and attach it to the CloudFront distribution. Update the ALB security group ingress to allow access only from the CloudFront security group.
B.
Update ALB security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing CloudFront managed prefix list.
C.
Create a com.amazonaws.region.elasticloadbalancing VPC interface endpoint for Elastic Load Balancing. Update the ALB scheme from internet-facing to internal.
D.
Extract CloudFront IPs from the AWS provided ip-ranges.json document. Update ALB security group ingress to allow access only from CloudFront IPs.
AnswerDiscussion
Correct Answer: B
To restrict access to the web application so it is accessible only via the CloudFront endpoint, the simplest and most effective solution involves updating the ALB security group ingress rules. By allowing access only from the CloudFront managed prefix list (com.amazonaws.global.cloudfront.origin-facing), you can ensure that requests reaching the ALB are exclusively those forwarded from CloudFront. This method is straightforward and requires minimal effort compared to other options, which involve more complex configurations or manual updates.
Question 333 of 529
A company hosts a community forum site using an Application Load Balancer (ALB) and a Docker application hosted in an Amazon ECS cluster. The site data is stored in Amazon RDS for MySQL and the container image is stored in ECR. The company needs to provide their customers with a disaster recovery SLA with an RTO of no more than 24 hours and RPO of no more than 8 hours.
Which of the following solutions is the MOST cost-effective way to meet the requirements?
A.
Use AWS CloudFormation to deploy identical ALB, EC2, ECS and RDS resources in two regions. Schedule RDS snapshots every 8 hours. Use RDS multi-region replication to update the secondary region's copy of the database. In the event of a failure, restore from the latest snapshot, and use an Amazon Route 53 DNS failover policy to automatically redirect customers to the ALB in the secondary region.
B.
Store the Docker image in ECR in two regions. Schedule RDS snapshots every 8 hours with snapshots copied to the secondary region. In the event of a failure, use AWS CloudFormation to deploy the ALB, EC2, ECS and RDS resources in the secondary region, restore from the latest snapshot, and update the DNS record to point to the ALB in the secondary region.
C.
Use AWS CloudFormation to deploy identical ALB, EC2, ECS, and RDS resources in a secondary region. Schedule hourly RDS MySQL backups to Amazon S3 and use cross-region replication to replicate data to a bucket in the secondary region. In the event of a failure, import the latest Docker image to Amazon ECR in the secondary region, deploy to the EC2 instance, restore the latest MySQL backup, and update the DNS record to point to the ALB in the secondary region.
D.
Deploy a pilot light environment in a secondary region with an ALB and a minimal resource EC2 deployment for Docker in an AWS Auto Scaling group with a scaling policy to increase instance size and number of nodes. Create a cross-region read replica of the RDS data. In the event of a failure, promote the replica to primary, and update the DNS record to point to the ALB in the secondary region.
AnswerDiscussion
Correct Answer: D
To meet the requirements of an RTO of 24 hours and an RPO of 8 hours in the most cost-effective manner, deploying a pilot light environment is ideal. This involves maintaining minimum resources running at all times in a secondary region, which keeps costs low while ensuring quick recovery. The pilot light environment includes an ALB and a minimal EC2 setup that can be scaled up quickly. Using a cross-region read replica for the RDS data ensures that the data is kept up-to-date within the RPO of 8 hours. In case of a failure, the read replica can be promoted to primary, and the DNS record can be updated to point to the ALB in the secondary region, ensuring that the recovery can be completed within the 24-hour RTO.
Question 334 of 529
A company is migrating its infrastructure to the AWS Cloud. The company must comply with a variety of regulatory standards for different projects. The company needs a multi-account environment.
A solutions architect needs to prepare the baseline infrastructure. The solution must provide a consistent baseline of management and security, but it must allow flexibility for different compliance requirements within various AWS accounts. The solution also needs to integrate with the existing on-premises Active Directory Federation Services (AD FS) server.
Which solution meets these requirements with the LEAST amount of operational overhead?
A.
Create an organization in AWS Organizations. Create a single SCP for least privilege access across all accounts. Create a single OU for all accounts. Configure an IAM identity provider for federation with the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with conformance packs for all accounts.
B.
Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Add OUs as necessary. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server.
C.
Create an organization in AWS Organizations. Create SCPs for least privilege access. Create an OU structure, and use it to group AWS accounts. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with aggregators and conformance packs.
D.
Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Configure an IAM identity provider for federation with the on-premises AD FS server.
AnswerDiscussion
Correct Answer: B
The solution that provides the least amount of operational overhead while fulfilling the company's requirements is to create an organization in AWS Organizations, enable AWS Control Tower, and connect AWS IAM Identity Center to the on-premises AD FS server. AWS Control Tower automates the setup of a multi-account environment and includes best-practice blueprints for governance, thus reducing manual configuration and ensuring consistency. IAM Identity Center provides seamless integration for workforce authentication and authorization with the existing AD FS. This approach ensures a consistent baseline of management and security while allowing for the necessary flexibility across various AWS accounts.
Question 335 of 529
An online magazine will launch its latest edition this month. This edition will be the first to be distributed globally. The magazine's dynamic website currently uses an Application Load Balancer in front of the web tier, a fleet of Amazon EC2 instances for web and application servers, and Amazon Aurora MySQL. Portions of the website include static content and almost all traffic is read-only.
The magazine is expecting a significant spike in internet traffic when the new edition is launched. Optimal performance is a top priority for the week following the launch.
Which combination of steps should a solutions architect take to reduce system response times for a global audience? (Choose two.)
A.
Use logical cross-Region replication to replicate the Aurora MySQL database to a secondary Region. Replace the web servers with Amazon S3. Deploy S3 buckets in cross-Region replication mode.
B.
Ensure the web and application tiers are each in Auto Scaling groups. Introduce an AWS Direct Connect connection. Deploy the web and application tiers in Regions across the world.
C.
Migrate the database from Amazon Aurora to Amazon RDS for MySQL. Ensure all three of the application tiers – web, application, and database – are in private subnets.
D.
Use an Aurora global database for physical cross-Region replication. Use Amazon S3 with cross-Region replication for static content and resources. Deploy the web and application tiers in Regions across the world.
E.
Introduce Amazon Route 53 with latency-based routing and Amazon CloudFront distributions. Ensure the web and application tiers are each in Auto Scaling groups.
AnswerDiscussion
Correct Answer: D, E
To reduce system response times for a global audience and ensure optimal performance during the launch, using an Aurora global database for physical cross-Region replication allows for faster data access across different regions. Additionally, deploying Amazon S3 with cross-Region replication ensures that static content is delivered quickly regardless of the user's geographic location. Introducing Amazon Route 53 with latency-based routing helps direct users to the nearest deployment, reducing latency, while Amazon CloudFront distributions provide faster delivery of static and dynamic content through edge locations. Ensuring the web and application tiers are each in Auto Scaling groups allows the infrastructure to handle varying load levels efficiently.
Question 336 of 529
An online gaming company needs to optimize the cost of its workloads on AWS. The company uses a dedicated account to host the production environment for its online gaming application and an analytics application.
Amazon EC2 instances host the gaming application and must always be available. The EC2 instances run all year. The analytics application uses data that is stored in Amazon S3. The analytics application can be interrupted and resumed without issue.
Which solution will meet these requirements MOST cost-effectively?
A.
Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use On-Demand Instances for the analytics application.
B.
Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use Spot Instances for the analytics application.
C.
Use Spot Instances for the online gaming application and the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount.
D.
Use On-Demand Instances for the online gaming application. Use Spot Instances for the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount.
AnswerDiscussion
Correct Answer: B
To optimize costs while ensuring reliability for the production environment of the online gaming application, an EC2 Instance Savings Plan should be used for the gaming application instances. This ensures that these instances, which need to run all year and be continuously available, are covered at a reduced cost compared to On-Demand pricing. For the analytics application, which can be interrupted and resumed without issue, Spot Instances are the most cost-effective solution. Spot Instances allow you to take advantage of unused EC2 capacity at significantly reduced prices. This combination satisfies the requirements most cost-effectively.
Question 337 of 529
A company runs applications in hundreds of production AWS accounts. The company uses AWS Organizations with all features enabled and has a centralized backup operation that uses AWS Backup.
The company is concerned about ransomware attacks. To address this concern, the company has created a new policy that all backups must be resilient to breaches of privileged-user credentials in any production account.
Which combination of steps will meet this new requirement? (Choose three.)
A.
Implement cross-account backup with AWS Backup vaults in designated non-production accounts.
B.
Add an SCP that restricts the modification of AWS Backup vaults.
C.
Implement AWS Backup Vault Lock in compliance mode.
C. Implement least privilege access for the IAM service role that is assigned to AWS Backup.
D.
Configure the backup frequency, lifecycle, and retention period to ensure that at least one backup always exists in the cold tier.
E.
Configure AWS Backup to write all backups to an Amazon S3 bucket in a designated non-production account. Ensure that the S3 bucket has S3 Object Lock enabled.
AnswerDiscussion
Correct Answer: A, B, C
To ensure backups are resilient to breaches of privileged-user credentials, implement cross-account backups with AWS Backup vaults in designated non-production accounts to isolate backups from potential breaches. Adding a Service Control Policy (SCP) that restricts the modification of AWS Backup vaults enhances security by preventing unauthorized changes. Implementing AWS Backup Vault Lock in compliance mode ensures that backups cannot be modified or deleted during their retention period, providing extra protection against compromised credentials.
Question 338 of 529
A company needs to aggregate Amazon CloudWatch logs from its AWS accounts into one central logging account. The collected logs must remain in the AWS Region of creation. The central logging account will then process the logs, normalize the logs into standard output format, and stream the output logs to a security tool for more processing.
A solutions architect must design a solution that can handle a large volume of logging data that needs to be ingested. Less logging will occur outside normal business hours than during normal business hours. The logging solution must scale with the anticipated load. The solutions architect has decided to use an AWS Control Tower design to handle the multi-account logging process.
Which combination of steps should the solutions architect take to meet the requirements? (Choose three.)
A.
Create a destination Amazon Kinesis data stream in the central logging account.
B.
Create a destination Amazon Simple Queue Service (Amazon SQS) queue in the central logging account.
C.
Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Kinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a subscription filter for each log group to send data to the Kinesis data stream.
D.
Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Simple Queue Service (Amazon SQS) queue. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a single subscription filter for all log groups to send data to the SQS queue.
E.
Create an AWS Lambda function. Program the Lambda function to normalize the logs in the central logging account and to write the logs to the security tool.
F.
Create an AWS Lambda function. Program the Lambda function to normalize the logs in the member accounts and to write the logs to the security tool.
AnswerDiscussion
Correct Answer: A, C, E
For aggregating Amazon CloudWatch logs from multiple AWS accounts, it's efficient to use Amazon Kinesis Data Streams as the destination in the central logging account because it can handle large volumes of data and allows for real-time processing. Creating an IAM role that grants CloudWatch Logs permission to add data to the Kinesis data stream with the appropriate subscription filter ensures logs are correctly routed from member accounts. An AWS Lambda function programmed within the central logging account will normalize these logs and forward them to the security tool, meeting the requirements for data processing and streaming.
Question 339 of 529
A company is migrating a legacy application from an on-premises data center to AWS. The application consists of a single application server and a Microsoft SQL Server database server. Each server is deployed on a VMware VM that consumes 500 TB of data across multiple attached volumes.
The company has established a 10 Gbps AWS Direct Connect connection from the closest AWS Region to its on-premises data center. The Direct Connect connection is not currently in use by other services.
Which combination of steps should a solutions architect take to migrate the application with the LEAST amount of downtime? (Choose two.)
A.
Use an AWS Server Migration Service (AWS SMS) replication job to migrate the database server VM to AWS.
B.
Use VM Import/Export to import the application server VM.
C.
Export the VM images to an AWS Snowball Edge Storage Optimized device.
D.
Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS.
E.
Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance.
AnswerDiscussion
Correct Answer: D, E
To migrate the application with the least amount of downtime, the solution architect should use AWS Server Migration Service (AWS SMS) to migrate the application server VM and AWS Database Migration Service (AWS DMS) to migrate the database to an Amazon RDS DB instance. AWS SMS allows for automated, incremental replication of VMs to the cloud, minimizing downtime during the cutover process. AWS DMS provides continuous replication and is specifically designed for migrating databases with minimal downtime. This combination ensures that both the application server and the database server are migrated in an efficient and timely manner, reducing the amount of downtime experienced during the migration.
Question 340 of 529
A company operates a fleet of servers on premises and operates a fleet of Amazon EC2 instances in its organization in AWS Organizations. The company's AWS accounts contain hundreds of VPCs. The company wants to connect its AWS accounts to its on-premises network. AWS Site-to-Site VPN connections are already established to a single AWS account. The company wants to control which VPCs can communicate with other VPCs.
Which combination of steps will achieve this level of control with the LEAST operational effort? (Choose three.)
A.
Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM).
B.
Configure attachments to all VPCs and VPNs.
C.
Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables.
D.
Configure VPC peering between the VPCs.
E.
Configure attachments between the VPCs and VPNs.
F.
Setup route tables on the VPCs and VPNs.
AnswerDiscussion
Correct Answer: A, B, C
To achieve control over VPC communication with the least operational effort, the following combination of steps is necessary: First, create a transit gateway in an AWS account and share it across accounts using AWS Resource Access Manager (AWS RAM). This allows centralized control over network connectivity. Second, configure attachments to all VPCs and VPNs, which involves attaching each VPC and VPN to the transit gateway, thereby enabling connectivity through the gateway. Lastly, set up transit gateway route tables and associate the VPCs and VPNs with the route tables, which will allow fine-grained control over the routing between different VPCs and the on-premises network. This setup avoids the need for complex VPC peering arrangements and manual route table updates in individual VPCs.
Question 341 of 529
A company needs to optimize the cost of its application on AWS. The application uses AWS Lambda functions and Amazon Elastic Container Service (Amazon ECS) containers that run on AWS Fargate. The application is write-heavy and stores data in an Amazon Aurora MySQL database.
The load on the application is not consistent. The application experiences long periods of no usage, followed by sudden and significant increases and decreases in traffic. The database runs on a memory optimized DB instance that cannot handle the load.
A solutions architect must design a solution that can scale to handle the changes in traffic.
Which solution will meet these requirements MOST cost-effectively?
A.
Add additional read replicas to the database. Purchase Instance Savings Plans and RDS Reserved Instances.
B.
Migrate the database to an Aurora DB cluster that has multiple writer instances. Purchase Instance Savings Plans.
C.
Migrate the database to an Aurora global database. Purchase Compute Savings Plans and RDS Reserved instances.
D.
Migrate the database to Aurora Serverless v1. Purchase Compute Savings Plans.
AnswerDiscussion
Correct Answer: D
To meet the requirements of handling sudden and significant increases and decreases in traffic cost-effectively, migrating the database to Aurora Serverless v1 is the best solution. Aurora Serverless v1 is designed to automatically scale the database resources up or down based on the application's demand, eliminating the need to manually manage the database capacity. This makes it a cost-effective option for workloads with variable traffic patterns, ensuring that you only pay for the resources you use. Additionally, purchasing Compute Savings Plans further optimizes costs by offering savings on a consistent amount of compute usage, even if the specific instances change.
Question 342 of 529
A company migrated an application to the AWS Cloud. The application runs on two Amazon EC2 instances behind an Application Load Balancer (ALB).
Application data is stored in a MySQL database that runs on an additional EC2 instance. The application's use of the database is read-heavy.
The application loads static content from Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. The static content is updated frequently and must be copied to each EBS volume.
The load on the application changes throughout the day. During peak hours, the application cannot handle all the incoming requests. Trace data shows that the database cannot handle the read load during peak hours.
Which solution will improve the reliability of the application?
A.
Migrate the application to a set of AWS Lambda functions. Set the Lambda functions as targets for the ALB. Create a new single EBS volume for the static content. Configure the Lambda functions to read from the new EBS volume. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster.
B.
Migrate the application to a set of AWS Step Functions state machines. Set the state machines as targets for the ALCreate an Amazon Elastic File System (Amazon EFS) file system for the static content. Configure the state machines to read from the EFS file system. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance.
C.
Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create a new single EBS volume for the static content. Mount the new EBS volume on the ECS cluster. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster.
D.
Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create an Amazon Elastic File System (Amazon EFS) file system for the static content. Mount the EFS file system to each container. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance.
AnswerDiscussion
Correct Answer: D
To improve the reliability of the application, containerizing it on Amazon Elastic Container Service (ECS) with the AWS Fargate launch type is an effective approach. Using Amazon EFS for static content allows for easier sharing and scalability, as EFS can be mounted across multiple containers. Additionally, enabling AWS Application Auto Scaling ensures that the ECS cluster can handle varying loads by adjusting resources automatically. Migrating the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance addresses the read-heavy requirement and ensures that the database can scale to handle peak load without manual intervention. This combination provides a scalable, highly available, and reliable solution for the application.
Question 343 of 529
A solutions architect wants to make sure that only AWS users or roles with suitable permissions can access a new Amazon API Gateway endpoint. The solutions architect wants an end-to-end view of each request to analyze the latency of the request and create service maps.
How can the solutions architect design the API Gateway access control and perform request inspections?
A.
For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.
B.
For the API Gateway resource, set CORS to enabled and only return the company's domain in Access-Control-Allow-Origin headers. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.
C.
Create an AWS Lambda function as the custom authorizer, ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system. Use AWS X-Ray to trace and analyze user requests to API Gateway.
D.
Create a client certificate for API Gateway. Distribute the certificate to the AWS users and roles that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.
AnswerDiscussion
Correct Answer: A
To ensure that only AWS users or roles with appropriate permissions can access the new Amazon API Gateway endpoint, the API Gateway method should have its authorization set to AWS_IAM. This allows you to define the IAM user or role with execute-api:Invoke permission on the REST API resource. The API caller then signs requests using AWS Signature when accessing the endpoint to ensure secure access. To analyze the latency and create service maps from an end-to-end perspective, AWS X-Ray can be used to trace and analyze user requests to the API Gateway. This approach meets the requirements for both access control and detailed request inspection.
Question 344 of 529
A company is using AWS CodePipeline for the CI/CD of an application to an Amazon EC2 Auto Scaling group. All AWS resources are defined in AWS CloudFormation templates. The application artifacts are stored in an Amazon S3 bucket and deployed to the Auto Scaling group using instance user data scripts. As the application has become more complex, recent resource changes in the CloudFormation templates have caused unplanned downtime.
How should a solutions architect improve the CI/CD pipeline to reduce the likelihood that changes in the templates will cause downtime?
A.
Adapt the deployment scripts to detect and report CloudFormation error conditions when performing deployments. Write test plans for a testing team to run in a non-production environment before approving the change for production.
B.
Implement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed.
C.
Use plugins for the integrated development environment (IDE) to check the templates for errors, and use the AWS CLI to validate that the templates are correct. Adapt the deployment code to check for error conditions and generate notifications on errors. Deploy to a test environment and run a manual test plan before approving the change for production.
D.
Use AWS CodeDeploy and a blue/green deployment pattern with CloudFormation to replace the user data deployment scripts. Have the operators log in to running instances and go through a manual test plan to verify the application is running as expected.
AnswerDiscussion
Correct Answer: B
To reduce the likelihood that changes in the CloudFormation templates will cause downtime, a solutions architect should implement automated testing to catch issues early and use deployment strategies that minimize the impact of changes. Automated testing using AWS CodeBuild can validate the changes in a test environment, reducing human error and ensuring the changes do not break the application. Implementing CloudFormation change sets allows for a precise evaluation of changes before deploying them. Using AWS CodeDeploy with blue/green deployment patterns allows for a smooth transition between current and new versions, minimizing downtime and enabling easy rollback if issues are detected.
Question 345 of 529
A North American company with headquarters on the East Coast is deploying a new web application running on Amazon EC2 in the us-east-1 Region. The application should dynamically scale to meet user demand and maintain resiliency. Additionally, the application must have disaster recovery capabilities in an active-passive configuration with the us-west-1 Region.
Which steps should a solutions architect take after creating a VPC in the us-east-1 Region?
A.
Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs in each Region as part of an Auto Scaling group spanning both VPCs and served by the ALB.
B.
Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALDeploy the same solution to the us-west-1 Region. Create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both Regions.
C.
Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) that spans both VPCs. Deploy EC2 instances across multiple Availability Zones as part of an Auto Scaling group in each VPC served by the ALB. Create an Amazon Route 53 record that points to the ALB.
D.
Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create separate Amazon Route 53 records in each Region that point to the ALB in the Region. Use Route 53 health checks to provide high availability across both Regions.
AnswerDiscussion
Correct Answer: B
After creating a VPC in the us-east-1 Region, you should deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) in that VPC. Then, you should deploy EC2 instances across those multiple AZs as part of an Auto Scaling group served by the ALB. To ensure disaster recovery and resiliency, deploy the same solution to the us-west-1 Region. Finally, create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both Regions. This setup ensures that the application can dynamically scale, maintain resiliency, and achieve disaster recovery in an active-passive configuration.
Question 346 of 529
A company has a legacy application that runs on multiple NET Framework components. The components share the same Microsoft SQL Server database and communicate with each other asynchronously by using Microsoft Message Queueing (MSMQ).
The company is starting a migration to containerized .NET Core components and wants to refactor the application to run on AWS. The .NET Core components require complex orchestration. The company must have full control over networking and host configuration. The application's database model is strongly relational.
Which solution will meet these requirements?
A.
Host the INET Core components on AWS App Runner. Host the database on Amazon RDS for SQL Server. Use Amazon EventBiridge for asynchronous messaging.
B.
Host the .NET Core components on Amazon Elastic Container Service (Amazon ECS) with the AWS Fargate launch type. Host the database on Amazon DynamoDUse Amazon Simple Notification Service (Amazon SNS) for asynchronous messaging.
C.
Host the .NET Core components on AWS Elastic Beanstalk. Host the database on Amazon Aurora PostgreSQL Serverless v2. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) for asynchronous messaging.
D.
Host the NET Core components on Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type. Host the database on Amazon Aurora MySQL Serverless v2. Use Amazon Simple Queue Service (Amazon SQS) for asynchronous messaging.
AnswerDiscussion
Correct Answer: D
Hosting the .NET Core components on Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type provides the necessary control over networking and host configuration, which is crucial for the application's complex orchestration needs. Amazon Aurora MySQL Serverless v2 supports a strongly relational database model, which aligns with the application's requirements. Finally, Amazon Simple Queue Service (Amazon SQS) is a suitable replacement for MSMQ for asynchronous messaging, ensuring smooth communication between components.
Question 347 of 529
A solutions architect has launched multiple Amazon EC2 instances in a placement group within a single Availability Zone. Because of additional load on the system, the solutions architect attempts to add new instances to the placement group. However, the solutions architect receives an insufficient capacity error.
What should the solutions architect do to troubleshoot this issue?
A.
Use a spread placement group. Set a minimum of eight instances for each Availability Zone.
B.
Stop and start all the instances in the placement group. Try the launch again.
C.
Create a new placement group. Merge the new placement group with the original placement group.
D.
Launch the additional instances as Dedicated Hosts in the placement groups.
AnswerDiscussion
Correct Answer: B
To troubleshoot an insufficient capacity error when adding new instances to an existing placement group, the best approach is to stop and start all instances in the placement group. This action can potentially migrate the instances to hardware with sufficient capacity to fulfill the request for additional instances.
Question 348 of 529
A company has used infrastructure as code (IaC) to provision a set of two Amazon EC2 instances. The instances have remained the same for several years.
The company's business has grown rapidly in the past few months. In response, the company’s operations team has implemented an Auto Scaling group to manage the sudden increases in traffic. Company policy requires a monthly installation of security updates on all operating systems that are running.
The most recent security update required a reboot. As a result, the Auto Scaling group terminated the instances and replaced them with new, unpatched instances.
Which combination of steps should a solutions architect recommend to avoid a recurrence of this issue? (Choose two.)
A.
Modify the Auto Scaling group by setting the Update policy to target the oldest launch configuration for replacement.
B.
Create a new Auto Scaling group before the next patch maintenance. During the maintenance window, patch both groups and reboot the instances.
C.
Create an Elastic Load Balancer in front of the Auto Scaling group. Configure monitoring to ensure that target group health checks return healthy after the Auto Scaling group replaces the terminated instances.
D.
Create automation scripts to patch an AMI, update the launch configuration, and invoke an Auto Scaling instance refresh.
E.
Create an Elastic Load Balancer in front of the Auto Scaling group. Configure termination protection on the instances.
AnswerDiscussion
Correct Answer: A, D
The issue arises because the Auto Scaling group replaces instances after a security update that requires a reboot, resulting in new, unpatched instances being created. To avoid this, actions must ensure instances are up-to-date and not unnecessarily replaced. Modifying the Auto Scaling group to target the oldest launch configuration for replacement ensures that the oldest, and potentially least secure, instances are replaced first, which can help maintain a more updated and secure environment. Creating automation scripts to patch an AMI, update the launch configuration, and invoke an Auto Scaling instance refresh ensures that new instances launched by the Auto Scaling group will be up-to-date with the latest security patches, thereby solving the root cause of the issue. This approach aligns with maintaining security compliance and operational efficiency.
Question 349 of 529
A team of data scientists is using Amazon SageMaker instances and SageMaker APIs to train machine learning (ML) models. The SageMaker instances are deployed in a VPC that does not have access to or from the internet. Datasets for ML model training are stored in an Amazon S3 bucket. Interface VPC endpoints provide access to Amazon S3 and the SageMaker APIs.
Occasionally, the data scientists require access to the Python Package Index (PyPI) repository to update Python packages that they use as part of their workflow. A solutions architect must provide access to the PyPI repository while ensuring that the SageMaker instances remain isolated from the internet.
Which solution will meet these requirements?
A.
Create an AWS CodeCommit repository for each package that the data scientists need to access. Configure code synchronization between the PyPI repository and the CodeCommit repository. Create a VPC endpoint for CodeCommit.
B.
Create a NAT gateway in the VPC. Configure VPC routes to allow access to the internet with a network ACL that allows access to only the PyPI repository endpoint.
C.
Create a NAT instance in the VPConfigure VPC routes to allow access to the internet. Configure SageMaker notebook instance firewall rules that allow access to only the PyPI repository endpoint.
D.
Create an AWS CodeArtifact domain and repository. Add an external connection for public:pypi to the CodeArtifact repository. Configure the Python client to use the CodeArtifact repository. Create a VPC endpoint for CodeArtifact.
AnswerDiscussion
Correct Answer: D
To meet the requirement of providing access to the PyPI repository while ensuring that the SageMaker instances remain isolated from the internet, the best solution is to create an AWS CodeArtifact domain and repository. By adding an external connection for public:pypi to the CodeArtifact repository and configuring the Python client to use this repository, you enable the necessary access to the required Python packages. Additionally, creating a VPC endpoint for CodeArtifact ensures that the SageMaker instances can access CodeArtifact without needing direct internet access, thereby maintaining their isolation. This approach ensures security and efficiency.
Question 350 of 529
A solutions architect works for a government agency that has strict disaster recovery requirements. All Amazon Elastic Block Store (Amazon EBS) snapshots are required to be saved in at least two additional AWS Regions. The agency also is required to maintain the lowest possible operational overhead.
Which solution meets these requirements?
A.
Configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to copy the EBS snapshots to the additional Regions.
B.
Use Amazon EventBridge to schedule an AWS Lambda function to copy the EBS snapshots to the additional Regions.
C.
Setup AWS Backup to create the EBS snapshots. Configure Amazon S3 Cross-Region Replication to copy the EBS snapshots to the additional Regions.
D.
Schedule Amazon EC2 Image Builder to run once daily to create an AMI and copy the AMI to the additional Regions.
AnswerDiscussion
Correct Answer: A
Configuring a policy in Amazon Data Lifecycle Manager (Amazon DLM) is the most appropriate solution. DLM automates the process of copying EBS snapshots to additional regions, ensuring compliance with the disaster recovery requirements while minimizing operational overhead. It provides a straightforward, automated way to manage snapshots without requiring additional setup or management, aligning perfectly with the need for low operational overhead.
Question 351 of 529
A company has a project that is launching Amazon EC2 instances that are larger than required. The project's account cannot be part of the company's organization in AWS Organizations due to policy restrictions to keep this activity outside of corporate IT. The company wants to allow only the launch of t3.small EC2 instances by developers in the project's account. These EC2 instances must be restricted to the us-east-2 Region.
What should a solutions architect do to meet these requirements?
A.
Create a new developer account. Move all EC2 instances, users, and assets into us-east-2. Add the account to the company's organization in AWS Organizations. Enforce a tagging policy that denotes Region affinity.
B.
Create an SCP that denies the launch of all EC2 instances except t3.small EC2 instances in us-east-2. Attach the SCP to the project's account.
C.
Create and purchase a t3.small EC2 Reserved Instance for each developer in us-east-2. Assign each developer a specific EC2 instance with their name as the tag.
D.
Create an IAM policy than allows the launch of only t3.small EC2 instances in us-east-2. Attach the policy to the roles and groups that the developers use in the project's account.
AnswerDiscussion
Correct Answer: D
To meet the requirement of restricting developers to launching only t3.small EC2 instances in the us-east-2 region, you should create an IAM policy that enforces these constraints. Attach this policy to the roles and groups that the developers use in the project's account. Since the project's account cannot be part of the company's AWS Organizations due to policy restrictions, an SCP (Service Control Policy) cannot be used. Therefore, the appropriate solution is to use an IAM policy tailored to the specific needs of the project account.
Question 352 of 529
A scientific company needs to process text and image data from an Amazon S3 bucket. The data is collected from several radar stations during a live, time-critical phase of a deep space mission. The radar stations upload the data to the source S3 bucket. The data is prefixed by radar station identification number.
The company created a destination S3 bucket in a second account. Data must be copied from the source S3 bucket to the destination S3 bucket to meet a compliance objective. This replication occurs through the use of an S3 replication rule to cover all objects in the source S3 bucket.
One specific radar station is identified as having the most accurate data. Data replication at this radar station must be monitored for completion within 30 minutes after the radar station uploads the objects to the source S3 bucket.
What should a solutions architect do to meet these requirements?
A.
Setup an AWS DataSync agent to replicate the prefixed data from the source S3 bucket to the destination S3 bucket. Select to use all available bandwidth on the task, and monitor the task to ensure that itis in the TRANSFERRING status. Create an Amazon EventBridge rule to initiate an alert if this status changes.
B.
In the second account, create another S3 bucket to receive data from the radar station with the most accurate data. Set up a new replication rule for this new S3 bucket to separate the replication from the other radar stations. Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold.
C.
Enable Amazon S3 Transfer Acceleration on the source S3 bucket, and configure the radar station with the most accurate data to use the new endpoint. Monitor the S3 destination bucket's TotalRequestLatency metric. Create an Amazon EventBridge rule to initiate an alert if this status changes.
D.
Create a new S3 replication rule on the source S3 bucket that filters for the keys that use the prefix of the radar station with the most accurate data. Enable S3 Replication Time Control (S3 RTC). Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold.
AnswerDiscussion
Correct Answer: D
To meet the requirements, create a new S3 replication rule on the source S3 bucket that filters for the keys matching the radar station's prefix with the most accurate data. Enable S3 Replication Time Control (S3 RTC), which provides an SLA for 15-minute replication, ensuring the data is transferred within the precise time frame. Monitor the maximum replication time for compliance and create an Amazon EventBridge rule to alert when the time exceeds the desired threshold. This approach ensures timely and reliable data replication specifically for the most critical radar station data.
Question 353 of 529
A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MySQL, and Oracle databases. There are many dependent services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration.
Which tools or services should the solutions architect use to plan the cloud migration? (Choose three.)
A.
AWS Application Discovery Service
B.
AWS SMS
C.
AWS X-Ray
D.
AWS Cloud Adoption Readiness Tool (CART)
E.
Amazon Inspector
F.
AWS Migration Hub
AnswerDiscussion
Correct Answer: A, D, F
AWS Application Discovery Service, AWS Cloud Adoption Readiness Tool (CART), and AWS Migration Hub are suitable tools for planning a cloud migration. AWS Application Discovery Service helps in understanding the current environment by gathering information about on-premises servers, which is critical when technical documentation is incomplete or outdated. AWS Cloud Adoption Readiness Tool (CART) is useful for assessing the organization's readiness for cloud adoption and identifying any gaps, which is necessary in the planning phase. AWS Migration Hub provides a single location to track the progress and status of migrations across multiple AWS and partner solutions, helping to estimate cloud resource costs after migration.
Question 354 of 529
A solutions architect is reviewing an application's resilience before launch. The application runs on an Amazon EC2 instance that is deployed in a private subnet of a VPC. The EC2 instance is provisioned by an Auto Scaling group that has a minimum capacity of 1 and a maximum capacity of 1. The application stores data on an Amazon RDS for MySQL DB instance. The VPC has subnets configured in three Availability Zones and is configured with a single NAT gateway.
The solutions architect needs to recommend a solution to ensure that the application will operate across multiple Availability Zones.
Which solution will meet this requirement?
A.
Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to a Multi-AZ configuration. Configure the Auto Scaling group to launch the instances across Availability Zones. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.
B.
Replace the NAT gateway with a virtual private gateway. Replace the RDS for MySQL DB instance with an Amazon Aurora MySQL DB cluster. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.
C.
Replace the NAT gateway with a NAT instance. Migrate the RDS for MySQL DB instance to an RDS for PostgreSQL DB instance. Launch a new EC2 instance in the other Availability Zones.
D.
Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to turn on automatic backups and retain the backups for 7 days. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Keep the minimum capacity and the maximum capacity of the Auto Scaling group at 1.
AnswerDiscussion
Correct Answer: A
To ensure the application operates across multiple Availability Zones, the solution must address high availability and fault tolerance. Deploying additional NAT gateways in each Availability Zone avoids a single point of failure. Updating the route tables ensures correct routing through the new NAT gateways. Modifying the RDS for MySQL DB instance to a Multi-AZ configuration provides database redundancy and fault tolerance. Configuring the Auto Scaling group to launch instances across multiple Availability Zones and setting the appropriate minimum and maximum capacity ensures that the application can handle higher loads and remains available even if one Availability Zone fails. This approach provides comprehensive coverage for high availability, fault tolerance, and scalability.
Question 355 of 529
A company is planning to migrate its on-premises transaction-processing application to AWS. The application runs inside Docker containers that are hosted on VMs in the company's data center. The Docker containers have shared storage where the application records transaction data.
The transactions are time sensitive. The volume of transactions inside the application is unpredictable. The company must implement a low-latency storage solution that will automatically scale throughput to meet increased demand. The company cannot develop the application further and cannot continue to administer the Docker hosting environment.
How should the company migrate the application to AWS to meet these requirements?
A.
Migrate the containers that run the application to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon S3 to store the transaction data that the containers share.
B.
Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic File System (Amazon EFS) file system. Create a Fargate task definition. Add a volume to the task definition to point to the EFS file system.
C.
Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic Block Store (Amazon EBS) volume. Create a Fargate task definition. Attach the EBS volume to each running task.
D.
Launch Amazon EC2 instances. Install Docker on the EC2 instances. Migrate the containers to the EC2 instances. Create an Amazon Elastic File System (Amazon EFS) file system. Add a mount point to the EC2 instances for the EFS file system.
AnswerDiscussion
Correct Answer: B
The best way to meet the company's needs of low-latency storage, automatic scaling of throughput, and no further development or administrative overhead is to migrate the containers to AWS Fargate with Amazon Elastic Container Service (ECS). AWS Fargate eliminates the need to manage servers and enables the company to focus solely on deploying and managing containers. Amazon Elastic File System (EFS) automatically scales and provides a shared, low-latency storage solution suitable for the application’s transaction data. By creating a Fargate task definition and adding a volume that points to the Amazon EFS file system, the company can ensure that the storage will scale with the demand while providing the required low latency.
Question 356 of 529
A company is planning to migrate to the AWS Cloud. The company hosts many applications on Windows servers and Linux servers. Some of the servers are physical, and some of the servers are virtual. The company uses several types of databases in its on-premises environment. The company does not have an accurate inventory of its on-premises servers and applications.
The company wants to rightsize its resources during migration. A solutions architect needs to obtain information about the network connections and the application relationships. The solutions architect must assess the company’s current environment and develop a migration plan.
Which solution will provide the solutions architect with the required information to develop the migration plan?
A.
Use Migration Evaluator to request an evaluation of the environment from AWS. Use the AWS Application Discovery Service Agentless Collector to import the details into a Migration Evaluator Quick Insights report.
B.
Use AWS Migration Hub and install the AWS Application Discovery Agent on the servers. Deploy the Migration Hub Strategy Recommendations application data collector. Generate a report by using Migration Hub Strategy Recommendations.
C.
Use AWS Migration Hub and run the AWS Application Discovery Service Agentless Collector on the servers. Group the servers and databases by using AWS Application Migration Service. Generate a report by using Migration Hub Strategy Recommendations.
D.
Use the AWS Migration Hub import tool to load the details of the company’s on-premises environment. Generate a report by using Migration Hub Strategy Recommendations.
AnswerDiscussion
Correct Answer: B
To develop an accurate migration plan, the company first needs to obtain detailed information about its on-premises environment, including network connections and application relationships. Given that the company's inventory is incomplete, it's essential to collect data directly from the servers. Using AWS Migration Hub and installing the AWS Application Discovery Agent on the servers will capture comprehensive details about system configurations, performances, running processes, and network connections. Deploying the Migration Hub Strategy Recommendations application data collector will then help in generating a report with the necessary insights. Therefore, using AWS Migration Hub along with the AWS Application Discovery Agent and data collector is the most appropriate solution for this scenario.
Question 357 of 529
A financial services company sells its software-as-a-service (SaaS) platform for application compliance to large global banks. The SaaS platform runs on AWS and uses multiple AWS accounts that are managed in an organization in AWS Organizations. The SaaS platform uses many AWS resources globally.
For regulatory compliance, all API calls to AWS resources must be audited, tracked for changes, and stored in a durable and secure data store.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create a new AWS CloudTrail trail. Use an existing Amazon S3 bucket in the organization's management account to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 bucket.
B.
Create a new AWS CloudTrail trail in each member account of the organization. Create new Amazon S3 buckets to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 buckets.
C.
Create a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket with versioning turned on to store the logs. Deploy the trail for all accounts in the organization. Enable MFA delete and encryption on the S3 bucket.
D.
Create a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket to store the logs. Configure Amazon Simple Notification Service (Amazon SNS) to send log-file delivery notifications to an external management system that will track the logs. Enable MFA delete and encryption on the S3 bucket.
AnswerDiscussion
Correct Answer: C
To meet the requirements of auditing, tracking changes, and storing API call logs in a durable and secure data store with the least operational overhead, creating a new AWS CloudTrail trail in the organization's management account and using a centralized Amazon S3 bucket is the most efficient solution. This allows for centralized management and auditing of logs across all accounts in the organization with minimal management overhead. Enabling MFA delete and encryption ensures the security and durability of the logs. By deploying the trail for all accounts in the organization, compliance can be achieved globally without needing separate configurations for each account.
Question 358 of 529
A company is deploying a distributed in-memory database on a fleet of Amazon EC2 instances. The fleet consists of a primary node and eight worker nodes. The primary node is responsible for monitoring cluster health, accepting user requests, distributing user requests to worker nodes, and sending an aggregate response back to a client. Worker nodes communicate with each other to replicate data partitions.
The company requires the lowest possible networking latency to achieve maximum performance.
Which solution will meet these requirements?
A.
Launch memory optimized EC2 instances in a partition placement group.
B.
Launch compute optimized EC2 instances in a partition placement group.
C.
Launch memory optimized EC2 instances in a cluster placement group.
D.
Launch compute optimized EC2 instances in a spread placement group.
AnswerDiscussion
Correct Answer: C
For a distributed in-memory database on Amazon EC2 instances requiring the lowest possible networking latency, launching memory optimized EC2 instances in a cluster placement group is the best solution. Memory optimized instances are ideal for in-memory databases because they provide high memory capacity and bandwidth. A cluster placement group is specifically designed to ensure low-latency networking by placing instances physically close to each other within a single Availability Zone, which significantly reduces the communication time between nodes.
Question 359 of 529
A company maintains information on premises in approximately 1 million.csv files that are hosted on a VM. The data initially is 10 TB in size and grows at a rate of 1 TB each week. The company needs to automate backups of the data to the AWS Cloud.
Backups of the data must occur daily. The company needs a solution that applies custom filters to back up only a subset of the data that is located in designated source directories. The company has set up an AWS Direct Connect connection.
Which solution will meet the backup requirements with the LEAST operational overhead?
A.
Use the Amazon S3 CopyObject API operation with multipart upload to copy the existing data to Amazon S3. Use the CopyObject API operation to replicate new data to Amazon S3 daily.
B.
Create a backup plan in AWS Backup to back up the data to Amazon S3. Schedule the backup plan to run daily.
C.
Install the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily.
D.
Use an AWS Snowball Edge device for the initial backup. Use AWS DataSync for incremental backups to Amazon S3 daily.
AnswerDiscussion
Correct Answer: C
To automate backups of data from a VM on-premises to AWS with minimal operational overhead, installing the AWS DataSync agent as a VM on the on-premises hypervisor is optimal. AWS DataSync allows you to configure tasks to transfer and filter only the necessary data to Amazon S3 on a daily basis, meeting the requirement for custom filtering of designated source directories. The solution effectively handles the growing data size and involves minimal operational management once set up.
Question 360 of 529
A financial services company has an asset management product that thousands of customers use around the world. The customers provide feedback about the product through surveys. The company is building a new analytical solution that runs on Amazon EMR to analyze the data from these surveys. The following user personas need to access the analytical solution to perform different actions:
• Administrator: Provisions the EMR cluster for the analytics team based on the team’s requirements
• Data engineer: Runs ETL scripts to process, transform, and enrich the datasets
• Data analyst: Runs SQL and Hive queries on the data
A solutions architect must ensure that all the user personas have least privilege access to only the resources that they need. The user personas must be able to launch only applications that are approved and authorized. The solution also must ensure tagging for all resources that the user personas create.
Which solution will meet these requirements?
A.
Create IAM roles for each user persona. Attach identity-based policies to define which actions the user who assumes the role can perform. Create an AWS Config rule to check for noncompliant resources. Configure the rule to notify the administrator to remediate the noncompliant resources.
B.
Setup Kerberos-based authentication for EMR clusters upon launch. Specify a Kerberos security configuration along with cluster-specific Kerberos options.
C.
Use AWS Service Catalog to control the Amazon EMR versions available for deployment, the cluster configuration, and the permissions for each user persona.
D.
Launch the EMR cluster by using AWS CloudFormation, Attach resource-based policies to the EMR cluster during cluster creation. Create an AWS. Config rule to check for noncompliant clusters and noncompliant Amazon S3 buckets. Configure the rule to notify the administrator to remediate the noncompliant resources.
AnswerDiscussion
Correct Answer: C
Using AWS Service Catalog is the most appropriate solution to meet the requirements. AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS, which ensures that only approved applications and configurations are deployed. It supports defining permissions for each user persona, ensuring they have least privilege access. It also enforces resource tagging, which is essential for managing and tracking resources effectively.
Question 361 of 529
A software as a service (SaaS) company uses AWS to host a service that is powered by AWS PrivateLink. The service consists of proprietary software that runs on three Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in private subnets in multiple Availability Zones in the eu-west-2 Region. All the company's customers are in eu-west-2.
However, the company now acquires a new customer in the us-east-1 Region. The company creates a new VPC and new subnets in us-east-1. The company establishes inter-Region VPC peering between the VPCs in the two Regions.
The company wants to give the new customer access to the SaaS service, but the company does not want to immediately deploy new EC2 resources in us-east-1.
Which solution will meet these requirements?
A.
Configure a PrivateLink endpoint service in us-east-1 to use the existing NLB that is in eu-west-2. Grant specific AWS accounts access to connect to the SaaS service.
B.
Create an NLB in us-east-1. Create an IP target group that uses the IP addresses of the company's instances in eu-west-2 that host the SaaS service. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.
C.
Create an Application Load Balancer (ALB) in front of the EC2 instances in eu-west-2. Create an NLB in us-east-1. Associate the NLB that is in us-east-1 with an ALB target group that uses the ALB that is in eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.
D.
Use AWS Resource Access Manager (AWS RAM) to share the EC2 instances that are in eu-west-2. In us-east-1, create an NLB and an instance target group that includes the shared EC2 instances from eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.
AnswerDiscussion
Correct Answer: A
To meet the requirements of giving the new customer access to the SaaS service without deploying new EC2 resources in us-east-1, the best solution is to configure a PrivateLink endpoint service in us-east-1 that utilizes the existing Network Load Balancer (NLB) in eu-west-2. This setup allows the new customer in us-east-1 to access the SaaS service hosted in eu-west-2 without requiring additional EC2 instances in the us-east-1 region. Additionally, granting specific AWS accounts access to connect to the SaaS service ensures that only authorized users can access it, which meets the security requirements.
Question 362 of 529
A company needs to monitor a growing number of Amazon S3 buckets across two AWS Regions. The company also needs to track the percentage of objects that are encrypted in Amazon S3. The company needs a dashboard to display this information for internal compliance teams.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create a new 3 Storage Lens dashboard in each Region to track bucket and encryption metrics. Aggregate data from both Region dashboards into a single dashboard in Amazon QuickSight for the compliance teams.
B.
Deploy an AWS Lambda function in each Region to list the number of buckets and the encryption status of objects. Store this data in Amazon S3. Use Amazon Athena queries to display the data on a custom dashboard in Amazon QuickSight for the compliance teams.
C.
Use the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give the compliance teams access to the dashboard directly in the S3 console.
D.
Create an Amazon EventBridge rule to detect AWS CloudTrail events for S3 object creation. Configure the rule to invoke an AWS Lambda function to record encryption metrics in Amazon DynamoDB. Use Amazon QuickSight to display the metrics in a dashboard for the compliance teams.
AnswerDiscussion
Correct Answer: C
To meet the requirement of monitoring a growing number of Amazon S3 buckets across two AWS Regions and tracking the percentage of encrypted objects with the least operational overhead, using the S3 Storage Lens default dashboard is the best solution. S3 Storage Lens provides built-in capabilities for tracking various storage metrics, including encryption status, without the need for additional development or infrastructure management. This solution allows compliance teams to directly access the dashboard in the S3 console, ensuring minimal setup and ongoing maintenance. Other options, like deploying AWS Lambda functions or using EventBridge and DynamoDB, would increase operational complexity and overhead.
Question 363 of 529
A company’s CISO has asked a solutions architect to re-engineer the company's current CI/CD practices to make sure patch deployments to its application can happen as quickly as possible with minimal downtime if vulnerabilities are discovered. The company must also be able to quickly roll back a change in case of errors.
The web application is deployed in a fleet of Amazon EC2 instances behind an Application Load Balancer. The company is currently using GitHub to host the application source code, and has configured an AWS CodeBuild project to build the application. The company also intends to use AWS CodePipeline to trigger builds from GitHub commits using the existing CodeBuild project.
What CI/CD configuration meets all of the requirements?
A.
Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for in-place deployment. Monitor the newly deployed code, and, if there are any issues, push another code update
B.
Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy.
C.
Configure CodePipeline with a deploy stage using AWS CloudFormation to create a pipeline for test and production stacks. Monitor the newly deployed code, and, if there are any issues, push another code update.
D.
Configure the CodePipeline with a deploy stage using AWS OpsWorks and in-place deployments. Monitor the newly deployed code, and, if there are any issues, push another code update.
AnswerDiscussion
Correct Answer: B
To meet the requirement of deploying patches as quickly as possible with minimal downtime and ensuring a quick rollback in case of errors, AWS CodePipeline configured with CodeDeploy for blue/green deployments is the best choice. The blue/green deployment strategy allows for setting up two separate environments: one with the current version (blue) and one with the new version (green). By switching traffic between these environments, downtime is minimized. Additionally, in case of errors with the new deployment, traffic can be quickly routed back to the stable environment, ensuring a fast and reliable rollback. This meets the criteria of quick deployment, minimal downtime, and efficient rollback.
Question 364 of 529
A company is managing many AWS accounts by using an organization in AWS Organizations. Different business units in the company run applications on Amazon EC2 instances. All the EC2 instances must have a BusinessUnit tag so that the company can track the cost for each business unit.
A recent audit revealed that some instances were missing this tag. The company manually added the missing tag to the instances.
What should a solutions architect do to enforce the tagging requirement in the future?
A.
Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned off. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the root of the organization.
B.
Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned on. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the organization's management account.
C.
Create an SCP and attach the SCP to the root of the organization. Include the following statement in the SCP:
D.
Create an SCP and attach the SCP to the organization’s management account. Include the following statement in the SCP:
AnswerDiscussion
Correct Answer: C
To enforce that all EC2 instances are tagged with the BusinessUnit tag, you should create a Service Control Policy (SCP) and attach it to the root of the organization. This policy should include a rule that denies the creation of EC2 instances if the BusinessUnit tag is missing. Tag policies alone will not prevent the creation of resources without tags; instead, they help ensure that tags have the correct key values. SCPs, on the other hand, can enforce the tagging requirement by preventing the instantiation of resources that do not meet the tagging criteria. Hence, the correct approach involves using an SCP with the appropriate condition to deny EC2 instance creation when the required tag is not present.
Question 365 of 529
A company is running a workload that consists of thousands of Amazon EC2 instances. The workload is running in a VPC that contains several public subnets and private subnets. The public subnets have a route for 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existing NAT gateway.
A solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. The EC2 instances that are in private subnets must not be accessible from the public internet.
What should the solutions architect do to meet these requirements?
A.
Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Update all the VPC route tables, and add a route for ::/0 to the internet gateway.
B.
Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway.
C.
Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, and add a route for ::/0 to the egress-only internet gateway.
D.
Update the existing VPC, and associate a custom IPV6 CIDR block with the VPC and all subnets. Create a new NAT gateway, and enable IPV6 support. Update the VPC route tables for all private subnets, and add a route for ::/0 to the IPv6-enabled NAT gateway.
AnswerDiscussion
Correct Answer: C
The correct approach is to update the existing VPC and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. For the private subnets, creating an egress-only internet gateway and updating the VPC route tables to add a route for ::/0 to the egress-only internet gateway is necessary. This ensures that instances in private subnets can reach IPv6 addresses on the internet for outbound communication without being accessible from the public internet, aligning with the requirement to maintain privacy.
Question 366 of 529
A company is using Amazon API Gateway to deploy a private REST API that will provide access to sensitive data. The API must be accessible only from an application that is deployed in a VPC. The company deploys the API successfully. However, the API is not accessible from an Amazon EC2 instance that is deployed in the VPC.
Which solution will provide connectivity between the EC2 instance and the API?
A.
Create an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows apigateway:* actions. Disable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC. Use the VPC endpoint's DNS name to access the API.
B.
Create an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows the execute-api:Invoke action. Enable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC endpoint. Use the API endpoint’s DNS names to access the API.
C.
Create a Network Load Balancer (NLB) and a VPC link. Configure private integration between API Gateway and the NLB. Use the API endpoint’s DNS names to access the API.
D.
Create an Application Load Balancer (ALB) and a VPC Link. Configure private integration between API Gateway and the ALB. Use the ALB endpoint’s DNS name to access the API.
AnswerDiscussion
Correct Answer: B
To provide connectivity between the EC2 instance and the private REST API deployed in Amazon API Gateway, the correct approach involves using an interface VPC endpoint. By attaching an endpoint policy that allows the execute-api:Invoke action and enabling private DNS naming, the EC2 instance can resolve and connect to the API using predefined DNS names. Additionally, configuring an API resource policy that permits access from the VPC endpoint ensures that the API is only accessible within the desired VPC, thus maintaining security. Therefore, creating an interface VPC endpoint with the specified configuration is the appropriate solution.
Question 367 of 529
A large payroll company recently merged with a small staffing company. The unified company now has multiple business units, each with its own existing AWS account.
A solutions architect must ensure that the company can centrally manage the billing and access policies for all the AWS accounts. The solutions architect configures AWS Organizations by sending an invitation to all member accounts of the company from a centralized management account.
What should the solutions architect do next to meet these requirements?
A.
Create the OrganizationAccountAccess IAM group in each member account. Include the necessary IAM roles for each administrator.
B.
Create the OrganizationAccountAccessPolicy IAM policy in each member account. Connect the member accounts to the management account by using cross-account access.
C.
Create the OrganizationAccountAccessRole IAM role in each member account. Grant permission to the management account to assume the IAM role.
D.
Create the OrganizationAccountAccessRole IAM role in the management account. Attach the AdministratorAccess AWS managed policy to the IAM role. Assign the IAM role to the administrators in each member account.
AnswerDiscussion
Correct Answer: C
To centrally manage the billing and access policies for all AWS accounts using AWS Organizations, the solutions architect should create the OrganizationAccountAccessRole IAM role in each member account. This IAM role should grant permission to the management account to assume the role. This setup enables administrators in the management account to assume the IAM role in member accounts, allowing for centralized management and access control across all accounts.
Question 368 of 529
A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released.
What changes to the current architecture will reduce operational overhead and support the product release?
A.
Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
B.
Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
C.
Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
D.
Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
AnswerDiscussion
Correct Answer: D
To reduce operational overhead and improve efficiency, the application services should be deployed on Amazon Elastic Kubernetes Service (EKS) with AWS Fargate, which provides a serverless-like experience for containers and reduces the need to manage EC2 instances directly. Enabling auto scaling with Fargate will handle unexpected spikes in traffic. Deploying the database in Multi-AZ mode with read replicas ensures high availability and fault tolerance, meeting the expected increase in orders. Using Amazon Managed Streaming for Apache Kafka ensures managed, scalable Kafka services, reducing the operational burden. Storing static content in Amazon S3 behind a CloudFront distribution ensures high availability and faster content delivery, further supporting the product release.
Question 369 of 529
A company hosts a VPN in an on-premises data center. Employees currently connect to the VPN to access files in their Windows home directories. Recently, there has been a large growth in the number of employees who work remotely. As a result, bandwidth usage for connections into the data center has begun to reach 100% during business hours.
The company must design a solution on AWS that will support the growth of the company's remote workforce, reduce the bandwidth usage for connections into the data center, and reduce operational overhead.
Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)
A.
Create an AWS Storage Gateway Volume Gateway. Mount a volume from the Volume Gateway to the on-premises file server.
B.
Migrate the home directories to Amazon FSx for Windows File Server.
C.
Migrate the home directories to Amazon FSx for Lustre.
D.
Migrate remote users to AWS Client VPN.
E.
Create an AWS Direct Connect connection from the on-premises data center to AWS.
AnswerDiscussion
Correct Answer: B, D
To design a solution on AWS that supports the growth of the company's remote workforce, reduces the bandwidth usage for connections into the data center, and reduces operational overhead, migrating the home directories to Amazon FSx for Windows File Server and migrating remote users to AWS Client VPN are the most suitable steps. Amazon FSx for Windows File Server provides a fully managed native Windows file system, which simplifies the process of file storage and reduces operational overhead. AWS Client VPN allows remote users to securely connect to AWS resources, including Amazon FSx for Windows File Server, without relying on the on-premises VPN infrastructure, thereby reducing bandwidth usage into the data center.
Question 370 of 529
A company has multiple AWS accounts. The company recently had a security audit that revealed many unencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.
A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will be detected automatically in the future. Additionally, the company wants a solution that can centrally manage multiple AWS accounts with a focus on compliance and security.
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)
A.
Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.
B.
Use the AWS CLI to list all the unencrypted volumes in all the AWS accounts. Run a script to encrypt all the unencrypted volumes in place.
C.
Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume.
D.
Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the mandatory controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.
E.
Turn on AWS CloudTrail. Configure an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes.
AnswerDiscussion
Correct Answer: A, C
To meet the requirements of encrypting unencrypted EBS volumes and ensuring unencrypted volumes will be detected automatically in the future, two steps are necessary. First, setting up AWS Control Tower and turning on the strongly recommended controls (guardrails) within an organization in AWS Organizations will provide centralized management across multiple AWS accounts with a focus on compliance and security. This setup will enable continuous monitoring to detect any unencrypted EBS volumes. Second, since EBS volumes cannot be encrypted in place, it is necessary to first create a snapshot of each unencrypted volume, create a new encrypted volume from the unencrypted snapshot, and then detach the existing unencrypted volume and replace it with the encrypted volume. This ensures all volumes are encrypted as required.
Question 371 of 529
A company hosts an intranet web application on Amazon EC2 instances behind an Application Load Balancer (ALB). Currently, users authenticate to the application against an internal user database.
The company needs to authenticate users to the application by using an existing AWS Directory Service for Microsoft Active Directory directory. All users with accounts in the directory must have access to the application.
Which solution will meet these requirements?
A.
Create a new app client in the directory. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule. Configure the listener rule with the appropriate issuer, client ID and secret, and endpoint details for the Active Directory service. Configure the new app client with the callback URL that the ALB provides.
B.
Configure an Amazon Cognito user pool. Configure the user pool with a federated identity provider (ldP) that has metadata from the directory. Create an app client. Associate the app client with the user pool. Create a listener rule for the ALSpecify the authenticate-cognito action for the listener rule. Configure the listener rule to use the user pool and app client.
C.
Add the directory as a new IAM identity provider (ldP). Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Configure the new role as the default authenticated user role for the ldP. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule.
D.
Enable AWS IAM Identity Center (AWS Single Sign-On). Configure the directory as an external identity provider (ldP) that uses SAML. Use the automatic provisioning method. Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Attach the new role to all groups. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule.
AnswerDiscussion
Correct Answer: B
The company needs to authenticate users to an internal web application using AWS Directory Service for Microsoft Active Directory. One of the most appropriate solutions is to utilize Amazon Cognito, which can federate identities from the directory service. Configuring an Amazon Cognito user pool with a federated identity provider using metadata from the directory allows seamless integration with the existing Active Directory. Creating an app client associated with this user pool and a listener rule for the ALB specifying the authenticate-cognito action will ensure users are authenticated correctly. This setup meets the requirement of leveraging the existing AWS Directory Service for user authentication.
Question 372 of 529
A company has a website that serves many visitors. The company deploys a backend service for the website in a primary AWS Region and a disaster recovery (DR) Region.
A single Amazon CloudFront distribution is deployed for the website. The company creates an Amazon Route 53 record set with health checks and a failover routing policy for the primary Region’s backend service. The company configures the Route 53 record set as an origin for the CloudFront distribution. The company configures another record set that points to the backend service's endpoint in the DR Region as a secondary failover record type. The TTL for both record sets is 60 seconds.
Currently, failover takes more than 1 minute. A solutions architect must design a solution that will provide the fastest failover time.
Which solution will achieve this goal?
A.
Deploy an additional CloudFront distribution. Create a new Route 53 failover record set with health checks for both CloudFront distributions.
B.
Set the TTL to 4 second for the existing Route 53 record sets that are used for the backend service in each Region.
C.
Create new record sets for the backend services by using a latency routing policy. Use the record sets as an origin in the CloudFront distribution.
D.
Create a CloudFront origin group that includes two origins, one for each backend service Region. Configure origin failover as a cache behavior for the CloudFront distribution.
AnswerDiscussion
Correct Answer: D
To achieve the fastest failover time, creating a CloudFront origin group that includes two origins, one for each backend service region, and configuring origin failover as a cache behavior for the CloudFront distribution is the best approach. This method allows CloudFront to immediately switch to the backup origin if it detects a failure, providing faster failover compared to DNS-based failover which relies on TTL settings. This setup minimizes downtime and ensures that requests are quickly rerouted to the operational backend service.
Question 373 of 529
A company is using multiple AWS accounts and has multiple DevOps teams running production and non-production workloads in these accounts. The company would like to centrally-restrict access to some of the AWS services that the DevOps teams do not use. The company decided to use AWS Organizations and successfully invited all AWS accounts into the Organization. They would like to allow access to services that are currently in-use and deny a few specific services. Also they would like to administer multiple accounts together as a single unit.
What combination of steps should the solutions architect take to satisfy these requirements? (Choose three.)
A.
Use a Deny list strategy.
B.
Review the Access Advisor in AWS IAM to determine services recently used
C.
Review the AWS Trusted Advisor report to determine services recently used.
D.
Remove the default FullAWSAccess SCP.
E.
Define organizational units (OUs) and place the member accounts in the OUs.
F.
Remove the default DenyAWSAccess SCP.
AnswerDiscussion
Correct Answer: A, B, E
To centrally restrict access to AWS services that the DevOps teams do not use, the solutions architect should implement the following steps. Use a Deny list strategy to explicitly deny access to the specific AWS services that are not needed. This way, all other services remain accessible, aligning with the company's requirement. Review the Access Advisor in AWS IAM to determine which services have been recently used. This helps in identifying the services that are actively used and distinguishing them from those that can be restricted. Additionally, defining organizational units (OUs) and placing the member accounts in these OUs will help administer multiple accounts together as a single unit. This organizational structure simplifies the management of policies across multiple accounts.
Question 374 of 529
A live-events company is designing a scaling solution for its ticket application on AWS. The application has high peaks of utilization during sale events. Each sale event is a one-time event that is scheduled. The application runs on Amazon EC2 instances that are in an Auto Scaling group. The application uses PostgreSQL for the database layer.
The company needs a scaling solution to maximize availability during the sale events.
Which solution will meet these requirements?
A.
Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Serverless v2 Multi-AZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.
B.
Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL Mulli-AZ DB instance with automatically scaling read replicas. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger read replica before a sale event. Fail over to the larger read replica. Create another EventBridge rule that invokes another Lambda function to scale down the read replica after the sale event.
C.
Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL MultiAZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.
D.
Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger Aurora Replica. Create another EventBridge rule that invokes another Lambda function to scale down the Aurora Replica after the sale event.
AnswerDiscussion
Correct Answer: D
Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger Aurora Replica. Create another EventBridge rule that invokes another Lambda function to scale down the Aurora Replica after the sale event. This solution leverages the high performance and reliability of Amazon Aurora, uses scheduled scaling to handle predictable high-traffic peaks, and ensures efficient resource utilization by scaling up and down as needed.
Question 375 of 529
A company runs an intranet application on premises. The company wants to configure a cloud backup of the application. The company has selected AWS Elastic Disaster Recovery for this solution.
The company requires that replication traffic does not travel through the public internet. The application also must not be accessible from the internet. The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.
Which combination of steps will meet these requirements? (Choose three.)
A.
Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway.
B.
Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway.
C.
Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network.
D.
Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.
E.
During configuration of the replication servers, select the option to use private IP addresses for data replication.
F.
During configuration of the launch settings for the target servers, select the option to ensure that the Recovery instance’s private IP address matches the source server's private IP address.
AnswerDiscussion
Correct Answer: A, D, E
To meet the company's requirements, which include ensuring that replication traffic does not travel through the public internet and avoiding the consumption of all available network bandwidth, the following steps are recommended. First, create a VPC with at least two private subnets and a virtual private gateway to ensure the application remains inaccessible from the internet. AWS Direct Connect should be used to establish a dedicated, private connection between the on-premises network and the AWS network, thereby avoiding potential bandwidth competition with other applications. During the configuration of the replication servers, selecting the option to use private IP addresses for data replication will ensure that the replication traffic remains within the private network.
Question 376 of 529
A company that provides image storage services wants to deploy a customer-facing solution to AWS. Millions of individual customers will use the solution. The solution will receive batches of large image files, resize the files, and store the files in an Amazon S3 bucket for up to 6 months.
The solution must handle significant variance in demand. The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.
Which solution will meet these requirements MOST cost-effectively?
A.
Use AWS Step Functions to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months.
B.
Use Amazon EventBridge to process the S3 event that occurs when a user uploads an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months.
C.
Use S3 Event Notifications to invoke an AWS Lambda function when a user stores an image. Use the Lambda function to resize the image in place and to store the original file in the S3 bucket. Create an S3 Lifecycle policy to move all stored images to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months.
D.
Use Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image and stores the resized file in an S3 bucket that uses S3 Standard-Infrequent Access (S3 Standard-IA). Create an S3 Lifecycle policy to move all stored images to S3 Glacier Deep Archive after 6 months.
AnswerDiscussion
Correct Answer: C
To meet the requirements of handling significant variance in demand, reliability at enterprise scale, and the ability to rerun processing jobs in case of failure, using S3 Event Notifications to trigger an AWS Lambda function is a fitting approach. This ensures scalability and reliability. The Lambda function can efficiently handle image resizing in place. Additionally, creating an S3 Lifecycle policy to move files to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months aligns with the cost-effective storage solution. The redundant approach of archiving or adding steps for merely 6-month storage in Option D is unnecessary, making Option C the most cost-effective and practical choice.
Question 377 of 529
A company has an organization in AWS Organizations that includes a separate AWS account for each of the company’s departments. Application teams from different departments develop and deploy solutions independently.
The company wants to reduce compute costs and manage costs appropriately across departments. The company also wants to improve visibility into billing for individual departments. The company does not want to lose operational flexibility when the company selects compute resources.
Which solution will meet these requirements?
A.
Use AWS Budgets for each department. Use Tag Editor to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans.
B.
Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use SCPs to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans.
C.
Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use Tag Editor to apply tags to appropriate resources. Purchase Compute Savings Plans.
D.
Use AWS Budgets for each department. Use SCPs to apply tags to appropriate resources. Purchase Compute Savings Plans.
AnswerDiscussion
Correct Answer: C
To meet the company's requirements of reducing compute costs, managing costs across departments, and improving visibility into billing without losing operational flexibility, the best approach is to configure AWS Organizations to use consolidated billing. Consolidated billing will provide a centralized view of the costs incurred by each department. Implementing a tagging strategy and using Tag Editor for applying these tags ensures that each department's resources and costs can be accurately identified and tracked. Additionally, purchasing Compute Savings Plans is advantageous because these plans provide cost savings for a variety of compute resources, including EC2 instances, AWS Fargate, and AWS Lambda, thus maintaining operational flexibility.
Question 378 of 529
A company has a web application that securely uploads pictures and videos to an Amazon S3 bucket. The company requires that only authenticated users are allowed to post content. The application generates a presigned URL that is used to upload objects through a browser interface. Most users are reporting slow upload times for objects larger than 100 MB.
What can a solutions architect do to improve the performance of these uploads while ensuring only authenticated users are allowed to post content?
A.
Set up an Amazon API Gateway with an edge-optimized API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using a COGNITO_USER_POOLS authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects.
B.
Set up an Amazon API Gateway with a regional API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using an AWS Lambda authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects.
C.
Enable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API.
D.
Configure an Amazon CloudFront distribution for the destination S3 bucket. Enable PUT and POST methods for the CloudFront cache behavior. Update the CloudFront origin to use an origin access identity (OAI). Give the OAI user 3: PutObject permissions in the bucket policy. Have the browser interface upload objects using the CloudFront distribution.
AnswerDiscussion
Correct Answer: C
Using S3 Transfer Acceleration and multipart upload directly addresses the issue of slow uploads for large objects. Enabling S3 Transfer Acceleration allows the use of edge locations to accelerate data transfer to the S3 bucket. Additionally, the multipart upload API can optimize the upload speed by breaking down the file into smaller parts. Presigned URLs maintain the requirement that only authenticated users can upload content, as they are generated with authentication credentials and have temporary permissions.
Question 379 of 529
A large company is migrating its entire IT portfolio to AWS. Each business unit in the company has a standalone AWS account that supports both development and test environments. New accounts to support production workloads will be needed soon.
The finance department requires a centralized method for payment but must maintain visibility into each group's spending to allocate costs.
The security team requires a centralized mechanism to control IAM usage in all the company’s accounts.
What combination of the following options meets the company’s needs with the LEAST effort? (Choose two.)
A.
Use a collection of parameterized AWS CloudFormation templates defining common IAM permissions that are launched into each account. Require all new and existing accounts to launch the appropriate stacks to enforce the least privilege model.
B.
Use AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Invite the existing accounts to join the organization and create new accounts using Organizations.
C.
Require each business unit to use its own AWS accounts. Tag each AWS account appropriately and enable Cost Explorer to administer chargebacks.
D.
Enable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts.
E.
Consolidate all of the company's AWS accounts into a single AWS account. Use tags for billing purposes and the IAM’s Access Advisor feature to enforce the least privilege model.
AnswerDiscussion
Correct Answer: B, D
Using AWS Organizations to create a new organization from a chosen payer account and defining an organizational unit hierarchy addresses the finance department's requirement for a centralized method of payment while maintaining visibility into each group's spending. Additionally, enabling all features of AWS Organizations and establishing appropriate service control policies filters IAM permissions effectively for the security team's centralized control of IAM usage across all company accounts. This combination meets the needs with the least effort and ensures proper cost allocation, management, and security policies.
Question 380 of 529
A company has a solution that analyzes weather data from thousands of weather stations. The weather stations send the data over an Amazon API Gateway REST API that has an AWS Lambda function integration. The Lambda function calls a third-party service for data pre-processing. The third-party service gets overloaded and fails the pre-processing, causing a loss of data.
A solutions architect must improve the resiliency of the solution. The solutions architect must ensure that no data is lost and that data can be processed later if failures occur.
What should the solutions architect do to meet these requirements?
A.
Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the queue as the dead-letter queue for the API.
B.
Create two Amazon Simple Queue Service (Amazon SQS) queues: a primary queue and a secondary queue. Configure the secondary queue as the dead-letter queue for the primary queue. Update the API to use a new integration to the primary queue. Configure the Lambda function as the invocation target for the primary queue.
C.
Create two Amazon EventBridge event buses: a primary event bus and a secondary event bus. Update the API to use a new integration to the primary event bus. Configure an EventBridge rule to react to all events on the primary event bus. Specify the Lambda function as the target of the rule. Configure the secondary event bus as the failure destination for the Lambda function.
D.
Create a custom Amazon EventBridge event bus. Configure the event bus as the failure destination for the Lambda function.
AnswerDiscussion
Correct Answer: B
To ensure no data is lost and that data can be processed later if failures occur, creating two Amazon Simple Queue Service (Amazon SQS) queues is the most reliable solution. The primary queue will serve as the initial collection point for data from the API Gateway, and the secondary queue will act as a dead-letter queue (DLQ). In this setup, if the third-party service fails during pre-processing, the unprocessed data will be moved to the secondary queue. This allows for later reprocessing of the data once the issues are resolved, effectively improving the solution's resiliency.
Question 381 of 529
A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database.
Last month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis.
Which combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)
A.
Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs.
B.
Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java.
C.
Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis.
D.
Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs.
E.
Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora
F.
Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS X-Ray.
AnswerDiscussion
Correct Answer: A, B, D
To improve application performance visibility during peak traffic events, configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs. This will help in analyzing and diagnosing database performance issues. Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and tracing of SQL queries to gain insights into how these queries impact performance. Additionally, install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs, providing a centralized location for log analysis.
Question 382 of 529
A company that provisions job boards for a seasonal workforce is seeing an increase in traffic and usage. The backend services run on a pair of Amazon EC2 instances behind an Application Load Balancer with Amazon DynamoDB as the datastore. Application read and write traffic is slow during peak seasons.
Which option provides a scalable application architecture to handle peak seasons with the LEAST development effort?
A.
Migrate the backend services to AWS Lambda. Increase the read and write capacity of DynamoDB.
B.
Migrate the backend services to AWS Lambda. Configure DynamoDB to use global tables.
C.
Use Auto Scaling groups for the backend services. Use DynamoDB auto scaling.
D.
Use Auto Scaling groups for the backend services. Use Amazon Simple Queue Service (Amazon SQS) and an AWS Lambda function to write to DynamoDB.
AnswerDiscussion
Correct Answer: C
Using Auto Scaling groups for the backend services and DynamoDB auto scaling provides a scalable application architecture to handle peak seasons with the least development effort. Auto Scaling ensures that the EC2 instances automatically scale in or out based on traffic demands, without requiring significant architectural changes or rewriting the application for AWS Lambda. DynamoDB auto scaling dynamically adjusts the read and write throughput capacity of the tables, ensuring that the database can handle traffic variations efficiently.
Question 383 of 529
A company is migrating to the cloud. It wants to evaluate the configurations of virtual machines in its existing data center environment to ensure that it can size new Amazon EC2 instances accurately. The company wants to collect metrics, such as CPU, memory, and disk utilization, and it needs an inventory of what processes are running on each instance. The company would also like to monitor network connections to map communications between servers.
Which would enable the collection of this data MOST cost effectively?
A.
Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center.
B.
Configure the Amazon CloudWatch agent on all servers within the local environment and publish metrics to Amazon CloudWatch Logs.
C.
Use AWS Application Discovery Service and enable agentless discovery in the existing virtualization environment.
D.
Enable AWS Application Discovery Service in the AWS Management Console and configure the corporate firewall to allow scans over a VPN.
AnswerDiscussion
Correct Answer: A
To collect detailed metrics such as CPU, memory, and disk utilization, as well as to get an inventory of running processes and monitor network connections, deploying the data collection agent is necessary. AWS Application Discovery Service with the data collection agent can accomplish this efficiently. The agent provides visibility into all these metrics and running processes, offering a comprehensive view of the existing environment for accurate sizing of Amazon EC2 instances.
Question 384 of 529
A company provides a software as a service (SaaS) application that runs in the AWS Cloud. The application runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in an Auto Scaling group and are distributed across three Availability Zones in a single AWS Region.
The company is deploying the application into additional Regions. The company must provide static IP addresses for the application to customers so that the customers can add the IP addresses to allow lists. The solution must automatically route customers to the Region that is geographically closest to them.
Which solution will meet these requirements?
A.
Create an Amazon CloudFront distribution. Create a CloudFront origin group. Add the NLB for each additional Region to the origin group. Provide customers with the IP address ranges of the distribution’s edge locations.
B.
Create an AWS Global Accelerator standard accelerator. Create a standard accelerator endpoint for the NLB in each additional Region. Provide customers with the Global Accelerator IP address.
C.
Create an Amazon CloudFront distribution. Create a custom origin for the NLB in each additional Region. Provide customers with the IP address ranges of the distribution’s edge locations.
D.
Create an AWS Global Accelerator custom routing accelerator. Create a listener for the custom routing accelerator. Add the IP address and ports for the NLB in each additional Region. Provide customers with the Global Accelerator IP address.
AnswerDiscussion
Correct Answer: B
The correct solution is to use AWS Global Accelerator standard accelerator. AWS Global Accelerator provides static IP addresses that can be used by customers in their allow lists. Standard accelerators automatically route traffic to the nearest healthy endpoint, which meets the requirement to route customers to the region geographically closest to them. Creating a standard accelerator endpoint for the Network Load Balancer in each additional region and providing customers with the Global Accelerator IP address achieves the desired outcome without the complexity of listing dynamic IP ranges from services like CloudFront.
Question 385 of 529
A company is running multiple workloads in the AWS Cloud. The company has separate units for software development. The company uses AWS Organizations and federation with SAML to give permissions to developers to manage resources in their AWS accounts. The development units each deploy their production workloads into a common production account.
Recently, an incident occurred in the production account in which members of a development unit terminated an EC2 instance that belonged to a different development unit. A solutions architect must create a solution that prevents a similar incident from happening in the future. The solution also must allow developers the possibility to manage the instances used for their workloads.
Which strategy will meet these requirements?
A.
Create separate OUs in AWS Organizations for each development unit. Assign the created OUs to the company AWS accounts. Create separate SCP with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag that matches the development unit name. Assign the SCP to the corresponding OU.
B.
Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Update the IAM policy for the developers’ assumed IAM role with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit.
C.
Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Create an SCP with an allow action and a StringEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit. Assign the SCP to the root OU.
D.
Create separate IAM policies for each development unit. For every IAM policy, add an allow action and a StringEquals condition for the DevelopmentUnit resource tag and the development unit name. During SAML federation, use AWS Security Token Service (AWS STS) to assign the IAM policy and match the development unit name to the assumed IAM role.
AnswerDiscussion
Correct Answer: B
To meet the requirements, the best strategy is to pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Then, update the IAM policy for the developers' assumed IAM role with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit. This approach ensures that each developer can only manage resources tagged with their specific development unit, preventing them from interfering with resources belonging to other units. This method effectively ties permissions to the federated identity and provides fine-grained access control that aligns with the requirement to allow developers to manage instances used for their workloads while preventing cross-unit interference.
Question 386 of 529
An enterprise company is building an infrastructure services platform for its users. The company has the following requirements:
• Provide least privilege access to users when launching AWS infrastructure so users cannot provision unapproved services.
• Use a central account to manage the creation of infrastructure services.
• Provide the ability to distribute infrastructure services to multiple accounts in AWS Organizations.
• Provide the ability to enforce tags on any infrastructure that is started by users.
Which combination of actions using AWS services will meet these requirements? (Choose three.)
A.
Develop infrastructure services using AWS CloudFormation templates. Add the templates to a central Amazon S3 bucket and add the IAM roles or users that require access to the S3 bucket policy.
B.
Develop infrastructure services using AWS CloudFormation templates. Upload each template as an AWS Service Catalog product to portfolios created in a central AWS account. Share these portfolios with the Organizations structure created for the company.
C.
Allow user IAM roles to have AWSCloudFormationFullAccess and AmazonS3ReadOnlyAccess permissions. Add an Organizations SCP at the AWS account root user level to deny all services except AWS CloudFormation and Amazon S3.
D.
Allow user IAM roles to have ServiceCatalogEndUserAccess permissions only. Use an automation script to import the central portfolios to local AWS accounts, copy the TagOption, assign users access, and apply launch constraints.
E.
Use the AWS Service Catalog TagOption Library to maintain a list of tags required by the company. Apply the TagOption to AWS Service Catalog products or portfolios.
F.
Use the AWS CloudFormation Resource Tags property to enforce the application of tags to any CloudFormation templates that will be created for users.
AnswerDiscussion
Correct Answer: B, D, E
To meet the company's requirements, using AWS Service Catalog is essential. Developing infrastructure services with AWS CloudFormation templates and uploading them as products in AWS Service Catalog portfolios (B) allows for the central management and sharing of approved services. Granting users ServiceCatalogEndUserAccess and using an automation script to manage portfolios and apply constraints (D) ensures that users have the right permissions without provisioning unapproved services. Lastly, using the AWS Service Catalog TagOption Library (E) allows for the enforcement of required tags on any started infrastructure, fulfilling the tagging requirement.
Question 387 of 529
A company deploys a new web application. As part of the setup, the company configures AWS WAF to log to Amazon S3 through Amazon Kinesis Data Firehose. The company develops an Amazon Athena query that runs once daily to return AWS WAF log data from the previous 24 hours. The volume of daily logs is constant. However, over time, the same query is taking more time to run.
A solutions architect needs to design a solution to prevent the query time from continuing to increase. The solution must minimize operational overhead.
Which solution will meet these requirements?
A.
Create an AWS Lambda function that consolidates each day's AWS WAF logs into one log file.
B.
Reduce the amount of data scanned by configuring AWS WAF to send logs to a different S3 bucket each day.
C.
Update the Kinesis Data Firehose configuration to partition the data in Amazon S3 by date and time. Create external tables for Amazon Redshift. Configure Amazon Redshift Spectrum to query the data source.
D.
Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions.
AnswerDiscussion
Correct Answer: D
Partitioning the data by date and time addresses the issue directly by reducing the amount of data scanned by Athena for each query. This solution ensures that only the relevant partitions (i.e., the logs from the previous 24 hours) are read and processed, which prevents the query time from increasing as the dataset grows. This approach minimizes operational overhead because the partitioning can be automated within the existing Kinesis Data Firehose and Athena configuration.
Question 388 of 529
A company is developing a web application that runs on Amazon EC2 instances in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). Only users from a specific country are allowed to access the application. The company needs the ability to log the access requests that have been blocked. The solution should require the least possible maintenance.
Which solution meets these requirements?
A.
Create an IPSet containing a list of IP ranges that belong to the specified country. Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from an IP range in the IPSet. Associate the rule with the web ACL. Associate the web ACL with the ALB.
B.
Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from the specified country. Associate the rule with the web ACL. Associate the web ACL with the ALB.
C.
Configure AWS Shield to block any requests that do not originate from the specified country. Associate AWS Shield with the ALB.
D.
Create a security group rule that allows ports 80 and 443 from IP ranges that belong to the specified country. Associate the security group with the ALB.
AnswerDiscussion
Correct Answer: B
The solution that meets the requirements is to create an AWS WAF web ACL, configure a rule to block any requests that do not originate from the specified country, and associate the rule with the web ACL and the ALB. AWS WAF provides the capability to create geo-matching rules, which allows blocking requests from specific countries without managing IP ranges manually. This setup logs the blocked requests, requires minimal maintenance, and offers the required granularity in blocking access based on geographic location.
Question 389 of 529
A company is migrating an application from on-premises infrastructure to the AWS Cloud. During migration design meetings, the company expressed concerns about the availability and recovery options for its legacy Windows file server. The file server contains sensitive business-critical data that cannot be recreated in the event of data corruption or data loss. According to compliance requirements, the data must not travel across the public internet. The company wants to move to AWS managed services where possible.
The company decides to store the data in an Amazon FSx for Windows File Server file system. A solutions architect must design a solution that copies the data to another AWS Region for disaster recovery (DR) purposes.
Which solution will meet these requirements?
A.
Create a destination Amazon S3 bucket in the DR Region. Establish connectivity between the FSx for Windows File Server file system in the primary Region and the S3 bucket in the DR Region by using Amazon FSx File Gateway. Configure the S3 bucket as a continuous backup source in FSx File Gateway.
B.
Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC the primary Region and the VPC in the DR Region by using AWS Site-to-Site VPN. Configure AWS DataSync to communicate by using VPN endpoints.
C.
Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using VPC peering. Configure AWS DataSync to communicate by using interface VPC endpoints with AWS PrivateLink.
D.
Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using AWS Transit Gateway in each Region. Use AWS Transfer Family to copy files between the FSx for Windows File Server file system in the primary Region and the FSx for Windows File Server file system in the DR Region over the private AWS backbone network.
AnswerDiscussion
Correct Answer: C
To address the company's concerns about availability and recovery options for its legacy Windows file server while ensuring data does not travel across the public internet, the most suitable solution involves creating an FSx for Windows File Server file system in the Disaster Recovery (DR) Region. Establishing connectivity between the VPC in the primary Region and the VPC in the DR Region via VPC peering is a secure option that keeps the data within AWS’s private network. Using interface VPC endpoints with AWS PrivateLink for AWS DataSync ensures that the data synchronization process remains secure and private, meeting compliance requirements.
Question 390 of 529
A company is currently in the design phase of an application that will need an RPO of less than 5 minutes and an RTO of less than 10 minutes. The solutions architecture team is forecasting that the database will store approximately 10 TB of data. As part of the design, they are looking for a database solution that will provide the company with the ability to fail over to a secondary Region.
Which solution will meet these business requirements at the LOWEST cost?
A.
Deploy an Amazon Aurora DB cluster and take snapshots of the cluster every 5 minutes. Once a snapshot is complete, copy the snapshot to a secondary Region to serve as a backup in the event of a failure.
B.
Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary.
C.
Deploy an Amazon Aurora DB cluster in the primary Region and another in a secondary Region. Use AWS DMS to keep the secondary Region in sync.
D.
Deploy an Amazon RDS instance with a read replica in the same Region. In the event of a failure, promote the read replica to become the primary.
AnswerDiscussion
Correct Answer: B
Deploying an Amazon RDS instance with a cross-Region read replica in a secondary Region is the most cost-effective solution that meets the requirements. It ensures a Recovery Point Objective (RPO) of less than 5 minutes by continuously replicating data to the cross-Region read replica. In the event of a failure, promoting the read replica to the primary can be accomplished within the required Recovery Time Objective (RTO) of less than 10 minutes. This setup leverages AWS RDS's built-in capabilities without incurring the higher costs associated with additional Aurora clusters or complex data migration solutions.
Question 391 of 529
A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the management account to create a new member account with finance1@example.com as the email address.
What should the solutions architect do to create IAM users in the new member account?
A.
Sign in to the AWS Management Console with AWS account root user credentials by using the 64-character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.
B.
From the management account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.
C.
Go to the AWS Management Console sign-in page. Choose “Sign in using root account credentials.” Sign in in by using the email address finance 1@example.com and the management account's root password. Set up the IAM users as required.
D.
Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.
AnswerDiscussion
Correct Answer: B
To create IAM users in the new member account, the solutions architect should switch roles from the management account to assume the OrganizationAccountAccessRole role with the account ID of the new member account. This role is automatically created by AWS Organizations when the member account is set up and grants full administrative permissions. This approach enhances security and simplifies management by avoiding the use of root credentials and directly accessing the member account through an established and secure role.
Question 392 of 529
A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors.
Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events.
The company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution.
Which strategy meets these requirements?
A.
Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.
B.
Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache.
C.
Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.
D.
Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls.
AnswerDiscussion
Correct Answer: A
The company needs to handle a significant increase in the number of requests while minimizing costs. Converting the API Gateway Regional endpoint to an edge-optimized endpoint with caching enabled can significantly reduce the number of requests hitting the Lambda functions and the database. This approach uses the AWS infrastructure to cache responses closer to the users, thus reducing the frequency of repeated queries reaching the backend, and helps in controlling costs better compared to implementing a separate caching layer with ElastiCache for Redis.
Question 393 of 529
A company is migrating an on-premises application and a MySQL database to AWS. The application processes highly sensitive data, and new data is constantly updated in the database. The data must not be transferred over the internet. The company also must encrypt the data in transit and at rest.
The database is 5 TB in size. The company already has created the database schema in an Amazon RDS for MySQL DB instance. The company has set up a 1 Gbps AWS Direct Connect connection to AWS. The company also has set up a public VIF and a private VIF. A solutions architect needs to design a solution that will migrate the data to AWS with the least possible downtime.
Which solution will meet these requirements?
A.
Perform a database backup. Copy the backup files to an AWS Snowball Edge Storage Optimized device. Import the backup to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.
B.
Use AWS Database Migration Service (AWS DMS) to migrate the data to AWS. Create a DMS replication instance in a private subnet. Create VPC endpoints for AWS DMS. Configure a DMS task to copy data from the on-premises database to the DB instance by using full load plus change data capture (CDC). Use the AWS Key Management Service (AWS KMS) default key for encryption at rest. Use TLS for encryption in transit.
C.
Perform a database backup. Use AWS DataSync to transfer the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.
D.
Use Amazon S3 File Gateway. Set up a private connection to Amazon S3 by using AWS PrivateLink. Perform a database backup. Copy the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.
AnswerDiscussion
Correct Answer: B
To meet the requirements of migrating a 5 TB MySQL database with highly sensitive data to AWS with the least possible downtime, using AWS Database Migration Service (AWS DMS) is the most appropriate solution. AWS DMS allows for continuous data replication using Change Data Capture (CDC) which ensures minimal downtime. Creating a DMS replication instance in a private subnet and using VPC endpoints for AWS DMS ensures that the data is not transferred over the internet. For encryption at rest, the AWS Key Management Service (AWS KMS) default key can be used, while TLS can be used for encryption in transit.
Question 394 of 529
Accompany is deploying a new cluster for big data analytics on AWS. The cluster will run across many Linux Amazon EC2 instances that are spread across multiple Availability Zones.
All of the nodes in the cluster must have read and write access to common underlying file storage. The file storage must be highly available, must be resilient, must be compatible with the Portable Operating System Interface (POSIX), and must accommodate high levels of throughput.
Which storage solution will meet these requirements?
A.
Provision an AWS Storage Gateway file gateway NFS file share that is attached to an Amazon S3 bucket. Mount the NFS file share on each EC2 instance in the cluster.
B.
Provision a new Amazon Elastic File System (Amazon EFS) file system that uses General Purpose performance mode. Mount the EFS file system on each EC2 instance in the cluster.
C.
Provision a new Amazon Elastic Block Store (Amazon EBS) volume that uses the io2 volume type. Attach the EBS volume to all of the EC2 instances in the cluster.
D.
Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster.
AnswerDiscussion
Correct Answer: D
To meet the requirements of high availability, resilience, compatibility with the Portable Operating System Interface (POSIX), and accommodation of high levels of throughput for a big data analytics cluster running across multiple Linux Amazon EC2 instances spread across multiple Availability Zones, the most suitable option is to provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. The Max I/O performance mode is designed to scale to higher levels of aggregate throughput and operations per second, which is ideal for data analytics workloads that require performing parallel operations from numerous nodes. While this comes at the cost of slightly higher latencies, the primary need for high throughput aligns better with the capabilities of the Max I/O performance mode compared to other options.
Question 395 of 529
A company hosts a software as a service (SaaS) solution on AWS. The solution has an Amazon API Gateway API that serves an HTTPS endpoint. The API uses AWS Lambda functions for compute. The Lambda functions store data in an Amazon Aurora Serverless v1 database.
The company used the AWS Serverless Application Model (AWS SAM) to deploy the solution. The solution extends across multiple Availability Zones and has no disaster recovery (DR) plan.
A solutions architect must design a DR strategy that can recover the solution in another AWS Region. The solution has an RTO of 5 minutes and an RPO of 1 minute.
What should the solutions architect do to meet these requirements?
A.
Create a read replica of the Aurora Serverless v1 database in the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region. Promote the read replica to primary in case of disaster.
B.
Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region.
C.
Create an Aurora Serverless v1 DB cluster that has multiple writer instances in the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration.
D.
Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration.
AnswerDiscussion
Correct Answer: D
To meet the requirements of a 5-minute Recovery Time Objective (RTO) and a 1-minute Recovery Point Objective (RPO), the best approach is to change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Then, launch the solution in the target Region and configure the two Regional solutions to work in an active-passive configuration. This setup ensures quick failover and data replication between regions, which are essential to meet the strict RTO and RPO requirements.
Question 396 of 529
A company owns a chain of travel agencies and is running an application in the AWS Cloud. Company employees use the application to search for information about travel destinations. Destination content is updated four times each year.
Two fixed Amazon EC2 instances serve the application. The company uses an Amazon Route 53 public hosted zone with a multivalue record of travel.example.com that returns the Elastic IP addresses for the EC2 instances. The application uses Amazon DynamoDB as its primary data store. The company uses a self-hosted Redis instance as a caching solution.
During content updates, the load on the EC2 instances and the caching solution increases drastically. This increased load has led to downtime on several occasions. A solutions architect must update the application so that the application is highly available and can handle the load that is generated by the content updates.
Which solution will meet these requirements?
A.
Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the EC2 instances before the content updates.
B.
Set up Amazon ElastiCache for Redis. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution’s DNS alias. Manually scale up EC2 instances before the content updates.
C.
Set up Amazon ElastiCache for Memcached. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the application before the content updates.
D.
Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution's DNS alias. Manually scale up EC2 instances before the content updates.
AnswerDiscussion
Correct Answer: A
To ensure the application is highly available and can handle the increased load during content updates, establishing DynamoDB Accelerator (DAX) as the in-memory cache is essential because DAX is specifically optimized for DynamoDB, which is the primary data store for the application. Creating an Auto Scaling group for the EC2 instances combined with an Application Load Balancer (ALB) will enable the application to distribute the load efficiently and scale dynamically. Additionally, updating the Route 53 record to target the ALB's DNS alias and configuring scheduled scaling for the EC2 instances will ensure that the resources are appropriately scaled up before content updates. This approach addresses the need for automated and efficient resource management during high-load periods, ensuring high availability and minimizing downtime.
Question 397 of 529
A company needs to store and process image data that will be uploaded from mobile devices using a custom mobile app. Usage peaks between 8 AM and 5 PM on weekdays, with thousands of uploads per minute. The app is rarely used at any other time. A user is notified when image processing is complete.
Which combination of actions should a solutions architect take to ensure image processing can scale to handle the load? (Choose three.)
A.
Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon MQ queue.
B.
Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue.
C.
Invoke an AWS Lambda function to perform image processing when a message is available in the queue.
D.
Invoke an S3 Batch Operations job to perform image processing when a message is available in the queue.
E.
Send a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS) when processing is complete.
F.
Send a push notification to the mobile app by using Amazon Simple Email Service (Amazon SES) when processing is complete.
AnswerDiscussion
Correct Answer: B, C, E
To ensure the image processing can scale to handle the load: First, upload files from the mobile app directly to Amazon S3. Use S3 event notifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is better suited for high-throughput and scalable message queuing. Second, invoke an AWS Lambda function to perform image processing when a message is available in the SQS queue; Lambda can automatically scale to accommodate the load. Finally, send a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS) when processing is complete, as SNS is specifically designed for notifications and can easily handle the required push notifications to mobile applications.
Question 398 of 529
A company is building an application on AWS. The application sends logs to an Amazon OpenSearch Service cluster for analysis. All data must be stored within a VPC.
Some of the company’s developers work from home. Other developers work from three different company office locations. The developers need to access OpenSearch Service to analyze and visualize logs directly from their local development machines.
Which solution will meet these requirements?
A.
Configure and set up an AWS Client VPN endpoint. Associate the Client VPN endpoint with a subnet in the VPC. Configure a Client VPN self-service portal. Instruct the developers to connect by using the client for Client VPN.
B.
Create a transit gateway, and connect it to the VPC. Create an AWS Site-to-Site VPN. Create an attachment to the transit gateway. Instruct the developers to connect by using an OpenVPN client.
C.
Create a transit gateway, and connect it to the VPOrder an AWS Direct Connect connection. Set up a public VIF on the Direct Connect connection. Associate the public VIF with the transit gateway. Instruct the developers to connect to the Direct Connect connection.
D.
Create and configure a bastion host in a public subnet of the VPC. Configure the bastion host security group to allow SSH access from the company CIDR ranges. Instruct the developers to connect by using SSH.
AnswerDiscussion
Correct Answer: A
To meet the requirements of allowing developers both from home and office locations to access the Amazon OpenSearch Service and ensuring that all data is stored within a VPC, configuring and setting up an AWS Client VPN endpoint is the best solution. The Client VPN endpoint can be associated with a subnet in the VPC and a self-service portal can be configured for ease of access. Developers can connect securely using the VPN client, ensuring secure and remote access to the OpenSearch Service from various locations.
Question 399 of 529
A company wants to migrate its website from an on-premises data center onto AWS. At the same time, it wants to migrate the website to a containerized microservice-based architecture to improve the availability and cost efficiency. The company’s security policy states that privileges and network permissions must be configured according to best practice, using least privilege.
A solutions architect must create a containerized architecture that meets the security requirements and has deployed the application to an Amazon ECS cluster.
What steps are required after the deployment to meet the requirements? (Choose two.)
A.
Create tasks using the bridge network mode.
B.
Create tasks using the awsvpc network mode.
C.
Apply security groups to Amazon EC2 instances, and use IAM roles for EC2 instances to access other resources.
D.
Apply security groups to the tasks, and pass IAM credentials into the container at launch time to access other resources.
E.
Apply security groups to the tasks, and use IAM roles for tasks to access other resources.
AnswerDiscussion
Correct Answer: B, E
To ensure the containerized architecture adheres to the company's security policy and follows the least privilege principle, two critical steps should be taken. First, tasks should be created using the awsvpc network mode. This network mode assigns each task an Elastic Network Interface (ENI), which allows for granular control of network traffic using security groups applied specifically to the tasks. Second, IAM roles for tasks should be used to access other resources. This ensures that each task has the minimum required permissions, aligning with the least privilege principle. Applying security groups directly to the tasks and using IAM roles for tasks provides better security and isolation compared to using more general solution like applying roles and security groups at the EC2 instance level.
Question 400 of 529
A company is running a serverless application that consists of several AWS Lambda functions and Amazon DynamoDB tables. The company has created new functionality that requires the Lambda functions to access an Amazon Neptune DB cluster. The Neptune DB cluster is located in three subnets in a VPC.
Which of the possible solutions will allow the Lambda functions to access the Neptune DB cluster and DynamoDB tables? (Choose two.)
A.
Create three public subnets in the Neptune VPC, and route traffic through an internet gateway. Host the Lambda functions in the three new public subnets.
B.
Create three private subnets in the Neptune VPC, and route internet traffic through a NAT gateway. Host the Lambda functions in the three new private subnets.
C.
Host the Lambda functions outside the VPUpdate the Neptune security group to allow access from the IP ranges of the Lambda functions.
D.
Host the Lambda functions outside the VPC. Create a VPC endpoint for the Neptune database, and have the Lambda functions access Neptune over the VPC endpoint.
E.
Create three private subnets in the Neptune VPC. Host the Lambda functions in the three new isolated subnets. Create a VPC endpoint for DynamoDB, and route DynamoDB traffic to the VPC endpoint.
AnswerDiscussion
Correct Answer: B, E
To allow AWS Lambda functions to access an Amazon Neptune DB cluster, which is located in subnets within a VPC, the Lambda functions must also reside within the same VPC. Also, to interact with Amazon DynamoDB tables, the connection does not require being in a VPC, but it's important to manage the traffic efficiently. Creating three private subnets in the Neptune VPC and routing internet traffic through a NAT gateway ensures the Lambda functions can access external services while residing in the same VPC as Neptune, fulfilling part of the requirement. Additionally, creating three private subnets in the Neptune VPC and establishing a VPC endpoint for DynamoDB ensures secure and efficient access to DynamoDB tables and Neptune DB cluster from the Lambda functions within the VPC. Therefore, the correct solutions are creating private subnets for the Lambda functions in the Neptune VPC, routing traffic accordingly, and using a VPC endpoint for DynamoDB.
Question 401 of 529
A company wants to design a disaster recovery (DR) solution for an application that runs in the company’s data center. The application writes to an SMB file share and creates a copy on a second file share. Both file shares are in the data center. The application uses two types of files: metadata files and image files.
The company wants to store the copy on AWS. The company needs the ability to use SMB to access the data from either the data center or AWS if a disaster occurs. The copy of the data is rarely accessed but must be available within 5 minutes.
A.
Deploy AWS Outposts with Amazon S3 storage. Configure a Windows Amazon EC2 instance on Outposts as a file server.
B.
Deploy an Amazon FSx File Gateway. Configure an Amazon FSx for Windows File Server Multi-AZ file system that uses SSD storage.
C.
Deploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and to use S3 Glacier Deep Archive for the image files.
D.
Deploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and image files.
AnswerDiscussion
Correct Answer: B
To fulfill the given requirements, deploying an Amazon FSx File Gateway and configuring an Amazon FSx for Windows File Server Multi-AZ file system with SSD storage is the most suitable option. This setup provides SMB file share access and ensures that the data can be accessed from either the data center or AWS in case of a disaster. The AWS FSx for Windows File Server is designed for fast access, which meets the requirement of data being available within 5 minutes. Additionally, it supports the SMB protocol, making it accessible from both the data center and AWS, aligning with the disaster recovery needs.
Question 402 of 529
A company is creating a solution that can move 400 employees into a remote working environment in the event of an unexpected disaster. The user desktops have a mix of Windows and Linux operating systems. Multiple types of software, such as web browsers and mail clients, are installed on each desktop.
A solutions architect needs to implement a solution that can be integrated with the company’s on-premises Active Directory to allow employees to use their existing identity credentials. The solution must provide multifactor authentication (MFA) and must replicate the user experience from the existing desktops.
Which solution will meet these requirements?
A.
Use Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Activate MFA for Amazon WorkSpaces by using the AWS Management Console.
B.
Use Amazon AppStream 2.0 as an application streaming service. Configure Desktop View for the employees. Set up a VPN connection to the on-premises network. Set up Active Directory Federation Services (AD FS) on premises. Connect the VPC network to AD FS through the VPN connection.
C.
Use Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Configure a RADIUS server for MFA.
D.
Use Amazon AppStream 2.0 as an application streaming service. Set up Active Directory Federation Services on premises. Configure MFA to grant users access on AppStream 2.0.
AnswerDiscussion
Correct Answer: C
Using Amazon WorkSpaces for the cloud desktop service, setting up a VPN connection to the on-premises network, creating an AD Connector to connect to the on-premises Active Directory, and configuring a RADIUS server for MFA is the most suitable approach. This solution ensures that employees can use their existing identity credentials by integrating with the company's Active Directory and providing the necessary multifactor authentication. Additionally, Amazon WorkSpaces supports both Windows and Linux desktops, which matches the company's existing environment.
Question 403 of 529
A company has deployed an Amazon Connect contact center. Contact center agents are reporting large numbers of computer-generated calls. The company is concerned about the cost and productivity effects of these calls. The company wants a solution that will allow agents to flag the call as spam and automatically block the numbers from going to an agent in the future.
What is the MOST operationally efficient solution to meet these requirements?
A.
Customize the Contact Control Panel (CCP) by adding a flag call button that will invoke an AWS Lambda function that calls the UpdateContactAttributes API. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the updated attribute and to use a Lambda function to read and write to the DynamoDB table.
B.
Use a Contact Lens for Amazon Connect rule that will look for spam calls. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the rule and to invoke an AWS Lambda function to read and write to the DynamoDB table.
C.
Use an Amazon DynamoDB table to store the spam numbers. Create a quick connect that the agents can transfer the spam call to from the Contact Control Panel (CCP). Modify the quick connect contact flow to invoke an AWS Lambda function to write to the DynamoDB table.
D.
Modify the initial contact flow to ask for caller input. If the agent does not receive input, the agent should mark the caller as spam. Use an Amazon DynamoDB table to store the spam numbers. Use an AWS Lambda function to read and write to the DynamoDB table.
AnswerDiscussion
Correct Answer: A
Customizing the Contact Control Panel (CCP) by adding a 'flag call' button allows agents to easily mark calls as spam. This button can invoke an AWS Lambda function that calls the UpdateContactAttributes API to flag the call. Using an Amazon DynamoDB table to store the spam numbers ensures there is a reliable storage mechanism for tracking blocked numbers. Modifying the contact flows to check for the spam attribute and interact with the DynamoDB table via Lambda ensures that future calls from these numbers are automatically blocked. This solution provides a seamless experience for agents, requires minimal disruption to their workflow, and leverages AWS services efficiently for real-time processing and storage.
Question 404 of 529
A company has mounted sensors to collect information about environmental parameters such as humidity and light throughout all the company's factories. The company needs to stream and analyze the data in the AWS Cloud in real time. If any of the parameters fall out of acceptable ranges, the factory operations team must receive a notification immediately.
Which solution will meet these requirements?
A.
Stream the data to an Amazon Kinesis Data Firehose delivery stream. Use AWS Step Functions to consume and analyze the data in the Kinesis Data Firehose delivery stream. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.
B.
Stream the data to an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster. Set up a trigger in Amazon MSK to invoke an AWS Fargate task to analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team.
C.
Stream the data to an Amazon Kinesis data stream. Create an AWS Lambda function to consume the Kinesis data stream and to analyze the data. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.
D.
Stream the data to an Amazon Kinesis Data Analytics application. Use an automatically scaled and containerized service in Amazon Elastic Container Service (Amazon ECS) to consume and analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team.
AnswerDiscussion
Correct Answer: C
In this scenario, the company needs a solution that can handle real-time data streaming, analysis, and immediate notification capabilities. Streaming the data to an Amazon Kinesis data stream and using an AWS Lambda function to consume and analyze the data provides an efficient and scalable way to process the information in real time. AWS Lambda triggers can act upon data immediately as it enters the stream. Using Amazon Simple Notification Service (Amazon SNS) ensures that the operations team receives notifications without delay. This solution covers all the requirements: real-time data processing, analysis, and instant notification.
Question 405 of 529
A company is preparing to deploy an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for a workload. The company expects the cluster to support an unpredictable number of stateless pods. Many of the pods will be created during a short time period as the workload automatically scales the number of replicas that the workload uses.
Which solution will MAXIMIZE node resilience?
A.
Use a separate launch template to deploy the EKS control plane into a second cluster that is separate from the workload node groups.
B.
Update the workload node groups. Use a smaller number of node groups and larger instances in the node groups.
C.
Configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned.
D.
Configure the workload to use topology spread constraints that are based on Availability Zone.
AnswerDiscussion
Correct Answer: D
To maximize node resilience in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster expected to support an unpredictable number of stateless pods, you should configure the workload to use topology spread constraints based on Availability Zone. This approach enhances the stability and availability of the EKS cluster by distributing the workload evenly across different Availability Zones, thereby reducing the impact of any zone-specific failures or performance issues. This method ensures that nodes remain resilient by limiting the effects of zonal disruptions.
Question 406 of 529
A company needs to implement a disaster recovery (DR) plan for a web application. The application runs in a single AWS Region.
The application uses microservices that run in containers. The containers are hosted on AWS Fargate in Amazon Elastic Container Service (Amazon ECS). The application has an Amazon RDS for MySQL DB instance as its data layer and uses Amazon Route 53 for DNS resolution. An Amazon CloudWatch alarm invokes an Amazon EventBridge rule if the application experiences a failure.
A solutions architect must design a DR solution to provide application recovery to a separate Region. The solution must minimize the time that is necessary to recover from a failure.
Which solution will meet these requirements?
A.
Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create an AWS Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.
B.
Create an AWS Lambda function that creates a second ECS cluster and ECS service in the separate Region. Configure the Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.
C.
Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create a cross-Region read replica of the RDS DB instance in the separate Region. Create an AWS Lambda function to promote the read replica to the primary database. Configure the Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.
D.
Setup a second ECS cluster and ECS service on Fargate in the separate Region. Take a snapshot of the RDS DB instance. Convert the snapshot to an Amazon DynamoDB global table. Create an AWS Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.
AnswerDiscussion
Correct Answer: C
The best solution to minimize recovery time involves setting up a second ECS cluster and ECS service on Fargate in the separate Region and using a cross-Region read replica of the RDS DB instance. In the event of a failure, the read replica can quickly be promoted to the primary database, ensuring minimal downtime. The solution should also include an AWS Lambda function to promote the read replica to the primary database and update Route 53 to route traffic to the second ECS cluster. Updating the EventBridge rule to invoke the Lambda function ensures automatic failover when a failure is detected.
Question 407 of 529
A company has AWS accounts that are in an organization in AWS Organizations. The company wants to track Amazon EC2 usage as a metric. The company’s architecture team must receive a daily alert if the EC2 usage is more than 10% higher the average EC2 usage from the last 30 days.
Which solution will meet these requirements?
A.
Configure AWS Budgets in the organization's management account. Specify a usage type of EC2 running hours. Specify a daily period. Set the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer. Configure an alert to notify the architecture team if the usage threshold is met
B.
Configure AWS Cost Anomaly Detection in the organization's management account. Configure a monitor type of AWS Service. Apply a filter of Amazon EC2. Configure an alert subscription to notify the architecture team if the usage is 10% more than the average usage for the last 30 days.
C.
Enable AWS Trusted Advisor in the organization's management account. Configure a cost optimization advisory alert to notify the architecture team if the EC2 usage is 10% more than the reported average usage for the last 30 days.
D.
Configure Amazon Detective in the organization's management account. Configure an EC2 usage anomaly alert to notify the architecture team if Detective identifies a usage anomaly of more than 10%.
AnswerDiscussion
Correct Answer: A
To meet the requirement of tracking Amazon EC2 usage and receiving a daily alert if usage exceeds the average from the last 30 days by more than 10%, the best solution would be to configure AWS Budgets. AWS Budgets allows for detailed tracking and alerting based on both cost and usage metrics, including specific usage types like EC2 running hours. By specifying the usage type and setting the budget amount based on the average usage from the last 30 days, one can set an alert to notify the architecture team when the usage exceeds the defined threshold. Other options, such as AWS Cost Anomaly Detection, are more focused on cost anomalies rather than specific usage metrics. Therefore, AWS Budgets is the appropriate tool for this scenario.
Question 408 of 529
An e-commerce company is revamping its IT infrastructure and is planning to use AWS services. The company’s CIO has asked a solutions architect to design a simple, highly available, and loosely coupled order processing application. The application is responsible for receiving and processing orders before storing them in an Amazon DynamoDB table. The application has a sporadic traffic pattern and should be able to scale during marketing campaigns to process the orders with minimal delays.
Which of the following is the MOST reliable approach to meet the requirements?
A.
Receive the orders in an Amazon EC2-hosted database and use EC2 instances to process them.
B.
Receive the orders in an Amazon SQS queue and invoke an AWS Lambda function to process them.
C.
Receive the orders using the AWS Step Functions program and launch an Amazon ECS container to process them.
D.
Receive the orders in Amazon Kinesis Data Streams and use Amazon EC2 instances to process them.
AnswerDiscussion
Correct Answer: B
The most reliable approach for the given requirements is to use Amazon SQS to receive orders and AWS Lambda to process them. Amazon SQS (Simple Queue Service) ensures that the application is loosely coupled and can handle sporadic traffic patterns by queuing the incoming orders efficiently. AWS Lambda provides automatic scaling, allowing the application to scale during marketing campaigns to process orders with minimal delays. This setup is simple, highly available, and eliminates the need for managing servers, making it suitable for the specified requirements.
Question 409 of 529
A company is deploying AWS Lambda functions that access an Amazon RDS for PostgreSQL database. The company needs to launch the Lambda functions in a QA environment and in a production environment.
The company must not expose credentials within application code and must rotate passwords automatically.
Which solution will meet these requirements?
A.
Store the database credentials for both environments in AWS Systems Manager Parameter Store. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Within the application code of the Lambda functions, pull the credentials from the Parameter Store parameter by using the AWS SDK for Python (Boto3). Add a role to the Lambda functions to provide access to the Parameter Store parameter.
B.
Store the database credentials for both environments in AWS Secrets Manager with distinct key entry for the QA environment and the production environment. Turn on rotation. Provide a reference to the Secrets Manager key as an environment variable for the Lambda functions.
C.
Store the database credentials for both environments in AWS Key Management Service (AWS KMS). Turn on rotation. Provide a reference to the credentials that are stored in AWS KMS as an environment variable for the Lambda functions.
D.
Create separate S3 buckets for the QA environment and the production environment. Turn on server-side encryption with AWS KMS keys (SSE-KMS) for the S3 buckets. Use an object naming pattern that gives each Lambda function’s application code the ability to pull the correct credentials for the function's corresponding environment. Grant each Lambda function's execution role access to Amazon S3.
AnswerDiscussion
Correct Answer: B
AWS Secrets Manager is the optimal choice for storing and managing database credentials in both QA and production environments because it provides built-in support for automatic password rotation, which is a critical requirement. By storing credentials in AWS Secrets Manager and enabling rotation, you ensure that passwords are automatically updated without manual intervention. Additionally, providing a reference to the Secrets Manager key as an environment variable for the Lambda functions allows easy and secure access to the credentials, meeting the need to avoid exposing them within application code.
Question 410 of 529
A company is using AWS Control Tower to manage AWS accounts in an organization in AWS Organizations. The company has an OU that contains accounts. The company must prevent any new or existing Amazon EC2 instances in the OU's accounts from gaining a public IP address.
Which solution will meet these requirements?
A.
Configure all instances in each account in the OU to use AWS Systems Manager. Use a Systems Manager Automation runbook to prevent public IP addresses from being attached to the instances.
B.
Implement the AWS Control Tower proactive control to check whether instances in the OU's accounts have a public IP address. Set the AssociatePublicIpAddress property to False. Attach the proactive control to the OU.
C.
Create an SCP that prevents the launch of instances that have a public IP address. Additionally, configure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP to the OU.
D.
Create an AWS Config custom rule that detects instances that have a public IP address. Configure a remediation action that uses an AWS Lambda function to detach the public IP addresses from the instances.
AnswerDiscussion
Correct Answer: C
To meet the requirement of preventing any new or existing Amazon EC2 instances in the organization's accounts from gaining a public IP address, creating an SCP (Service Control Policy) is a suitable solution. An SCP can enforce policies at the account or OU level to restrict specific actions, in this case, preventing both the launch of new instances with public IP addresses and the attachment of public IPs to existing instances. This approach ensures comprehensive control over the IP addressing policies across all accounts within the OU.
Question 411 of 529
A company is deploying a third-party web application on AWS. The application is packaged as a Docker image. The company has deployed the Docker image as an AWS Fargate service in Amazon Elastic Container Service (Amazon ECS). An Application Load Balancer (ALB) directs traffic to the application.
The company needs to give only a specific list of users the ability to access the application from the internet. The company cannot change the application and cannot integrate the application with an identity provider. All users must be authenticated through multi-factor authentication (MFA).
Which solution will meet these requirements?
A.
Create a user pool in Amazon Cognito. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFConfigure a listener rule on the ALB to require authentication through the Amazon Cognito hosted UI.
B.
Configure the users in AWS Identity and Access Management (IAM). Attach a resource policy to the Fargate service to require users to use MFA. Configure a listener rule on the ALB to require authentication through IAM.
C.
Configure the users in AWS Identity and Access Management (IAM). Enable AWS IAM Identity Center (AWS Single Sign-On). Configure resource protection for the ALB. Create a resource protection rule to require users to use MFA.
D.
Create a user pool in AWS Amplify. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFA. Configure a listener rule on the ALB to require authentication through the Amplify hosted UI.
AnswerDiscussion
Correct Answer: A
To meet the requirement of restricting access only to a specific list of users with multi-factor authentication (MFA) and not changing the application or integrating it with an identity provider, Amazon Cognito is the appropriate solution. By creating a user pool in Amazon Cognito, you can manage the user authentication and ensure MFA is required. The Application Load Balancer (ALB) can be configured with a listener rule to use Cognito for authentication, directing users to the hosted UI for login and MFA verification. This method ensures that only authenticated users with the required MFA can access the application, fulfilling all the stated requirements.
Question 412 of 529
A solutions architect is preparing to deploy a new security tool into several previously unused AWS Regions. The solutions architect will deploy the tool by using an AWS CloudFormation stack set. The stack set's template contains an IAM role that has a custom name. Upon creation of the stack set, no stack instances are created successfully.
What should the solutions architect do to deploy the stacks successfully?
A.
Enable the new Regions in all relevant accounts. Specify the CAPABILITY_NAMED_IAM capability during the creation of the stack set.
B.
Use the Service Quotas console to request a quota increase for the number of CloudFormation stacks in each new Region in all relevant accounts. Specify the CAPABILITY_IAM capability during the creation of the stack set.
C.
Specify the CAPABILITY_NAMED_IAM capability and the SELF_MANAGED permissions model during the creation of the stack set.
D.
Specify an administration role ARN and the CAPABILITY_IAM capability during the creation of the stack set.
AnswerDiscussion
Correct Answer: A
To successfully deploy the stacks using a CloudFormation stack set in previously unused AWS Regions, it is crucial to ensure that these Regions are enabled in all relevant accounts. Additionally, since the stack set's template contains an IAM role with a custom name, it is necessary to specify the CAPABILITY_NAMED_IAM capability during the creation of the stack set. This capability explicitly acknowledges that the CloudFormation stack contains IAM resources with custom names, which is required for the proper deployment of such resources.
Question 413 of 529
A company has an application that uses an Amazon Aurora PostgreSQL DB cluster for the application's database. The DB cluster contains one small primary instance and three larger replica instances. The application runs on an AWS Lambda function. The application makes many short-lived connections to the database's replica instances to perform read-only operations.
During periods of high traffic, the application becomes unreliable and the database reports that too many connections are being established. The frequency of high-traffic periods is unpredictable.
Which solution will improve the reliability of the application?
A.
Use Amazon RDS Proxy to create a proxy for the DB cluster. Configure a read-only endpoint for the proxy. Update the Lambda function to connect to the proxy endpoint.
B.
Increase the max_connections setting on the DB cluster's parameter group. Reboot all the instances in the DB cluster. Update the Lambda function to connect to the DB cluster endpoint.
C.
Configure instance scaling for the DB cluster to occur when the DatabaseConnections metric is close to the max connections setting. Update the Lambda function to connect to the Aurora reader endpoint.
D.
Use Amazon RDS Proxy to create a proxy for the DB cluster. Configure a read-only endpoint for the Aurora Data API on the proxy. Update the Lambda function to connect to the proxy endpoint.
AnswerDiscussion
Correct Answer: A
To address the issue of too many connections being established during high traffic periods, using Amazon RDS Proxy is a suitable solution. RDS Proxy can manage a pool of database connections and share them among multiple clients, which can help reduce the overhead of creating and closing connections frequently. By configuring a read-only endpoint for the proxy and updating the Lambda function to connect to this proxy endpoint, the application can improve its reliability during high-traffic periods. Increasing the max_connections setting or configuring instance scaling, as mentioned in other options, may not effectively address the root cause of the issue, which is the high frequency of short-lived connections. Therefore, using Amazon RDS Proxy with a read-only endpoint is the best solution.
Question 414 of 529
A retail company is mounting IoT sensors in all of its stores worldwide. During the manufacturing of each sensor, the company’s private certificate authority (CA) issues an X.509 certificate that contains a unique serial number. The company then deploys each certificate to its respective sensor.
A solutions architect needs to give the sensors the ability to send data to AWS after they are installed. Sensors must not be able to send data to AWS until they are installed.
Which solution will meet these requirements?
A.
Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. During manufacturing, call the RegisterThing API operation and specify the template and parameters.
B.
Create an AWS Step Functions state machine that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Specify the Step Functions state machine to validate parameters. Call the StartThingRegistrationTask API operation during installation.
C.
Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. Register the CA with AWS IoT Core, specify the provisioning template, and set the allow-auto-registration parameter.
D.
Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Include parameter validation in the template. Provision a claim certificate and a private key for each device that uses the CA. Grant AWS IoT Core service permissions to update AWS IoT things during provisioning.
AnswerDiscussion
Correct Answer: A
To meet the requirement of sensors not sending data to AWS until they are installed, the most suitable solution involves validating the serial number only at the time of installation and not during the manufacturing process. Option A suggests creating an AWS Lambda function to validate the serial number and using an AWS IoT Core provisioning template with a pre-provisioning hook. The pre-provisioning hook ensures that the sensors are validated and provisioned only at the time of installation by calling the RegisterThing API operation and specifying the template and parameters. This approach effectively restricts sensors from sending data until they are properly installed, aligning with the requirements.
Question 415 of 529
A startup company recently migrated a large ecommerce website to AWS. The website has experienced a 70% increase in sales. Software engineers are using a private GitHub repository to manage code. The DevOps team is using Jenkins for builds and unit testing. The engineers need to receive notifications for bad builds and zero downtime during deployments. The engineers also need to ensure any changes to production are seamless for users and can be rolled back in the event of a major issue.
The software engineers have decided to use AWS CodePipeline to manage their build and deployment process.
Which solution will meet these requirements?
A.
Use GitHub websockets to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy.
B.
Use GitHub webhooks to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.
C.
Use GitHub websockets to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.
D.
Use GitHub webhooks to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy.
AnswerDiscussion
Correct Answer: B
The correct solution involves triggering the CodePipeline pipeline using GitHub webhooks, as they provide a more straightforward and reliable method to integrate with AWS CodePipeline. Since the engineers are already using Jenkins for builds and unit testing, integrating it with AWS CodeBuild is a strategic choice to maintain consistency and efficiency. Alerts for any failed builds can be efficiently sent to an Amazon SNS topic, which serves the notification requirement. Importantly, to ensure seamless deployments and easy rollbacks, a blue/green deployment strategy using AWS CodeDeploy is essential. This approach allows for zero downtime during deployments and the ability to revert to the previous version if any issues occur, meeting all the specified requirements.
Question 416 of 529
A software as a service (SaaS) company has developed a multi-tenant environment. The company uses Amazon DynamoDB tables that the tenants share for the storage layer. The company uses AWS Lambda functions for the application services.
The company wants to offer a tiered subscription model that is based on resource consumption by each tenant. Each tenant is identified by a unique tenant ID that is sent as part of each request to the Lambda functions. The company has created an AWS Cost and Usage Report (AWS CUR) in an AWS account. The company wants to allocate the DynamoDB costs to each tenant to match that tenant's resource consumption.
Which solution will provide a granular view of the DynamoDB cost for each tenant with the LEAST operational effort?
A.
Associate a new tag that is named tenant ID with each table in DynamoDB. Activate the tag as a cost allocation tag in the AWS Billing and Cost Management console. Deploy new Lambda function code to log the tenant ID in Amazon CloudWatch Logs. Use the AWS CUR to separate DynamoDB consumption cost for each tenant ID.
B.
Configure the Lambda functions to log the tenant ID and the number of RCUs and WCUs consumed from DynamoDB for each transaction to Amazon CloudWatch Logs. Deploy another Lambda function to calculate the tenant costs by using the logged capacity units and the overall DynamoDB cost from the AWS Cost Explorer API. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule.
C.
Create a new partition key that associates DynamoDB items with individual tenants. Deploy a Lambda function to populate the new column as part of each transaction. Deploy another Lambda function to calculate the tenant costs by using Amazon Athena to calculate the number of tenant items from DynamoDB and the overall DynamoDB cost from the AWS CUR. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule.
D.
Deploy a Lambda function to log the tenant ID, the size of each response, and the duration of the transaction call as custom metrics to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to query the custom metrics for each tenant. Use AWS Pricing Calculator to obtain the overall DynamoDB costs and to calculate the tenant costs.
AnswerDiscussion
Correct Answer: B
The optimal solution should provide fine-grained cost allocation with minimal operational effort. Configuring the Lambda functions to log the tenant ID and the consumption of read and write capacity units (RCUs and WCUs) to CloudWatch Logs allows the precise tracking of each tenant's usage. Deploying an additional Lambda function to periodically calculate tenant costs based on logged data and overall DynamoDB costs via the AWS Cost Explorer API ensures an automated, scalable approach. This method leverages existing AWS services effectively without extensive manual intervention.
Question 417 of 529
A company has an application that stores data in a single Amazon S3 bucket. The company must keep all data for 1 year. The company’s security team is concerned that an attacker could gain access to the AWS account through leaked long-term credentials.
Which solution will ensure that existing and future objects in the S3 bucket are protected?
A.
Create a new AWS account that is accessible only to the security team through an assumed role. Create an S3 bucket in the new account. Enable S3 Versioning and S3 Object Lock. Configure a default retention period of 1 year. Set up replication from the existing S3 bucket to the new S3 bucket. Create an S3 Batch Replication job to copy all existing data.
B.
Use the s3-bucket-versioning-enabled AWS Config managed rule. Configure an automatic remediation action that uses an AWS Lambda function to enable S3 Versioning and MFA Delete on noncompliant resources. Add an S3 Lifecycle rule to delete objects after 1 year.
C.
Explicitly deny bucket creation from all users and roles except for an AWS Service Catalog launch constraint role. Define a Service Catalog product for the creation of the S3 bucket to force S3 Versioning and MFA Delete to be enabled. Authorize users to launch the product when they need to create an S3 bucket.
D.
Enable Amazon GuardDuty with the S3 protection feature for the account and the AWS Region. Add an S3 Lifecycle rule to delete objects after 1 year.
AnswerDiscussion
Correct Answer: A
To ensure that existing and future objects in the S3 bucket are protected against unauthorized access or deletion due to leaked long-term credentials, the best solution is to use an isolated account setup where only the security team can assume roles to manage it. By creating a new AWS account specifically for the security team, setting up an S3 bucket with Versioning and Object Lock enabled, and configuring a default retention period of 1 year, the data is protected from accidental or malicious deletion. Additionally, replicating the existing bucket's contents to the new S3 bucket ensures that even if the original account's credentials are compromised, the data remains secure in the new bucket. This approach combines the principles of least privilege, account isolation, and data integrity measures to offer robust protection.
Question 418 of 529
A company needs to improve the security of its web-based application on AWS. The application uses Amazon CloudFront with two custom origins. The first custom origin routes requests to an Amazon API Gateway HTTP API. The second custom origin routes traffic to an Application Load Balancer (ALB). The application integrates with an OpenID Connect (OIDC) identity provider (IdP) for user management.
A security audit shows that a JSON Web Token (JWT) authorizer provides access to the API. The security audit also shows that the ALB accepts requests from unauthenticated users.
A solutions architect must design a solution to ensure that all backend services respond to only authenticated users.
Which solution will meet this requirement?
A.
Configure the ALB to enforce authentication and authorization by integrating the ALB with the IdP. Allow only authenticated users to access the backend services.
B.
Modify the CloudFront configuration to use signed URLs. Implement a permissive signing policy that allows any request to access the backend services.
C.
Create an AWS WAF web ACL that filters out unauthenticated requests at the ALB level. Allow only authenticated traffic to reach the backend services.
D.
Enable AWS CloudTrail to log all requests that come to the ALB. Create an AWS Lambda function to analyze the logs and block any requests that come from unauthenticated users.
AnswerDiscussion
Correct Answer: A
To ensure that all backend services respond only to authenticated users, the best solution is to configure the Application Load Balancer (ALB) to enforce authentication and authorization by integrating it with the OpenID Connect (OIDC) identity provider (IdP). This will allow only authenticated users to access the backend services, ensuring that the ALB does not accept requests from unauthenticated users. This approach directly addresses the issue found in the security audit and fulfills the requirement to authenticate users before they can reach any backend services.
Question 419 of 529
A company creates an AWS Control Tower landing zone to manage and govern a multi-account AWS environment. The company's security team will deploy preventive controls and detective controls to monitor AWS services across all the accounts. The security team needs a centralized view of the security state of all the accounts.
Which solution will meet these requirements?
A.
From the AWS Control Tower management account, use AWS CloudFormation StackSets to deploy an AWS Config conformance pack to all accounts in the organization.
B.
Enable Amazon Detective for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Detective.
C.
From the AWS Control Tower management account, deploy an AWS CloudFormation stack set that uses the automatic deployment option to enable Amazon Detective for the organization.
D.
Enable AWS Security Hub for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Security Hub.
AnswerDiscussion
Correct Answer: D
The security team requires a centralized view of the security state across all accounts, and AWS Security Hub is designed to provide this functionality. By enabling AWS Security Hub for the organization in AWS Organizations and designating one AWS account as the delegated administrator for Security Hub, the company can aggregate and centralize security findings from various AWS services and accounts. This solution meets the requirements of having a comprehensive and centralized security overview, allowing the deployment of preventive and detective controls effectively.
Question 420 of 529
A company that develops consumer electronics with offices in Europe and Asia has 60 TB of software images stored on premises in Europe. The company wants to transfer the images to an Amazon S3 bucket in the ap-northeast-1 Region. New software images are created daily and must be encrypted in transit. The company needs a solution that does not require custom development to automatically transfer all existing and new software images to Amazon S3.
What is the next step in the transfer process?
A.
Deploy an AWS DataSync agent and configure a task to transfer the images to the S3 bucket.
B.
Configure Amazon Kinesis Data Firehose to transfer the images using S3 Transfer Acceleration.
C.
Use an AWS Snowball device to transfer the images with the S3 bucket as the target.
D.
Transfer the images over a Site-to-Site VPN connection using the S3 API with multipart upload.
AnswerDiscussion
Correct Answer: A
Deploying an AWS DataSync agent and configuring a task to transfer the images to the S3 bucket is the best solution. AWS DataSync is designed to simplify and automate the process of transferring data between on-premises storage and AWS storage services, such as Amazon S3. It supports encryption in transit and can handle both existing and new files efficiently. This solution does not require custom development and meets the company's requirement for ongoing, automated transfers of new software images.
Question 421 of 529
A company has a web application that uses Amazon API Gateway. AWS Lambda, and Amazon DynamoDB. A recent marketing campaign has increased demand. Monitoring software reports that many requests have significantly longer response times than before the marketing campaign.
A solutions architect enabled Amazon CloudWatch Logs for API Gateway and noticed that errors are occurring on 20% of the requests. In CloudWatch, the Lambda function Throttles metric represents 1% of the requests and the Errors metric represents 10% of the requests. Application logs indicate that, when errors occur, there is a call to DynamoDB.
What change should the solutions architect make to improve the current response times as the web application becomes more popular?
A.
Increase the concurrency limit of the Lambda function.
B.
Implement DynamoDB auto scaling on the table.
C.
Increase the API Gateway throttle limit.
D.
Re-create the DynamoDB table with a better-partitioned primary index.
AnswerDiscussion
Correct Answer: B
The reported issue stems from the errors occurring in DynamoDB, as indicated by the application logs. Implementing DynamoDB auto scaling on the table will dynamically adjust the provisioned throughput capacity, allowing it to handle increased demand efficiently. This change should reduce the errors and improve the response times as the web application becomes more popular, addressing the core problem related to DynamoDB capacity.
Question 422 of 529
A company has an application that has a web frontend. The application runs in the company's on-premises data center and requires access to file storage for critical data. The application runs on three Linux VMs for redundancy. The architecture includes a load balancer with HTTP request-based routing.
The company needs to migrate the application to AWS as quickly as possible. The architecture on AWS must be highly available.
Which solution will meet these requirements with the FEWEST changes to the architecture?
A.
Migrate the application to Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type in three Availability Zones. Use Amazon S3 to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers.
B.
Migrate the application to Amazon EC2 instances in three Availability Zones. Use Amazon Elastic File System (Amazon EFS) for file storage. Mount the file storage on all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances.
C.
Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use the Fargate launch type in three Availability Zones. Use Amazon FSx for Lustre to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers.
D.
Migrate the application to Amazon EC2 instances in three AWS Regions. Use Amazon Elastic Block Store (Amazon EBS) for file storage. Enable Cross-Region Replication (CRR) for all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances.
AnswerDiscussion
Correct Answer: B
To migrate the application to AWS with the fewest changes to the existing architecture, the suitable solution is to use Amazon EC2 instances in three Availability Zones, Amazon Elastic File System (EFS) for file storage, and an Application Load Balancer (ALB) to direct traffic. This setup closely mirrors the company's on-premises architecture by maintaining the use of EC2 instances as direct equivalents to the current Linux VMs for compute capacity and using Amazon EFS for shared file storage, similar to on-premises file storage. Additionally, an ALB matches the requirement for HTTP request-based routing, ensuring high availability and minimal changes to the existing setup.
Question 423 of 529
A company is planning to migrate an on-premises data center to AWS. The company currently hosts the data center on Linux-based VMware VMs. A solutions architect must collect information about network dependencies between the VMs. The information must be in the form of a diagram that details host IP addresses, hostnames, and network connection information.
Which solution will meet these requirements?
A.
Use AWS Application Discovery Service. Select an AWS Migration Hub home AWS Region. Install the AWS Application Discovery Agent on the on-premises servers for data collection. Grant permissions to Application Discovery Service to use the Migration Hub network diagrams.
B.
Use the AWS Application Discovery Service Agentless Collector for server data collection. Export the network diagrams from the AWS Migration Hub in .png format.
C.
Install the AWS Application Migration Service agent on the on-premises servers for data collection. Use AWS Migration Hub data in Workload Discovery on AWS to generate network diagrams.
D.
Install the AWS Application Migration Service agent on the on-premises servers for data collection. Export data from AWS Migration Hub in .csv format into an Amazon CloudWatch dashboard to generate network diagrams.
AnswerDiscussion
Correct Answer: A
To collect information about network dependencies between the VMs, including host IP addresses, hostnames, and network connection information, the appropriate solution is to use AWS Application Discovery Service. This service requires selecting an AWS Migration Hub home AWS Region and installing the AWS Application Discovery Agent on the on-premises servers for accurate data collection. It also necessitates granting permissions to the Application Discovery Service to use the Migration Hub for generating network diagrams.
Question 424 of 529
A company runs a software-as-a-service (SaaS) application on AWS. The application consists of AWS Lambda functions and an Amazon RDS for MySQL Multi-AZ database. During market events, the application has a much higher workload than normal. Users notice slow response times during the peak periods because of many database connections. The company needs to improve the scalable performance and availability of the database.
Which solution meets these requirements?
A.
Create an Amazon CloudWatch alarm action that triggers a Lambda function to add an Amazon RDS for MySQL read replica when resource utilization hits a threshold.
B.
Migrate the database to Amazon Aurora, and add a read replica. Add a database connection pool outside of the Lambda handler function.
C.
Migrate the database to Amazon Aurora, and add a read replica. Use Amazon Route 53 weighted records.
D.
Migrate the database to Amazon Aurora, and add an Aurora Replica. Configure Amazon RDS Proxy to manage database connection pools.
AnswerDiscussion
Correct Answer: D
Migrating the database to Amazon Aurora and adding an Aurora Replica will improve the scalability and availability of the database by providing built-in replication. Configuring Amazon RDS Proxy will manage database connection pools effectively, preventing Lambda functions from opening too many database connections and reducing connection overhead. This combination addresses both the performance and scaling issues caused by peak workload periods, ensuring the database can handle increased demand with better response times.
Question 425 of 529
A company is planning to migrate an application from on premises to the AWS Cloud. The company will begin the migration by moving the application’s underlying data storage to AWS. The application data is stored on a shared file system on premises, and the application servers connect to the shared file system through SMB.
A solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage. Until the application is fully migrated and code is rewritten to use native Amazon S3 APIs, the application must continue to have access to the data through SMB. The solutions architect must migrate the application data to AWS to its new location while still allowing the on-premises application to access the data.
Which solution will meet these requirements?
A.
Create a new Amazon FSx for Windows File Server file system. Configure AWS DataSync with one location for the on-premises file share and one location for the new Amazon FSx file system. Create a new DataSync task to copy the data from the on-premises file share location to the Amazon FSx file system.
B.
Create an S3 bucket for the application. Copy the data from the on-premises storage to the S3 bucket.
C.
Deploy an AWS Server Migration Service (AWS SMS) VM to the on-premises environment. Use AWS SMS to migrate the file storage server from on premises to an Amazon EC2 instance.
D.
Create an S3 bucket for the application. Deploy a new AWS Storage Gateway file gateway on an on-premises VM. Create a new file share that stores data in the S3 bucket and is associated with the file gateway. Copy the data from the on-premises storage to the new file gateway endpoint.
AnswerDiscussion
Correct Answer: D
To facilitate the migration while ensuring the application continues to access data through SMB, deploying an AWS Storage Gateway file gateway is the best solution. This allows the application to interact with the S3 bucket as if it were a traditional SMB file system. The file gateway will handle the translation between SMB and the S3 APIs, ensuring seamless access to the data during the migration phase. This approach meets the requirement of using an Amazon S3 bucket for shared storage while maintaining SMB access. Copying the data to the new file gateway endpoint ensures the application can continue to function without major changes until it is fully migrated and refactored. Therefore, creating an S3 bucket and deploying a file gateway on premises is the correct approach.
Question 426 of 529
A global company has a mobile app that displays ticket barcodes. Customers use the tickets on the mobile app to attend live events. Event scanners read the ticket barcodes and call a backend API to validate the barcode data against data in a database. After the barcode is scanned, the backend logic writes to the database's single table to mark the barcode as used.
The company needs to deploy the app on AWS with a DNS name of api.example.com. The company will host the database in three AWS Regions around the world.
Which solution will meet these requirements with the LOWEST latency?
A.
Host the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Container Service (Amazon ECS) clusters that are in the same Regions as the database. Create an accelerator in AWS Global Accelerator to route requests to the nearest ECS cluster. Create an Amazon Route 53 record that maps api.example.com to the accelerator endpoint
B.
Host the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Kubernetes Service (Amazon EKS) clusters that are in the same Regions as the database. Create an Amazon CloudFront distribution with the three clusters as origins. Route requests to the nearest EKS cluster. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution.
C.
Host the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a CloudFront function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution.
D.
Host the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a Lambda@Edge function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution.
AnswerDiscussion
Correct Answer: D
To achieve the lowest latency, the best approach is to use Amazon DynamoDB global tables for the database and Amazon CloudFront with Lambda@Edge for the backend logic. DynamoDB global tables provide multi-region replication, ensuring that data access is fast from any region. Using Lambda@Edge allows you to run the backend logic at the edge locations closest to the users, drastically reducing latency by avoiding the need to route requests to a central server. This setup ensures the barcode validation happens as close to the scanner as possible, providing the lowest latency solution.
Question 427 of 529
A medical company is running a REST API on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group behind an Application Load Balancer (ALB). The ALB runs in three public subnets, and the EC2 instances run in three private subnets. The company has deployed an Amazon CloudFront distribution that has the ALB as the only origin.
Which solution should a solutions architect recommend to enhance the origin security?
A.
Store a random string in AWS Secrets Manager. Create an AWS Lambda function for automatic secret rotation. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Create an AWS WAF web ACL rule with a string match rule for the custom header. Associate the web ACL with the ALB.
B.
Create an AWS WAF web ACL rule with an IP match condition of the CloudFront service IP address ranges. Associate the web ACL with the ALMove the ALB into the three private subnets.
C.
Store a random string in AWS Systems Manager Parameter Store. Configure Parameter Store automatic rotation for the string. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Inspect the value of the custom HTTP header, and block access in the ALB.
D.
Configure AWS Shield Advanced Create a security group policy to allow connections from CloudFront service IP address ranges. Add the policy to AWS Shield Advanced, and attach the policy to the ALB.
AnswerDiscussion
Correct Answer: A
To enhance the origin security for the company's REST API running on Amazon EC2 instances behind an Application Load Balancer (ALB), it is essential to ensure that only requests routed through CloudFront can reach the ALB. One effective solution involves using a combination of AWS services to achieve this goal. By storing a random string in AWS Secrets Manager and using AWS Lambda for automatic secret rotation, you can create a custom HTTP header in CloudFront that includes this random string. You then create an AWS WAF web ACL rule to match this custom header, ensuring that only requests containing the correct header value are allowed to reach the ALB. This approach restricts access to the origin and enhances security. Therefore, storing the random string in AWS Secrets Manager, configuring CloudFront to inject it as a custom HTTP header, and utilizing AWS WAF to enforce this in the ALB is the recommended solution.
Question 428 of 529
To abide by industry regulations, a solutions architect must design a solution that will store a company's critical data in multiple public AWS Regions, including in the United States, where the company's headquarters is located. The solutions architect is required to provide access to the data stored in AWS to the company’s global WAN network. The security team mandates that no traffic accessing this data should traverse the public internet.
How should the solutions architect design a highly available solution that meets the requirements and is cost-effective?
A.
Establish AWS Direct Connect connections from the company headquarters to all AWS Regions in use. Use the company WAN to send traffic over to the headquarters and then to the respective DX connection to access the data.
B.
Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use inter-region VPC peering to access the data in other AWS Regions.
C.
Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use an AWS transit VPC solution to access data in other AWS Regions.
D.
Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use Direct Connect Gateway to access data in other AWS Regions.
AnswerDiscussion
Correct Answer: D
To ensure the company's critical data is accessible in multiple public AWS Regions, including the United States, and that no traffic traverses the public internet, the solution needs to be highly available and cost-effective. Establishing two AWS Direct Connect connections to one AWS Region ensures redundancy. Using Direct Connect Gateway allows secure access to data across multiple AWS Regions without the complexity and added cost of managing transit VPCs or inter-region VPC peering. Direct Connect Gateway provides a scalable and centralized way to manage connections to multiple regions efficiently.
Question 429 of 529
A company has developed an application that is running Windows Server on VMware vSphere VMs that the company hosts on premises. The application data is stored in a proprietary format that must be read through the application. The company manually provisioned the servers and the application.
As part of its disaster recovery plan, the company wants the ability to host its application on AWS temporarily if the company's on-premises environment becomes unavailable. The company wants the application to return to on-premises hosting after a disaster recovery event is complete. The RPO is 5 minutes.
Which solution meets these requirements with the LEAST amount of operational overhead?
A.
Configure AWS DataSync. Replicate the data to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and attach the EBS volumes.
B.
Configure AWS Elastic Disaster Recovery. Replicate the data to replication Amazon EC2 instances that are attached to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use Elastic Disaster Recovery to launch EC2 instances that use the replicated volumes.
C.
Provision an AWS Storage Gateway file gateway. Replicate the data to an Amazon S3 bucket. When the on-premises environment is unavailable, use AWS Backup to restore the data to Amazon Elastic Block Store (Amazon EBS) volumes and launch Amazon EC2 instances from these EBS volumes.
D.
Provision an Amazon FSx for Windows File Server file system on AWS. Replicate the data to the file system. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and use AWS::CloudFormation::Init commands to mount the Amazon FSx file shares.
AnswerDiscussion
Correct Answer: B
The best solution for the company's disaster recovery requirements is to use AWS Elastic Disaster Recovery. This service minimizes operational overhead by continuously replicating the data from the on-premises environment to AWS, ensuring that the RPO (Recovery Point Objective) of 5 minutes is met. When the on-premises environment becomes unavailable, the company can use Elastic Disaster Recovery to quickly launch Amazon EC2 instances that use the replicated Amazon Elastic Block Store (EBS) volumes. This setup allows for a seamless transition to AWS during a disaster and an easy migration back to the on-premises environment once the disaster recovery event is resolved.
Question 430 of 529
A company runs a highly available data collection application on Amazon EC2 in the eu-north-1 Region. The application collects data from end-user devices and writes records to an Amazon Kinesis data stream and a set of AWS Lambda functions that process the records. The company persists the output of the record processing to an Amazon S3 bucket in eu-north-1. The company uses the data in the S3 bucket as a data source for Amazon Athena.
The company wants to increase its global presence. A solutions architect must launch the data collection capabilities in the sa-east-1 and ap-northeast-1 Regions. The solutions architect deploys the application, the Kinesis data stream, and the Lambda functions in the two new Regions. The solutions architect keeps the S3 bucket in eu-north-1 to meet a requirement to centralize the data analysis.
During testing of the new setup, the solutions architect notices a significant lag on the arrival of data from the new Regions to the S3 bucket.
Which solution will improve this lag time the MOST?
A.
In each of the two new Regions, set up the Lambda functions to run in a VPC. Set up an S3 gateway endpoint in that VPC.
B.
Turn on S3 Transfer Acceleration on the S3 bucket in eu-north-1. Change the application to use the new S3 accelerated endpoint when the application uploads data to the S3 bucket.
C.
Create an S3 bucket in each of the two new Regions. Set the application in each new Region to upload to its respective S3 bucket. Set up S3 Cross-Region Replication to replicate data to the S3 bucket in eu-north-1.
D.
Increase the memory requirements of the Lambda functions to ensure that they have multiple cores available. Use the multipart upload feature when the application uploads data to Amazon S3 from Lambda.
AnswerDiscussion
Correct Answer: C
The best solution to improve the lag time is to create an S3 bucket in each of the two new Regions and set the application in each region to upload to its respective S3 bucket. Then, set up S3 Cross-Region Replication to replicate data to the S3 bucket in eu-north-1. This approach ensures that data is first uploaded to a bucket within the same region, reducing the initial transfer time, and then uses AWS's optimized cross-region replication to centralize the data in eu-north-1. This method effectively leverages AWS's infrastructure to minimize latency while meeting the requirement to centralize data analysis in the eu-north-1 region.
Question 431 of 529
A company provides a centralized Amazon EC2 application hosted in a single shared VPC. The centralized application must be accessible from client applications running in the VPCs of other business units. The centralized application front end is configured with a Network Load Balancer (NLB) for scalability.
Up to 10 business unit VPCs will need to be connected to the shared VPC. Some of the business unit VPC CIDR blocks overlap with the shared VPC, and some overlap with each other Network connectivity to the centralized application in the shared VPC should be allowed from authorized business unit VPCs only.
Which network configuration should a solutions architect use to provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC?
A.
Create an AWS Transit Gateway. Attach the shared VPC and the authorized business unit VPCs to the transit gateway. Create a single transit gateway route table and associate it with all of the attached VPCs. Allow automatic propagation of routes from the attachments into the route table. Configure VPC routing tables to send traffic to the transit gateway.
B.
Create a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service. Accept authorized endpoint requests from the endpoint service console.
C.
Create a VPC peering connection from each business unit VPC to the shared VPAccept the VPC peering connections from the shared VPC console. Configure VPC routing tables to send traffic to the VPC peering connection.
D.
Configure a virtual private gateway for the shared VPC and create customer gateways for each of the authorized business unit VPCs. Establish a Site-to-Site VPN connection from the business unit VPCs to the shared VPC. Configure VPC routing tables to send traffic to the VPN connection.
AnswerDiscussion
Correct Answer: B
To provide connectivity from client applications in the business unit VPCs to the centralized application in the shared VPC, especially when the CIDR blocks of the VPCs overlap, the best solution is to create a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Then, create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service, and accept authorized endpoint requests from the endpoint service console. This configuration allows for secure and controlled access to the centralized application despite the overlapping IP addresses.
Question 432 of 529
A company wants to migrate its website to AWS. The website uses microservices and runs on containers that are deployed in an on-premises, self-managed Kubernetes cluster. All the manifests that define the deployments for the containers in the Kubernetes deployment are in source control.
All data for the website is stored in a PostgreSQL database. An open source container image repository runs alongside the on-premises environment.
A solutions architect needs to determine the architecture that the company will use for the website on AWS.
Which solution will meet these requirements with the LEAST effort to migrate?
A.
Create an AWS App Runner service. Connect the App Runner service to the open source container image repository. Deploy the manifests from on premises to the App Runner service. Create an Amazon RDS for PostgreSQL database.
B.
Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the manifests from on premises to the EKS cluster. Create an Amazon Aurora PostgreSQL DB cluster.
C.
Create an Amazon Elastic Container Service (Amazon ECS) cluster that has an Amazon EC2 capacity pool. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Register each container image as a new task definition. Configure ECS services for each task definition to match the original Kubernetes deployments. Create an Amazon Aurora PostgreSQL DB cluster.
D.
Rebuild the on-premises Kubernetes cluster by hosting the cluster on Amazon EC2 instances. Migrate the open source container image repository to the EC2 instances. Deploy the manifests from on premises to the new cluster on AWS. Deploy an open source PostgreSQL database on the new cluster.
AnswerDiscussion
Correct Answer: B
The best solution is to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups, copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository, deploy the manifests from on premises to the EKS cluster, and create an Amazon Aurora PostgreSQL DB cluster. This option is optimal because it involves the least effort in terms of migration: the existing manifests can largely be reused with minimal modifications, allowing for a smoother transition to a managed service. Using EKS also provides familiar Kubernetes management, which aligns closely with the current on-premises setup, reducing the need for extensive reconfiguration or retraining. Additionally, Amazon Aurora PostgreSQL is a managed database service that would simplify database operations and maintenance. Other options require more significant changes either in terms of deployment configuration or involve managing the infrastructure more manually, increasing complexity and effort. Therefore, option B is the most suitable with the least effort required.
Question 433 of 529
A company uses a mobile app on AWS to run online contests. The company selects a winner at random at the end of each contest. The contests run for variable lengths of time. The company does not need to retain any data from a contest after the contest is finished.
The company uses custom code that is hosted on Amazon EC2 instances to process the contest data and select a winner. The EC2 instances run behind an Application Load Balancer and store contest entries on Amazon RDS DB instances. The company must design a new architecture to reduce the cost of running the contests.
Which solution will meet these requirements MOST cost-effectively?
A.
Migrate storage of the contest entries to Amazon DynamoDB. Create a DynamoDB Accelerator (DAX) cluster. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. At the end of the contest, delete the DynamoDB table.
B.
Migrate the storage of the contest entries to Amazon Redshift. Rewrite the code as AWS Lambda functions. At the end of the contest, delete the Redshift cluster.
C.
Add an Amazon ElastiCache for Redis cluster in front of the RDS DB instances to cache the contest entries. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. Set the ElastiCache TTL attribute on each entry to expire each entry at the end of the contest.
D.
Migrate the storage of the contest entries to Amazon DynamoDB. Rewrite the code as AWS Lambda functions. Set the DynamoDB TTL attribute on each entry to expire each entry at the end of the contest.
AnswerDiscussion
Correct Answer: D
To achieve a cost-effective solution, migrate the storage to Amazon DynamoDB due to its scalability and efficient automatic data expiration when TTL is set on each entry to expire at the end of the contest. Rewrite the code as AWS Lambda functions to execute the necessary operations such as adding/fetching entries and selecting a winner. Since the contests run for variable lengths of time, manually setting the TTL for each contest ensures that the data automatically expires when the contest concludes, avoiding the need to retain or manually delete data and minimizing costs by using resources only when necessary.
Question 434 of 529
A company has implemented a new security requirement. According to the new requirement, the company must scan all traffic from corporate AWS instances in the company's VPC for violations of the company's security policies. As a result of these scans, the company can block access to and from specific IP addresses.
To meet the new requirement, the company deploys a set of Amazon EC2 instances in private subnets to serve as transparent proxies. The company installs approved proxy server software on these EC2 instances. The company modifies the route tables on all subnets to use the corresponding EC2 instances with proxy software as the default route. The company also creates security groups that are compliant with the security policies and assigns these security groups to the EC2 instances.
Despite these configurations, the traffic of the EC2 instances in their private subnets is not being properly forwarded to the internet.
What should a solutions architect do to resolve this issue?
A.
Disable source/destination checks on the EC2 instances that run the proxy software.
B.
Add a rule to the security group that is assigned to the proxy EC2 instances to allow all traffic between instances that have this security group. Assign this security group to all EC2 instances in the VPC.
C.
Change the VPCs DHCP options set. Set the DNS server options to point to the addresses of the proxy EC2 instances.
D.
Assign one additional elastic network interface to each proxy EC2 instance. Ensure that one of these network interfaces has a route to the private subnets. Ensure that the other network interface has a route to the internet.
AnswerDiscussion
Correct Answer: A
The issue is likely due to source/destination checks being enabled on the EC2 instances acting as transparent proxies. By default, EC2 instances are not designed to forward traffic; they are intended to be the endpoint. Disabling source/destination checks on the proxy instances allows them to forward traffic properly, which is necessary for them to function as proxies and route traffic as intended.
Question 435 of 529
A company is running its solution on AWS in a manually created VPC. The company is using AWS CloudFormation to provision other parts of the infrastructure. According to a new requirement, the company must manage all infrastructure in an automatic way.
What should the company do to meet this new requirement with the LEAST effort?
A.
Create a new AWS Cloud Development Kit (AWS CDK) stack that strictly provisions the existing VPC resources and configuration. Use AWS CDK to import the VPC into the stack and to manage the VPC.
B.
Create a CloudFormation stack set that creates the VPC. Use the stack set to import the VPC into the stack.
C.
Create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the Existing resources.
D.
Create a new CloudFormation template that creates the VPC. Use the AWS Serverless Application Model (AWS SAM) CLI to import the VPC.
AnswerDiscussion
Correct Answer: C
To manage all infrastructure automatically with the least effort, the company should create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, the company can then create a new stack by importing the existing resources. This option allows the company to integrate the VPC into CloudFormation without needing to recreate it, ensuring an automated management process while leveraging the existing CloudFormation setup.
Question 436 of 529
A company has developed a new release of a popular video game and wants to make it available for public download. The new release package is approximately 5 GB in size. The company provides downloads for existing releases from a Linux-based, publicly facing FTP site hosted in an on-premises data center. The company expects the new release will be downloaded by users worldwide. The company wants a solution that provides improved download performance and low transfer costs, regardless of a user's location.
A.
Store the game files on Amazon EBS volumes mounted on Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package.
B.
Store the game files on Amazon EFS volumes that are attached to Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on each of the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package.
C.
Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Use Amazon CloudFront for the website. Publish the game download URL for users to download the package.
D.
Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Set Requester Pays for the S3 bucket. Publish the game download URL for users to download the package.
AnswerDiscussion
Correct Answer: C
To improve download performance and provide low transfer costs for a global user base, it's best to use Amazon S3 for storing the game files because it is highly scalable and cost-effective. Using Amazon CloudFront as a Content Delivery Network (CDN) in front of the S3 bucket ensures that the game files are cached and distributed to users worldwide, minimizing latency and improving download speeds. Amazon Route 53 can be used for DNS management to route users to the closest CloudFront edge location. This setup meets the requirements for improved performance and low transfer costs for users across different locations.
Question 437 of 529
A company runs an application in the cloud that consists of a database and a website. Users can post data to the website, have the data processed, and have the data sent back to them in an email. Data is stored in a MySQL database running on an Amazon EC2 instance. The database is running in a VPC with two private subnets. The website is running on Apache Tomcat in a single EC2 instance in a different VPC with one public subnet. There is a single VPC peering connection between the database and website VPC.
The website has suffered several outages during the last month due to high traffic.
Which actions should a solutions architect take to increase the reliability of the application? (Choose three.)
A.
Place the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an Application Load Balancer.
B.
Provision an additional VPC peering connection.
C.
Migrate the MySQL database to Amazon Aurora with one Aurora Replica.
D.
Provision two NAT gateways in the database VPC.
E.
Move the Tomcat server to the database VPC.
F.
Create an additional public subnet in a different Availability Zone in the website VPC.
AnswerDiscussion
Correct Answer: A, C, F
To enhance the reliability of the application, several strategies should be implemented. Placing the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an Application Load Balancer helps to handle high traffic and ensures availability through load balancing and auto-scaling. Migrating the MySQL database to Amazon Aurora with one Aurora Replica provides better performance and high availability since Aurora is designed for cloud environments with automated backups and failover capabilities. Creating an additional public subnet in a different Availability Zone in the website VPC ensures higher availability and fault tolerance by spreading the infrastructure across multiple Availability Zones, reducing the risk of downtime from a single point of failure.
Question 438 of 529
A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.
After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.
While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.
Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)
A.
Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.
B.
Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.
C.
Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.
D.
Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.
E.
Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.
AnswerDiscussion
Correct Answer: A, E
To meet the requirement of providing a custom error page with the least amount of operational overhead, the best solution involves steps that leverage existing AWS services with minimal configuration changes. First, create an Amazon S3 bucket to host the static custom error page, as S3 is a cost-effective and straightforward solution to serve static content (A). Second, configure CloudFront to use a custom error response, which allows handling of specific HTTP error codes and returning a custom error page stored in the S3 bucket (E). This approach uses CloudFront's capabilities to handle errors and avoids the need for complex health check configurations or Lambda functions to modify load balancer rules, reducing operational overhead.
Question 439 of 529
A company wants to migrate an Amazon Aurora MySQL DB cluster from an existing AWS account to a new AWS account in the same AWS Region. Both accounts are members of the same organization in AWS Organizations.
The company must minimize database service interruption before the company performs DNS cutover to the new database.
Which migration strategy will meet this requirement? (Choose two.)
A.
Take a snapshot of the existing Aurora database. Share the snapshot with the new AWS account. Create an Aurora DB cluster in the new account from the snapshot.
B.
Create an Aurora DB cluster in the new AWS account. Use AWS Database Migration Service (AWS DMS) to migrate data between the two Aurora DB clusters.
C.
Use AWS Backup to share an Aurora database backup from the existing AWS account to the new AWS account. Create an Aurora DB cluster in the new AWS account from the snapshot.
D.
Create an Aurora DB cluster in the new AWS account. Use AWS Application Migration Service to migrate data between the two Aurora DB clusters.
AnswerDiscussion
Correct Answer: A, B
To migrate an Amazon Aurora MySQL DB cluster from one AWS account to another with minimal service interruption, two effective strategies can be employed. First, taking a snapshot of the existing Aurora database and sharing it with the new AWS account allows you to create an Aurora DB cluster in the new account from this snapshot. This method provides a direct and reliable way to transfer the database's data entirely. Secondly, setting up an Aurora DB cluster in the new AWS account and using AWS Database Migration Service (AWS DMS) to migrate data between the two clusters ensures that data synchronization is maintained before the DNS cutover, thus minimizing downtime and ensuring data continuity.
Question 440 of 529
A software as a service (SaaS) company provides a media software solution to customers. The solution is hosted on 50 VPCs across various AWS Regions and AWS accounts. One of the VPCs is designated as a management VPC. The compute resources in the VPCs work independently.
The company has developed a new feature that requires all 50 VPCs to be able to communicate with each other. The new feature also requires one-way access from each customer's VPC to the company's management VPC. The management VPC hosts a compute resource that validates licenses for the media software solution.
The number of VPCs that the company will use to host the solution will continue to increase as the solution grows.
Which combination of steps will provide the required VPC connectivity with the LEAST operational overhead? (Choose two.)
A.
Create a transit gateway. Attach all the company's VPCs and relevant subnets to the transit gateway.
B.
Create VPC peering connections between all the company's VPCs.
C.
Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer's VPAssociate the endpoint service with the NLB.
D.
Create a VPN appliance in each customer's VPC. Connect the company's management VPC to each customer's VPC by using AWS Site-to-Site VPN.
E.
Create a VPC peering connection between the company's management VPC and each customer's VPC.
AnswerDiscussion
Correct Answer: A, C
Creating a transit gateway and attaching all the VPCs and relevant subnets to it offers a scalable solution to manage the communication among increasing numbers of VPCs with minimal operational overhead. The transit gateway facilitates centralized management of VPC attachments and routing, making it easier to handle a growing number of VPCs across various AWS regions and accounts. Secondly, creating a Network Load Balancer (NLB) for the compute resource in the management VPC and associating it with an AWS PrivateLink endpoint service ensures secure, one-way access from each customer's VPC to the management VPC. This method leverages AWS PrivateLink to maintain security while providing the necessary communication without the complexity and operational burden associated with managing numerous VPC peering connections or VPN setups.
Question 441 of 529
A company has multiple lines of business (LOBs) that roll up to the parent company. The company has asked its solutions architect to develop a solution with the following requirements:
• Produce a single AWS invoice for all of the AWS accounts used by its LOBs.
• The costs for each LOB account should be broken out on the invoice.
• Provide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.
• Each LOB account should be delegated full administrator permissions, regardless of the governance policy.
Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)
A.
Use AWS Organizations to create an organization in the parent account for each LOB. Then invite each LOB account to the appropriate organization.
B.
Use AWS Organizations to create a single organization in the parent account. Then, invite each LOB's AWS account to join the organization.
C.
Implement service quotas to define the services and features that are permitted and apply the quotas to each LOB. as appropriate.
D.
Create an SCP that allows only approved services and features, then apply the policy to the LOB accounts.
E.
Enable consolidated billing in the parent account's billing console and link the LOB accounts.
AnswerDiscussion
Correct Answer: B, E
Option B is necessary because using AWS Organizations to create a single organization in the parent account and inviting each LOB's AWS account to join the organization will centralize management and billing. Option E is essential because enabling consolidated billing in the parent account's billing console and linking the LOB accounts will ensure that a single AWS invoice is produced for all accounts, with costs broken out for each LOB account. These steps meet the requirements of producing a single invoice with detailed cost breakdowns and providing governance while allowing each LOB account full administrative permissions.
Question 442 of 529
A solutions architect has deployed a web application that serves users across two AWS Regions under a custom domain. The application uses Amazon Route 53 latency-based routing. The solutions architect has associated weighted record sets with a pair of web servers in separate Availability Zones for each Region.
The solutions architect runs a disaster recovery scenario. When all the web servers in one Region are stopped, Route 53 does not automatically redirect users to the other Region.
Which of the following are possible root causes of this issue? (Choose two.)
A.
The weight for the Region where the web servers were stopped is higher than the weight for the other Region.
B.
One of the web servers in the secondary Region did not pass its HTTP health check.
C.
Latency resource record sets cannot be used in combination with weighted resource record sets.
D.
The setting to evaluate target health is not turned on for the latency alias resource record set that is associated with the domain in the Region where the web servers were stopped.
E.
An HTTP health check has not been set up for one or more of the weighted resource record sets associated with the stopped web servers.
AnswerDiscussion
Correct Answer: D, E
Latency resource record sets can be used in combination with other routing policies like weighted resource record sets. However, for Route 53 to perform failover correctly when using latency-based routing, health checks must be set up and the 'evaluate target health' setting must be enabled. Without health checks, Route 53 won't know if a web server is down and won't redirect traffic accordingly. This suggests that the probable root causes are not having the 'evaluate target health' setting turned on and not having HTTP health checks set up for the weighted resource record sets.
Question 443 of 529
A flood monitoring agency has deployed more than 10,000 water-level monitoring sensors. Sensors send continuous data updates, and each update is less than 1 MB in size. The agency has a fleet of on-premises application servers. These servers receive updates from the sensors, convert the raw data into a human readable format, and write the results to an on-premises relational database server. Data analysts then use simple SQL queries to monitor the data.
The agency wants to increase overall application availability and reduce the effort that is required to perform maintenance tasks. These maintenance tasks, which include updates and patches to the application servers, cause downtime. While an application server is down, data is lost from sensors because the remaining servers cannot handle the entire workload.
The agency wants a solution that optimizes operational overhead and costs. A solutions architect recommends the use of AWS IoT Core to collect the sensor data.
What else should the solutions architect recommend to meet these requirements?
A.
Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to .csv format, and insert it into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance.
B.
Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to Apache Parquet format, and save it to an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena.
C.
Send the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to .csv format and store it in an Amazon S3 bucket. Import the data into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance.
D.
Send the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to Apache Parquet format and store it in an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena.
AnswerDiscussion
Correct Answer: B
To increase overall application availability and reduce maintenance tasks while handling continuous data from more than 10,000 water-level monitoring sensors efficiently, the recommended approach is to use Amazon Kinesis Data Firehose to ingest and process the streaming data. Using an AWS Lambda function to transform the data to Apache Parquet format and storing it in an Amazon S3 bucket offers several advantages. Apache Parquet is a columnar storage format optimized for analytics workloads, which provides efficient compression and query performance. Data analysts can then use Amazon Athena to query the data directly from S3. This solution minimizes operational overhead and ensures high availability by leveraging managed AWS services that automatically scale and handle the incoming workload without downtime.
Question 444 of 529
A public retail web application uses an Application Load Balancer (ALB) in front of Amazon EC2 instances running across multiple Availability Zones (AZs) in a Region backed by an Amazon RDS MySQL Multi-AZ deployment. Target group health checks are configured to use HTTP and pointed at the product catalog page. Auto Scaling is configured to maintain the web fleet size based on the ALB health check.
Recently, the application experienced an outage. Auto Scaling continuously replaced the instances during the outage. A subsequent investigation determined that the web server metrics were within the normal range, but the database tier was experiencing high load, resulting in severely elevated query response times.
Which of the following changes together would remediate these issues while improving monitoring capabilities for the availability and functionality of the entire application stack for future growth? (Choose two.)
A.
Configure read replicas for Amazon RDS MySQL and use the single reader endpoint in the web application to reduce the load on the backend database tier.
B.
Configure the target group health check to point at a simple HTML page instead of a product catalog page and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.
C.
Configure the target group health check to use a TCP check of the Amazon EC2 web server and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.
D.
Configure an Amazon CloudWatch alarm for Amazon RDS with an action to recover a high-load, impaired RDS instance in the database tier.
E.
Configure an Amazon ElastiCache cluster and place it between the web application and RDS MySQL instances to reduce the load on the backend database tier.
AnswerDiscussion
Correct Answer: A, E
To remediate the issue and improve monitoring capabilities, two critical changes should be implemented. First, configure read replicas for Amazon RDS MySQL and use the single reader endpoint in the web application to reduce the load on the backend database tier. This ensures that read operations are distributed among replicas, alleviating the strain on the primary database instance during high load scenarios. Second, placing an Amazon ElastiCache cluster between the web application and RDS MySQL instances can also reduce the load on the backend database tier by caching frequently accessed data. This improves response times and overall application performance, thus addressing the elevated query response times experienced during high load periods.
Question 445 of 529
A company has an on-premises data center and is using Kubernetes to develop a new solution on AWS. The company uses Amazon Elastic Kubernetes Service (Amazon EKS) clusters for its development and test environments.
The EKS control plane and data plane for production workloads must reside on premises. The company needs an AWS managed solution for Kubernetes management.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using a local cluster configuration on the Outposts server for the production workloads.
B.
Install Amazon EKS Anywhere on the company's hardware in the on-premises data center. Deploy the production workloads on an EKS Anywhere cluster.
C.
Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using an extended cluster configuration on the Outposts server for the production workloads.
D.
Install an AWS Outposts server in the on-premises data center. Install Amazon EKS Anywhere on the Outposts server. Deploy the production workloads on an EKS Anywhere cluster.
AnswerDiscussion
Correct Answer: A
The requirement is to have both the control plane and data plane for production workloads on premises with AWS-managed Kubernetes management. Amazon EKS on Outposts with a local cluster configuration meets these criteria, as the control plane and data plane reside on the Outposts server and are AWS-managed. In contrast, EKS Anywhere is a customer-managed product, which does not fulfill the requirement for AWS management. Thus, installing an AWS Outposts server in the on-premises data center and deploying Amazon EKS using a local cluster configuration is the correct solution.
Question 446 of 529
A company uses AWS Organizations to manage its development environment. Each development team at the company has its own AWS account. Each account has a single VPC and CIDR blocks that do not overlap.
The company has an Amazon Aurora DB cluster in a shared services account. All the development teams need to work with live data from the DB cluster.
Which solution will provide the required connectivity to the DB cluster with the LEAST operational overhead?
A.
Create an AWS Resource Access Manager (AWS RAM) resource share for the DB cluster. Share the DB cluster with all the development accounts.
B.
Create a transit gateway in the shared services account. Create an AWS Resource Access Manager (AWS RAM) resource share for the transit gateway. Share the transit gateway with all the development accounts. Instruct the developers to accept the resource share. Configure networking.
C.
Create an Application Load Balancer (ALB) that points to the IP address of the DB cluster. Create an AWS PrivateLink endpoint service that uses the ALB. Add permissions to allow each development account to connect to the endpoint service.
D.
Create an AWS Site-to-Site VPN connection in the shared services account. Configure networking. Use AWS Marketplace VPN software in each development account to connect to the Site-to-Site VPN connection.
AnswerDiscussion
Correct Answer: B
The solution that provides connectivity to the Amazon Aurora DB cluster with the least operational overhead is creating a transit gateway in the shared services account and creating an AWS Resource Access Manager (AWS RAM) resource share for the transit gateway. Sharing the transit gateway with all the development accounts and configuring networking provides a centralized and straightforward approach that allows all teams to access the live data from the DB cluster. This avoids the complexities and limitations of other options, such as cloning with AWS RAM or the requirement for a Network Load Balancer with PrivateLink.
Question 447 of 529
A company used AWS CloudFormation to create all new infrastructure in its AWS member accounts. The resources rarely change and are properly sized for the expected load. The monthly AWS bill is consistent.
Occasionally, a developer creates a new resource for testing and forgets to remove the resource when the test is complete. Most of these tests last a few days before the resources are no longer needed.
The company wants to automate the process of finding unused resources. A solutions architect needs to design a solution that determines whether the cost in the AWS bill is increasing. The solution must help identify resources that cause an increase in cost and must automatically notify the company's operations team.
Which solution will meet these requirements?
A.
Turn on billing alerts. Use AWS Cost Explorer to determine the costs for the past month. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached.
B.
Turn on billing alerts. Use AWS Cost Explorer to determine the average monthly costs for the past 3 months. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached.
C.
Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of Linked account. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance.
D.
Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of AWS services. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance.
AnswerDiscussion
Correct Answer: D
To identify and manage unused resources that cause an increase in cost, it is essential to monitor spending at a granular level across individual AWS services. Using AWS Cost Anomaly Detection to create a cost monitor with a monitor type of AWS services allows for this detailed tracking. It can detect unusual spending patterns specific to different services, even within a single account, which helps in pinpointing exactly which resource is contributing to increased costs. Setting up daily cost summaries and specifying a threshold for cost variance ensures the operations team is promptly notified of any anomalies, allowing for timely corrective actions.
Question 448 of 529
A company is deploying a new web-based application and needs a storage solution for the Linux application servers. The company wants to create a single location for updates to application data for all instances. The active dataset will be up to 100 GB in size. A solutions architect has determined that peak operations will occur for 3 hours daily and will require a total of 225 MiBps of read throughput.
The solutions architect must design a Multi-AZ solution that makes a copy of the data available in another AWS Region for disaster recovery (DR). The DR copy has an RPO of less than 1 hour.
Which solution will meet these requirements?
A.
Deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. Configure the file system for 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region.
B.
Deploy a new Amazon FSx for Lustre file system. Configure Bursting Throughput mode for the file system. Use AWS Backup to back up the file system to the DR Region.
C.
Deploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume with 225 MiBps of throughput. Enable Multi-Attach for the EBS volume. Use AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region.
D.
Deploy an Amazon FSx for OpenZFS file system in both the production Region and the DR Region. Create an AWS DataSync scheduled task to replicate the data from the production file system to the DR file system every 10 minutes.
AnswerDiscussion
Correct Answer: A
The correct solution is to deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. EFS supports cross-region replication, meeting the requirement for a Multi-AZ solution with a disaster recovery copy in another region. EFS can accumulate bursting credits, allowing it to handle peak throughput requirements. With a provisioned throughput of 75 MiBps and the capability to burst up to 300 MiBps per 100 GB of storage, EFS can effectively manage the peak operational needs while maintaining a consistent and reliable RPO of less than 1 hour through its replication capabilities.
Question 449 of 529
A company needs to gather data from an experiment in a remote location that does not have internet connectivity. During the experiment, sensors that are connected to a local network will generate 6 TB of data in a proprietary format over the course of 1 week. The sensors can be configured to upload their data files to an FTP server periodically, but the sensors do not have their own FTP server. The sensors also do not support other protocols. The company needs to collect the data centrally and move the data to object storage in the AWS Cloud as soon as possible after the experiment.
Which solution will meet these requirements?
A.
Order an AWS Snowball Edge Compute Optimized device. Connect the device to the local network. Configure AWS DataSync with a target bucket name, and unload the data over NFS to the device. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3.
B.
Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Create a shell script that periodically downloads data from each sensor. After the experiment, return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume.
C.
Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Install and configure an FTP server on the EC2 instance. Configure the sensors to upload data to the EC2 instance. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3.
D.
Order an AWS Snowcone device. Connect the device to the local network. Configure the device to use Amazon FSx. Configure the sensors to upload data to the device. Configure AWS DataSync on the device to synchronize the uploaded data with an Amazon S3 bucket. Return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume.
AnswerDiscussion
Correct Answer: C
In this scenario, the sensors can only use the FTP protocol to upload their data. The best solution is to order an AWS Snowcone device and install and configure an FTP server on an EC2 instance running on the Snowcone. This setup allows the sensors to upload their data directly to the FTP server. Once the data is collected, the Snowcone device can be returned to AWS, and the data can be loaded into Amazon S3. This ensures that the data is gathered reliably and transported efficiently to the AWS Cloud.
Question 450 of 529
A company that has multiple business units is using AWS Organizations with all features enabled. The company has implemented an account structure in which each business unit has its own AWS account. Administrators in each AWS account need to view detailed cost and utilization data for their account by using Amazon Athena.
Each business unit can have access to only its own cost and utilization data. The IAM policies that govern the ability to set up AWS Cost and Usage Reports are in place. A central Cost and Usage Report that contains all data for the organization is already available in an Amazon S3 bucket.
Which solution will meet these requirements with the LEAST operational complexity?
A.
In the organization's management account, use AWS Resource Access Manager (AWS RAM) to share the Cost and Usage Report data with each member account.
B.
In the organization's management account, configure an S3 event to invoke an AWS Lambda function each time a new file arrives in the S3 bucket that contains the central Cost and Usage Report. Configure the Lambda function to extract each member account’s data and to place the data in Amazon S3 under a separate prefix. Modify the S3 bucket policy to allow each member account to access its own prefix.
C.
In each member account, access AWS Cost Explorer. Create a new report that contains relevant cost information for the account. Save the report in Cost Explorer. Provide instructions that the account administrators can use to access the saved report.
D.
In each member account, create a new S3 bucket to store Cost and Usage Report data. Set up a Cost and Usage Report to deliver the data to the new S3 bucket.
AnswerDiscussion
Correct Answer: D
The solution that meets the requirements with the least operational complexity is to have each member account create a new S3 bucket to store the Cost and Usage Report data. Setting up a separate Cost and Usage Report to deliver data to new S3 buckets in each account allows each business unit to have isolated access to only their data, without complex setups involving Lambda functions or cross-account sharing mechanisms. This simplifies the process by enabling independent management of Cost and Usage Reports within each account.
Question 451 of 529
A company is designing an AWS environment for a manufacturing application. The application has been successful with customers, and the application's user base has increased. The company has connected the AWS environment to the company's on-premises data center through a 1 Gbps AWS Direct Connect connection. The company has configured BGP for the connection.
The company must update the existing network connectivity solution to ensure that the solution is highly available, fault tolerant, and secure.
Which solution will meet these requirements MOST cost-effectively?
A.
Add a dynamic private IP AWS Site-to-Site VPN as a secondary path to secure data in transit and provide resilience for the Direct Connect connection. Configure MACsec to encrypt traffic inside the Direct Connect connection.
B.
Provision another Direct Connect connection between the company's on-premises data center and AWS to increase the transfer speed and provide resilience. Configure MACsec to encrypt traffic inside the Direct Connect connection.
C.
Configure multiple private VIFs. Load balance data across the VIFs between the on-premises data center and AWS to provide resilience.
D.
Add a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resilience for the Direct Connect connection.
AnswerDiscussion
Correct Answer: D
Adding a static AWS Site-to-Site VPN as a secondary path ensures high availability and fault tolerance for the Direct Connect connection. This approach is also secure as VPNs inherently encrypt data in transit. Moreover, it is cost-effective compared to setting up an additional Direct Connect connection. Configuring MACsec is not an option since MACsec is only supported on 10 Gbps and 100 Gbps Direct Connect links, and the company currently has a 1 Gbps connection.
Question 452 of 529
A company needs to modernize an application and migrate the application to AWS. The application stores user profile data as text in a single table in an on-premises MySQL database.
After the modernization, users will use the application to upload video files that are up to 4 GB in size. Other users must be able to download the video files from the application. The company needs a video storage solution that provides rapid scaling. The solution must not affect application performance.
Which solution will meet these requirements?
A.
Migrate the database to Amazon Aurora PostgreSQL by using AWS Database Migration Service (AWS DMS). Store the videos as base64-encoded strings in a TEXT column in the database.
B.
Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 key in the corresponding DynamoDB item.
C.
Migrate the database to Amazon Keyspaces (for Apache Cassandra) by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 object identifier in the corresponding Amazon Keyspaces entry.
D.
Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as base64-encoded strings in the corresponding DynamoDB item.
AnswerDiscussion
Correct Answer: B
To meet the requirements of modernizing the application, migrating to AWS, and handling video files up to 4 GB in size, the best solution is to store the videos as objects in Amazon S3. S3 provides scalable storage that can handle large files efficiently and cost-effectively without impacting application performance. Using DynamoDB to store metadata, including the S3 keys, ensures quick access and management of the videos. This approach separates the video storage from the database, preventing performance issues associated with storing large binary data directly within the database.
Question 453 of 529
A company stores and manages documents in an Amazon Elastic File System (Amazon EFS) file system. The file system is encrypted with an AWS Key Management Service (AWS KMS) key. The file system is mounted to an Amazon EC2 instance that runs proprietary software.
The company has enabled automatic backups for the file system. The automatic backups use the AWS Backup default backup plan.
A solutions architect must ensure that deleted documents can be recovered within an RPO of 100 minutes.
Which solution will meet these requirements?
A.
Create a new IAM role. Create a new backup plan. Use the new IAM role to create backups. Update the KMS key policy to allow the new IAM role to use the key. Implement an hourly backup schedule for the file system.
B.
Create a new backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Implement a custom cron expression to run a backup of the file system every 30 minutes.
C.
Create a new IAM role. Use the existing backup plan. Update the KMS key policy to allow the new IAM role to use the key. Enable continuous backups for point-in-time recovery.
D.
Use the existing backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Enable Cross-Region Replication for the file system.
AnswerDiscussion
Correct Answer: A
To ensure that deleted documents can be recovered within an RPO of 100 minutes, it is necessary to have a frequent backup schedule. Creating a new IAM role and a new backup plan ensures that the proper permissions are in place. Updating the KMS key policy allows the new IAM role to use the key, facilitating the encrypted backups. Implementing an hourly backup schedule meets the requirement of recovering data within 100 minutes, as hourly backups will ensure that no data is older than 60 minutes at worst. This approach provides a balance between frequency and managing operational overhead without requiring non-standard solutions like custom cron tasks.
Question 454 of 529
A solutions architect must provide a secure way for a team of cloud engineers to use the AWS CLI to upload objects into an Amazon S3 bucket. Each cloud engineer has an IAM user, IAM access keys, and a virtual multi-factor authentication (MFA) device. The IAM users for the cloud engineers are in a group that is named S3-access. The cloud engineers must use MFA to perform any actions in Amazon S3.
Which solution will meet these requirements?
A.
Attach a policy to the S3 bucket to prompt the IAM user for an MFA code when the IAM user performs actions on the S3 bucket. Use IAM access keys with the AWS CLI to call Amazon S3.
B.
Update the trust policy for the S3-access group to require principals to use MFA when principals assume the group. Use IAM access keys with the AWS CLI to call Amazon S3.
C.
Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Use IAM access keys with the AWS CLI to call Amazon S3.
D.
Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Request temporary credentials from AWS Security Token Service (AWS STS). Attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3.
AnswerDiscussion
Correct Answer: D
To ensure a secure way for cloud engineers to upload objects into an Amazon S3 bucket using the AWS CLI, the best solution involves implementing MFA (Multi-Factor Authentication). Attaching a policy to the S3-access group that denies all S3 actions unless MFA is present ensures that MFA is enforced. Using AWS STS (Security Token Service) to request temporary credentials adds an additional layer of security by using temporary, short-lived credentials rather than long-term access keys. The temporary credentials, which can be tied to the user's MFA, can then be attached in a profile that Amazon S3 references when performing actions. This ensures MFA is required and securely handles the credentials used for accessing S3.
Question 455 of 529
A company needs to migrate 60 on-premises legacy applications to AWS. The applications are based on the NET Framework and run on Windows.
The company needs a solution that minimizes migration time and requires no application code changes. The company also does not want to manage the infrastructure.
Which solution will meet these requirements?
A.
Refactor the applications and containerize them by using AWS Toolkit for NET Refactoring. Use Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to host the containerized applications.
B.
Use the Windows Web Application Migration Assistant to migrate the applications to AWS Elastic Beanstalk. Use Elastic Beanstalk to deploy and manage the applications.
C.
Use the Windows Web Application Migration Assistant to migrate the applications to Amazon EC2 instances. Use the EC2 instances to deploy and manage the applications.
D.
Refactor the applications and containerize them by using AWS Toolkit for NET Refactoring. Use Amazon Elastic Kubernetes Service (Amazon EKS) with the Fargate launch type to host the containerized applications.
AnswerDiscussion
Correct Answer: B
To migrate 60 on-premises legacy .NET Framework applications running on Windows to AWS, the solution needs to minimize migration time, require no application code changes, and avoid infrastructure management responsibilities. Using the Windows Web Application Migration Assistant to migrate the applications to AWS Elastic Beanstalk meets these requirements. Elastic Beanstalk simplifies the deployment and management of applications by automatically handling the necessary infrastructure provisioning, scaling, and management, allowing the company to focus on their applications without needing to manage the underlying infrastructure.
Question 456 of 529
A company needs to run large batch-processing jobs on data that is stored in an Amazon S3 bucket. The jobs perform simulations. The results of the jobs are not time sensitive, and the process can withstand interruptions.
Each job must process 15-20 GB of data when the data is stored in the S3 bucket. The company will store the output from the jobs in a different Amazon S3 bucket for further analysis.
Which solution will meet these requirements MOST cost-effectively?
A.
Create a serverless data pipeline. Use AWS Step Functions for orchestration. Use AWS Lambda functions with provisioned capacity to process the data.
B.
Create an AWS Batch compute environment that includes Amazon EC2 Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy.
C.
Create an AWS Batch compute environment that includes Amazon EC2 On-Demand Instances and Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy for the Spot Instances.
D.
Use Amazon Elastic Kubernetes Service (Amazon EKS) to run the processing jobs. Use managed node groups that contain a combination of Amazon EC2 On-Demand Instances and Spot Instances.
AnswerDiscussion
Correct Answer: B
Using AWS Batch with Amazon EC2 Spot Instances while specifying the SPOT_CAPACITY_OPTIMIZED allocation strategy is the most cost-effective solution for running large batch-processing jobs on data stored in an Amazon S3 bucket. Spot Instances are typically available at a lower cost compared to On-Demand Instances, and since the jobs are not time-sensitive and can withstand interruptions, this allocation strategy maximizes cost savings while minimizing the likelihood of job interruptions.
Question 457 of 529
A company has an application that analyzes and stores image data on premises. The application receives millions of new image files every day. Files are an average of 1 MB in size. The files are analyzed in batches of 1 GB. When the application analyzes a batch, the application zips the images together. The application then archives the images as a single file in an on-premises NFS server for long-term storage.
The company has a Microsoft Hyper-V environment on premises and has compute capacity available. The company does not have storage capacity and wants to archive the images on AWS. The company needs the ability to retrieve archived data within 1 week of a request.
The company has a 10 Gbps AWS Direct Connect connection between its on-premises data center and AWS. The company needs to set bandwidth limits and schedule archived images to be copied to AWS during non-business hours.
Which solution will meet these requirements MOST cost-effectively?
A.
Deploy an AWS DataSync agent on a new GPU-based Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Instant Retrieval. After the successful copy, delete the data from the on-premises storage.
B.
Deploy an AWS DataSync agent as a Hyper-V VM on premises. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive. After the successful copy, delete the data from the on-premises storage.
C.
Deploy an AWS DataSync agent on a new general purpose Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Standard. After the successful copy, delete the data from the on-premises storage. Create an S3 Lifecycle rule to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 day.
D.
Deploy an AWS Storage Gateway Tape Gateway on premises in the Hyper-V environment. Connect the Tape Gateway to AWS. Use automatic tape creation. Specify an Amazon S3 Glacier Deep Archive pool. Eject the tape after the batch of images is copied.
AnswerDiscussion
Correct Answer: B
The company needs to archive image data on AWS cost-effectively with the ability to retrieve archived data within one week. Deploying an AWS DataSync agent as a Hyper-V VM on premises (where the data is located) ensures efficient data transfer. Configuring the DataSync agent to copy the images directly to Amazon S3 Glacier Deep Archive minimizes costs since Deep Archive is the most cost-effective storage option for data that is rarely accessed and has flexible retrieval times within a week. After a successful copy, deleting the data from the on-premises storage helps manage local storage usage.
Question 458 of 529
A company wants to record key performance indicators (KPIs) from its application as part of a strategy to convert to a user-based licensing schema. The application is a multi-tier application with a web-based UI. The company saves all log files to Amazon CloudWatch by using the CloudWatch agent. All logins to the application are saved in a log file.
As part of the new license schema, the company needs to find out how many unique users each client has on a daily basis, weekly basis, and monthly basis.
Which solution will provide this information with the LEAST change to the application?
A.
Configure an Amazon CloudWatch Logs metric filter that saves each successful login as a metric. Configure the user name and client name as dimensions for the metric.
B.
Change the application logic to make each successful login generate a call to the AWS SDK to increment a custom metric that records user name and client name dimensions in CloudWatch.
C.
Configure the CloudWatch agent to extract successful login metrics from the logs. Additionally, configure the CloudWatch agent to save the successful login metrics as a custom metric that uses the user name and client name as dimensions for the metric.
D.
Configure an AWS Lambda function to consume an Amazon CloudWatch Logs stream of the application logs. Additionally, configure the Lambda function to increment a custom metric in CloudWatch that uses the user name and client name as dimensions for the metric.
AnswerDiscussion
Correct Answer: A
Configuring an Amazon CloudWatch Logs metric filter to save each successful login as a metric and using the user name and client name as dimensions for the metric is the optimal solution. This method leverages the existing functionality of CloudWatch Logs and the CloudWatch Logs agent, requiring minimal changes to the application. The other options suggest either modifying the application logic or adding additional infrastructure components, which are unnecessary for this requirement.
Question 459 of 529
A company is using GitHub Actions to run a CI/CD pipeline that accesses resources on AWS. The company has an IAM user that uses a secret key in the pipeline to authenticate to AWS. An existing IAM role with an attached policy grants the required permissions to deploy resources.
The company’s security team implements a new requirement that pipelines can no longer use long-lived secret keys. A solutions architect must replace the secret key with a short-lived solution.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Create an IAM SAML 2.0 identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Update GitHub to use SAML authentication for the pipeline.
B.
Create an IAM OpenID Connect (OIDC) identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP. Update GitHub to assume the role for the pipeline.
C.
Create an Amazon Cognito identity pool. Configure the authentication provider to use GitHub. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub authentication provider. Configure the pipeline to use Cognito as its authentication provider.
D.
Create a trust anchor to AWS Private Certificate Authority. Generate a client certificate to use with AWS IAM Roles Anywhere. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Configure the pipeline to use the credential helper tool and to reference the client certificate public key to assume the new IAM role.
AnswerDiscussion
Correct Answer: B
To meet the new security requirement of avoiding long-lived secret keys while minimizing operational overhead, using an IAM OpenID Connect (OIDC) identity provider is the best option. The OIDC IdP allows GitHub Actions to assume roles through short-lived tokens, which are securely managed, and this process is natively supported by GitHub. This method simplifies integration and management, providing a seamless transition from long-lived secret keys with minimal additional configuration.
Question 460 of 529
A company is running a web-crawling process on a list of target URLs to obtain training documents for machine learning training algorithms. A fleet of Amazon EC2 t2.micro instances pulls the target URLs from an Amazon Simple Queue Service (Amazon SQS) queue. The instances then write the result of the crawling algorithm as a .csv file to an Amazon Elastic File System (Amazon EFS) volume. The EFS volume is mounted on all instances of the fleet.
A separate system adds the URLs to the SQS queue at infrequent rates. The instances crawl each URL in 10 seconds or less.
Metrics indicate that some instances are idle when no URLs are in the SQS queue. A solutions architect needs to redesign the architecture to optimize costs.
Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)
A.
Use m5.8xlarge instances instead of t2.micro instances for the web-crawling process. Reduce the number of instances in the fleet by 50%.
B.
Convert the web-crawling process into an AWS Lambda function. Configure the Lambda function to pull URLs from the SQS queue.
C.
Modify the web-crawling process to store results in Amazon Neptune.
D.
Modify the web-crawling process to store results in an Amazon Aurora Serverless MySQL instance.
E.
Modify the web-crawling process to store results in Amazon S3.
AnswerDiscussion
Correct Answer: B, E
In the current scenario, the company wants to optimize costs for a web-crawling process running on EC2 instances, which pull URLs from an SQS queue and store results in an EFS volume. Converting the web-crawling process to an AWS Lambda function will eliminate the need for constantly running EC2 instances, reducing costs significantly as AWS Lambda charges are based on the actual compute time consumed, not on reserved capacity. Storing the results in Amazon S3 is a cost-effective and scalable solution for storage, as S3 provides low-cost storage with high durability and availability. Combining AWS Lambda and Amazon S3 will optimize both compute and storage costs.
Question 461 of 529
A company needs to migrate its website from an on-premises data center to AWS. The website consists of a load balancer, a content management system (CMS) that runs on a Linux operating system, and a MySQL database.
The CMS requires persistent NFS-compatible storage for a file system. The new solution on AWS must be able to scale from 2 Amazon EC2 instances to 30 EC2 instances in response to unpredictable traffic increases. The new solution also must require no changes to the website and must prevent data loss.
Which solution will meet these requirements?
A.
Create an Amazon Elastic File System (Amazon EFS) file system. Deploy the CMS to AWS Elastic Beanstalk with an Application Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EFS file system to the EC2 instances. Create an Amazon Aurora MySQL database that is separate from the Elastic Beanstalk environment.
B.
Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Deploy the CMS to AWS Elastic Beanstalk with a Network Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EBS volume to the EC2 instances. Create an Amazon RDS for MySQL database in the Elastic Beanstalk environment.
C.
Create an Amazon Elastic File System (Amazon EFS) file system. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create a Network Load Balancer to distribute traffic. Create an Amazon Aurora MySQL database. Use an EC2 Auto Scaling scale-in lifecycle hook to mount the EFS file system to the EC2 instances.
D.
Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create an Application Load Balancer to distribute traffic. Create an Amazon ElastiCache for Redis cluster to support the MySQL database. Use EC2 user data to attach the EBS volume to the EC2 instances.
AnswerDiscussion
Correct Answer: A
Amazon Elastic File System (Amazon EFS) is the appropriate choice for persistent NFS-compatible storage required by the CMS. Deploying the CMS to AWS Elastic Beanstalk with an Application Load Balancer and an Auto Scaling group ensures that the solution can scale from 2 to 30 EC2 instances in response to traffic increases. By using .ebextensions to mount the EFS file system to the EC2 instances, the requirement of having no changes to the website is met. Separating the Amazon Aurora MySQL database from the Elastic Beanstalk environment provides a robust and scalable database solution, ensuring data integrity and preventing data loss.
Question 462 of 529
A company needs to implement disaster recovery for a critical application that runs in a single AWS Region. The application's users interact with a web frontend that is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application writes to an Amazon RDS for MySQL DB instance. The application also outputs processed documents that are stored in an Amazon S3 bucket.
The company’s finance team directly queries the database to run reports. During busy periods, these queries consume resources and negatively affect application performance.
A solutions architect must design a solution that will provide resiliency during a disaster. The solution must minimize data loss and must resolve the performance problems that result from the finance team's queries.
Which solution will meet these requirements?
A.
Migrate the database to Amazon DynamoDB and use DynamoDB global tables. Instruct the finance team to query a global table in a separate Region. Create an AWS Lambda function to periodically synchronize the contents of the original S3 bucket to a new S3 bucket in the separate Region. Launch EC2 instances and create an ALB in the separate Region. Configure the application to point to the new S3 bucket.
B.
Launch additional EC2 instances that host the application in a separate Region. Add the additional instances to the existing ALIn the separate Region, create a read replica of the RDS DB instance. Instruct the finance team to run queries against the read replica. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Configure the application to point to the new S3 bucket and to the newly promoted read replica.
C.
Create a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket.
D.
Create hourly snapshots of the RDS DB instance. Copy the snapshots to a separate Region. Add an Amazon ElastiCache cluster in front of the existing RDS database. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, restore the database from the latest RDS snapshot. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket.
AnswerDiscussion
Correct Answer: C
To ensure disaster recovery with minimal data loss and resolve performance issues from the finance team's queries, the optimal solution involves creating a read replica of the RDS DB instance in a different Region. The finance team should run their queries against the read replica to minimize the application performance impact. Additionally, creating AMIs of the EC2 instances allows for quick deployment in the event of a disaster. S3 Cross-Region Replication will keep the outputs safe by replicating the S3 bucket data to another Region. If a disaster occurs, the read replica can be promoted to a standalone DB instance, and new EC2 instances can be launched from the AMIs with an ALB to serve the application, ensuring continuity and minimally disrupted data access.
Question 463 of 529
A company has many services running in its on-premises data center. The data center is connected to AWS using AWS Direct Connect (DX) and an IPSec VPN. The service data is sensitive and connectivity cannot traverse the internet. The company wants to expand into a new market segment and begin offering its services to other companies that are using AWS.
Which solution will meet these requirements?
A.
Create a VPC Endpoint Service that accepts TCP traffic, host it behind a Network Load Balancer, and make the service available over DX.
B.
Create a VPC Endpoint Service that accepts HTTP or HTTPS traffic, host it behind an Application Load Balancer, and make the service available over DX.
C.
Attach an internet gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic.
D.
Attach a NAT gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic.
AnswerDiscussion
Correct Answer: A
To meet the requirement of ensuring that service data does not traverse the internet and is securely accessible, the best solution is to use a VPC Endpoint Service hosted behind a Network Load Balancer (NLB). VPC Endpoint Services, also known as PrivateLink, allow secure and private connectivity between VPCs without crossing the internet. The use of an NLB is ideal because it can handle TCP traffic, which is versatile and can cover various types of services. This ensures that the sensitive service data remains within the secure network boundaries provided by AWS Direct Connect.
Question 464 of 529
A company uses AWS Organizations to manage its AWS accounts. A solutions architect must design a solution in which only administrator roles are allowed to use IAM actions. However, the solutions architect does not have access to all the AWS accounts throughout the company.
Which solution meets these requirements with the LEAST operational overhead?
A.
Create an SCP that applies to all the AWS accounts to allow IAM actions only for administrator roles. Apply the SCP to the root OU.
B.
Configure AWS CloudTrail to invoke an AWS Lambda function for each event that is related to IAM actions. Configure the function to deny the action if the user who invoked the action is not an administrator.
C.
Create an SCP that applies to all the AWS accounts to deny IAM actions for all users except for those with administrator roles. Apply the SCP to the root OU.
D.
Set an IAM permissions boundary that allows IAM actions. Attach the permissions boundary to every administrator role across all the AWS accounts.
AnswerDiscussion
Correct Answer: C
The most effective solution with the least operational overhead is to create a Service Control Policy (SCP) that applies to all the AWS accounts to deny IAM actions for all users except those with administrator roles and apply this SCP to the root Organizational Unit (OU). SCPs manage permissions within an AWS Organization and can enforce such policies across all accounts in the organization at a high level, making it easier to maintain and manage permissions consistently without needing access to each individual AWS account.
Question 465 of 529
A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company hosts some applications in a VPC in the company's shared services account.
The company has attached a transit gateway to the VPC in the shared services account.
The company is developing a new capability and has created a development environment that requires access to the applications that are in the shared services account. The company intends to delete and recreate resources frequently in the development account. The company also wants to give a development team the ability to recreate the team's connection to the shared services account as required.
Which solution will meet these requirements?
A.
Create a transit gateway in the development account. Create a transit gateway peering request to the shared services account. Configure the shared services transit gateway to automatically accept peering connections.
B.
Turn on automatic acceptance for the transit gateway in the shared services account. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway resource in the shared services account with the development account. Accept the resource in the development account. Create a transit gateway attachment in the development account.
C.
Turn on automatic acceptance for the transit gateway in the shared services account. Create a VPC endpoint. Use the endpoint policy to grant permissions on the VPC endpoint for the development account. Configure the endpoint service to automatically accept connection requests. Provide the endpoint details to the development team.
D.
Create an Amazon EventBridge rule to invoke an AWS Lambda function that accepts the transit gateway attachment when the development account makes an attachment request. Use AWS Network Manager to share the transit gateway in the shared services account with the development account. Accept the transit gateway in the development account.
AnswerDiscussion
Correct Answer: B
The development account needs a flexible and automated solution to frequently recreate the connection to the applications in the shared services account. By turning on automatic acceptance for the transit gateway in the shared services account and using AWS Resource Access Manager (AWS RAM) to share the transit gateway resource, the development team is empowered to create transit gateway attachments as needed without manual intervention, thus meeting the requirement for frequent deletions and recreations. This approach provides an efficient, secure, and manageable solution.
Question 466 of 529
A company wants to migrate virtual Microsoft workloads from an on-premises data center to AWS. The company has successfully tested a few sample workloads on AWS. The company also has created an AWS Site-to-Site VPN connection to a VPC. A solutions architect needs to generate a total cost of ownership (TCO) report for the migration of all the workloads from the data center.
Simple Network Management Protocol (SNMP) has been enabled on each VM in the data center. The company cannot add more VMs in the data center and cannot install additional software on the VMs. The discovery data must be automatically imported into AWS Migration Hub.
Which solution will meet these requirements?
A.
Use the AWS Application Migration Service agentless service and the AWS Migration Hub Strategy Recommendations to generate the TCO report.
B.
Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Evaluator to generate the TCO report.
C.
Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Hub to generate the TCO report.
D.
Use the AWS Migration Readiness Assessment tool inside the VPC. Configure Migration Evaluator to generate the TCO report.
AnswerDiscussion
Correct Answer: B
The company needs to generate a TCO report for migrating all workloads from the data center, and the data must be automatically imported into AWS Migration Hub. Given the constraints of not being able to install additional software on the on-premises VMs, using an agentless collector is crucial. By launching a Windows Amazon EC2 instance and installing the Migration Evaluator agentless collector on this instance, the company can gather the necessary data via SNMP. Migration Evaluator is specifically designed to analyze such data and generate financial forecasts, including TCO reports. This solution meets all the requirements and leverages AWS tools effectively.
Question 467 of 529
A company that is developing a mobile game is making game assets available in two AWS Regions. Game assets are served from a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in each Region. The company requires game assets to be fetched from the closest Region. If game assets become unavailable in the closest Region, they should be fetched from the other Region.
What should a solutions architect do to meet these requirements?
A.
Create an Amazon CloudFront distribution. Create an origin group with one origin for each ALB. Set one of the origins as primary.
B.
Create an Amazon Route 53 health check for each ALCreate a Route 53 failover routing record pointing to the two ALBs. Set the Evaluate Target Health value to Yes.
C.
Create two Amazon CloudFront distributions, each with one ALB as the origin. Create an Amazon Route 53 failover routing record pointing to the two CloudFront distributions. Set the Evaluate Target Health value to Yes.
D.
Create an Amazon Route 53 health check for each ALB. Create a Route 53 latency alias record pointing to the two ALBs. Set the Evaluate Target Health value to Yes.
AnswerDiscussion
Correct Answer: D
To meet the requirements of fetching game assets from the closest region and failing over to the other region if the closest region becomes unavailable, using Amazon Route 53 latency alias records is appropriate. Latency alias records direct traffic to the region with the lowest latency, thereby usually pointing to the closest region. The health checks ensure that if the closest region becomes unhealthy, traffic is redirected to the other region. This method efficiently distributes traffic based on latency and ensures high availability in case of a regional failure.
Question 468 of 529
A company deploys workloads in multiple AWS accounts. Each account has a VPC with VPC flow logs published in text log format to a centralized Amazon S3 bucket. Each log file is compressed with gzip compression. The company must retain the log files indefinitely.
A security engineer occasionally analyzes the logs by using Amazon Athena to query the VPC flow logs. The query performance is degrading over time as the number of ingested logs is growing. A solutions architect must improve the performance of the log analysis and reduce the storage space that the VPC flow logs use.
Which solution will meet these requirements with the LARGEST performance improvement?
A.
Create an AWS Lambda function to decompress the gzip files and to compress the files with bzip2 compression. Subscribe the Lambda function to an s3:ObjectCreated:Put S3 event notification for the S3 bucket.
B.
Enable S3 Transfer Acceleration for the S3 bucket. Create an S3 Lifecycle configuration to move files to the S3 Intelligent-Tiering storage class as soon as the files are uploaded.
C.
Update the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files.
D.
Create a new Athena workgroup without data usage control limits. Use Athena engine version 2.
AnswerDiscussion
Correct Answer: C
To address both the query performance degradation and the optimization of storage space usage, converting log files to Apache Parquet format is an effective solution. Apache Parquet is a columnar storage file format that provides efficient data compression and encoding schemes, which improves performance and reduces storage space. Additionally, specifying hourly partitions for the log files helps to efficiently organize and manage the data, further enhancing the performance of queries in Amazon Athena. This approach leverages the advantages of columnar storage and partitioning to meet the requirements with the largest performance improvement.
Question 469 of 529
A company wants to establish a dedicated connection between its on-premises infrastructure and AWS. The company is setting up a 1 Gbps AWS Direct Connect connection to its account VPC. The architecture includes a transit gateway and a Direct Connect gateway to connect multiple VPCs and the on-premises infrastructure.
The company must connect to VPC resources over a transit VIF by using the Direct Connect connection.
Which combination of steps will meet these requirements? (Choose two.)
A.
Update the 1 Gbps Direct Connect connection to 10 Gbps.
B.
Advertise the on-premises network prefixes over the transit VIF.
C.
Advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF.
D.
Update the Direct Connect connection's MACsec encryption mode attribute to must_encrypt.
E.
Associate a MACsec Connection Key Name/Connectivity Association Key (CKN/CAK) pair with the Direct Connect connection.
AnswerDiscussion
Correct Answer: B, C
To establish a dedicated connection between on-premises infrastructure and AWS using AWS Direct Connect and a transit VIF, the appropriate steps are to advertise the on-premises network prefixes over the transit VIF to ensure routing between on-premises and the VPC, and to advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF. Updating the connection to 10 Gbps, encryption requirements, and associating a MACsec Connection Key Name/Connectivity Association Key are not necessary for basic connectivity and routing requirements in this scenario.
Question 470 of 529
A company wants to use Amazon WorkSpaces in combination with thin client devices to replace aging desktops. Employees use the desktops to access applications that work with Clinical trial data. Corporate security policy states that access to the applications must be restricted to only company branch office locations. The company is considering adding an additional branch office in the next 6 months.
Which solution meets these requirements with the MOST operational efficiency?
A.
Create an IP access control group rule with the list of public addresses from the branch offices. Associate the IP access control group with the WorkSpaces directory.
B.
Use AWS Firewall Manager to create a web ACL rule with an IPSet with the list of public addresses from the branch office locations. Associate the web ACL with the WorkSpaces directory.
C.
Use AWS Certificate Manager (ACM) to issue trusted device certificates to the machines deployed in the branch office locations. Enable restricted access on the WorkSpaces directory.
D.
Create a custom WorkSpace image with Windows Firewall configured to restrict access to the public addresses of the branch offices. Use the image to deploy the WorkSpaces.
AnswerDiscussion
Correct Answer: A
Creating an IP access control group with the list of public addresses from the branch offices and associating it with the WorkSpaces directory is the most operationally efficient solution. This approach directly addresses the requirement to restrict access to the applications from specific office locations and allows for easy updates when new branch offices are added. It is simple to implement and manage, ensuring that only devices within the specified IP range can access the WorkSpaces.
Question 471 of 529
A company uses AWS Organizations. The company runs two firewall appliances in a centralized networking account. Each firewall appliance runs on a manually configured highly available Amazon EC2 instance. A transit gateway connects the VPC from the centralized networking account to VPCs of member accounts. Each firewall appliance uses a static private IP address that is then used to route traffic from the member accounts to the internet.
During a recent incident, a badly configured script initiated the termination of both firewall appliances. During the rebuild of the firewall appliances, the company wrote a new script to configure the firewall appliances at startup.
The company wants to modernize the deployment of the firewall appliances. The firewall appliances need the ability to scale horizontally to handle increased traffic when the network expands. The company must continue to use the firewall appliances to comply with company policy. The provider of the firewall appliances has confirmed that the latest version of the firewall code will work with all AWS services.
Which combination of steps should the solutions architect recommend to meet these requirements MOST cost-effectively? (Choose three.)
A.
Deploy a Gateway Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.
B.
Deploy a Network Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.
C.
Create an Auto Scaling group and a launch template that uses the new script as user data to configure the firewall appliances. Create a target group that uses the instance target type.
D.
Create an Auto Scaling group. Configure an AWS Launch Wizard deployment that uses the new script as user data to configure the firewall appliances. Create a target group that uses the IP target type.
E.
Create VPC endpoints in each member account. Update the route tables to point to the VPC endpoints.
F.
Create VPC endpoints in the centralized networking account. Update the route tables in each member account to point to the VPC endpoints.
AnswerDiscussion
Correct Answer: A, C, E
To meet the requirements of modernizing the deployment of firewall appliances in a cost-effective manner, first, deploying a Gateway Load Balancer in the centralized networking account is essential. This will provide load balancing and scaling capabilities suitable for handling traffic across the firewall appliances. Second, creating an Auto Scaling group and a launch template that uses the new script as user data will automate the deployment and scaling of the firewall appliances. This approach ensures high availability and horizontal scalability. Finally, creating VPC endpoints in each member account and updating their route tables to point to these VPC endpoints is necessary to ensure that traffic is redirected through the centralized networking account effectively, maintaining compliance with company policy.
Question 472 of 529
A solutions architect must implement a multi-Region architecture for an Amazon RDS for PostgreSQL database that supports a web application. The database launches from an AWS CloudFormation template that includes AWS services and features that are present in both the primary and secondary Regions.
The database is configured for automated backups, and it has an RTO of 15 minutes and an RPO of 2 hours. The web application is configured to use an Amazon Route 53 record to route traffic to the database.
Which combination of steps will result in a highly available architecture that meets all the requirements? (Choose two.)
A.
Create a cross-Region read replica of the database in the secondary Region. Configure an AWS Lambda function in the secondary Region to promote the read replica during a failover event.
B.
In the primary Region, create a health check on the database that will invoke an AWS Lambda function when a failure is detected. Program the Lambda function to recreate the database from the latest database snapshot in the secondary Region and update the Route 53 host records for the database.
C.
Create an AWS Lambda function to copy the latest automated backup to the secondary Region every 2 hours.
D.
Create a failover routing policy in Route 53 for the database DNS record. Set the primary and secondary endpoints to the endpoints in each Region.
E.
Create a hot standby database in the secondary Region. Use an AWS Lambda function to restore the secondary database to the latest RDS automatic backup in the event that the primary database fails.
AnswerDiscussion
Correct Answer: A, D
To achieve a highly available architecture for an Amazon RDS for PostgreSQL database with a multi-Region setup, implementing a cross-Region read replica and configuring a failover mechanism are essential. Creating a cross-Region read replica allows for near real-time replication and quick promotion during a failover, meeting both the RTO and RPO requirements. Setting up a failover routing policy in Route 53 ensures that traffic is rerouted to the secondary Region seamlessly in the event of a primary Region failure. Together, these steps provide a resilient and efficient disaster recovery solution.
Question 473 of 529
An ecommerce company runs an application on AWS. The application has an Amazon API Gateway API that invokes an AWS Lambda function. The data is stored in an Amazon RDS for PostgreSQL DB instance.
During the company’s most recent flash sale, a sudden increase in API calls negatively affected the application's performance. A solutions architect reviewed the Amazon CloudWatch metrics during that time and noticed a significant increase in Lambda invocations and database connections. The CPU utilization also was high on the DB instance.
What should the solutions architect recommend to optimize the application's performance?
A.
Increase the memory of the Lambda function. Modify the Lambda function to close the database connections when the data is retrieved.
B.
Add an Amazon ElastiCache for Redis cluster to store the frequently accessed data from the RDS database.
C.
Create an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint.
D.
Modify the Lambda function to connect to the database outside of the function's handler. Check for an existing database connection before creating a new connection.
AnswerDiscussion
Correct Answer: C
During a flash sale, the application experienced increased API calls which led to higher Lambda invocations and database connections, resulting in high CPU utilization in the RDS instance. Using an RDS proxy can help optimize the performance by managing a pool of database connections efficiently. This way, Lambda functions can reuse existing connections instead of opening new ones frequently, reducing the load on the database and improving overall performance. Therefore, creating an RDS proxy and modifying the Lambda function to use the proxy endpoint is the best solution. Additionally, it is indeed possible to create an RDS proxy from the Lambda console, ensuring seamless integration.
Question 474 of 529
A retail company wants to improve its application architecture. The company's applications register new orders, handle returns of merchandise, and provide analytics. The applications store retail data in a MySQL database and an Oracle OLAP analytics database. All the applications and databases are hosted on Amazon EC2 instances.
Each application consists of several components that handle different parts of the order process. These components use incoming data from different sources. A separate ETL job runs every week and copies data from each application to the analytics database.
A solutions architect must redesign the architecture into an event-driven solution that uses serverless services. The solution must provide updated analytics in near real time.
Which solution will meet these requirements?
A.
Migrate the individual applications as microservices to Amazon Elastic Container Service (Amazon ECS) containers that use AWS Fargate. Keep the retail MySQL database on Amazon EC2. Move the analytics database to Amazon Neptune. Use Amazon Simple Queue Service (Amazon SQS) to send all the incoming data to the microservices and the analytics database.
B.
Create an Auto Scaling group for each application. Specify the necessary number of EC2 instances in each Auto Scaling group. Migrate the retail MySQL database and the analytics database to Amazon Aurora MySQL. Use Amazon Simple Notification Service (Amazon SNS) to send all the incoming data to the correct EC2 instances and the analytics database.
C.
Migrate the individual applications as microservices to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use AWS Fargate. Migrate the retail MySQL database to Amazon Aurora Serverless MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use Amazon EventBridge to send all the incoming data to the microservices and the analytics database.
D.
Migrate the individual applications as microservices to Amazon AppStream 2.0. Migrate the retail MySQL database to Amazon Aurora MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use AWS IoT Core to send all the incoming data to the microservices and the analytics database.
AnswerDiscussion
Correct Answer: C
To design an event-driven solution using serverless services that provides updated analytics in near real time, migrating individual applications as microservices to Amazon Elastic Kubernetes Service (Amazon EKS) containers using AWS Fargate ensures a scalable and containerized environment. Moving the retail MySQL database to Amazon Aurora Serverless MySQL provides a serverless, highly available relational database solution. Migrating the analytics database to Amazon Redshift Serverless offers a serverless, scalable data warehouse optimized for real-time analytics. Using Amazon EventBridge to send incoming data to both the microservices and the analytics database facilitates an event-driven architecture, providing the necessary real-time data streaming capabilities.
Question 475 of 529
A company is planning a migration from an on-premises data center to the AWS Cloud. The company plans to use multiple AWS accounts that are managed in an organization in AWS Organizations. The company will create a small number of accounts initially and will add accounts as needed. A solutions architect must design a solution that turns on AWS CloudTrail in all AWS accounts.
What is the MOST operationally efficient solution that meets these requirements?
A.
Create an AWS Lambda function that creates a new CloudTrail trail in all AWS accounts in the organization. Invoke the Lambda function daily by using a scheduled action in Amazon EventBridge.
B.
Create a new CloudTrail trail in the organization's management account. Configure the trail to log all events for all AWS accounts in the organization.
C.
Create a new CloudTrail trail in all AWS accounts in the organization. Create new trails whenever a new account is created. Define an SCP that prevents deletion or modification of trails. Apply the SCP to the root OU.
D.
Create an AWS Systems Manager Automation runbook that creates a CloudTrail trail in all AWS accounts in the organization. Invoke the automation by using Systems Manager State Manager.
AnswerDiscussion
Correct Answer: B
Creating a new CloudTrail trail in the organization's management account and configuring it to log all events for all AWS accounts in the organization is the most operationally efficient solution. This approach centralizes the logging configuration, eliminates the need to manage multiple trails across various accounts, and ensures consistent logging without the need for continuous manual intervention. Additionally, it allows for easier monitoring and auditing of all activities across the organization.
Question 476 of 529
A software development company has multiple engineers who are working remotely. The company is running Active Directory Domain Services (AD DS) on an Amazon EC2 instance. The company's security policy states that all internal, nonpublic services that are deployed in a VPC must be accessible through a VPN. Multi-factor authentication (MFA) must be used for access to a VPN.
What should a solutions architect do to meet these requirements?
A.
Create an AWS Site-to-Site VPN connection. Configure integration between a VPN and AD DS. Use an Amazon WorkSpaces client with MFA support enabled to establish a VPN connection.
B.
Create an AWS Client VPN endpoint. Create an AD Connector directory for integration with AD DS. Enable MFA for AD Connector. Use AWS Client VPN to establish a VPN connection.
C.
Create multiple AWS Site-to-Site VPN connections by using AWS VPN CloudHub. Configure integration between AWS VPN CloudHub and AD DS. Use AWS Copilot to establish a VPN connection.
D.
Create an Amazon WorkLink endpoint. Configure integration between Amazon WorkLink and AD DS. Enable MFA in Amazon WorkLink. Use AWS Client VPN to establish a VPN connection.
AnswerDiscussion
Correct Answer: B
To meet the requirements of providing secure, MFA-enabled VPN access to internal, nonpublic services deployed in a VPC while integrating with Active Directory Domain Services (AD DS), the correct solution is to create an AWS Client VPN endpoint. An AWS Client VPN endpoint allows for secure connections from client devices to AWS or on-premises networks. By using an AD Connector directory for integration with AD DS and enabling multi-factor authentication (MFA), the security policies of the company are met effectively. AWS Client VPN supports MFA, ensuring secure authentication for users accessing the VPN.
Question 477 of 529
A company is running a three-tier web application in an on-premises data center. The frontend is served by an Apache web server, the middle tier is a monolithic Java application, and the storage tier is a PostgreSQL database.
During a recent marketing promotion, customers could not place orders through the application because the application crashed. An analysis showed that all three tiers were overloaded. The application became unresponsive, and the database reached its capacity limit because of read operations. The company already has several similar promotions scheduled in the near future.
A solutions architect must develop a plan for migration to AWS to resolve these issues. The solution must maximize scalability and must minimize operational effort
Which combination of steps will meet these requirements? (Choose three.)
A.
Refactor the frontend so that static assets can be hosted on Amazon S3. Use Amazon CloudFront to serve the frontend to customers. Connect the frontend to the Java application.
B.
Rehost the Apache web server of the frontend on Amazon EC2 instances that are in an Auto Scaling group. Use a load balancer in front of the Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) to host the static assets that the Apache web server needs.
C.
Rehost the Java application in an AWS Elastic Beanstalk environment that includes auto scaling.
D.
Refactor the Java application, Develop a Docker container to run the Java application. Use AWS Fargate to host the container.
E.
Use AWS Database Migration Service (AWS DMS) to replatform the PostgreSQL database to an Amazon Aurora PostgreSQL database. Use Aurora Auto Scaling for read replicas.
F.
Rehost the PostgreSQL database on an Amazon EC2 instance that has twice as much memory as the on-premises server.
AnswerDiscussion
Correct Answer: A, C, E
To address the scalability and operational effort issues for the on-premises web application, the following steps are recommended: Refactor the frontend so that static assets can be hosted on Amazon S3 and use Amazon CloudFront to serve the frontend to customers, as this offloads the frontend static content delivery and enhances performance. Migrate the Java application to an AWS Elastic Beanstalk environment, which provides simplified deployment, auto-scaling, and reduced operational overhead. Use AWS Database Migration Service to replatform the PostgreSQL database to an Amazon Aurora PostgreSQL database, leveraging Aurora Auto Scaling to manage read replicas efficiently and handle increased load. These steps collectively maximize scalability and minimize operational effort, ensuring the application can handle future promotions successfully.
Question 478 of 529
A company is deploying a new application on AWS. The application consists of an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and an Amazon Elastic Container Registry (Amazon ECR) repository. The EKS cluster has an AWS managed node group.
The company's security guidelines state that all resources on AWS must be continuously scanned for security vulnerabilities.
Which solution will meet this requirement with the LEAST operational overhead?
A.
Activate AWS Security Hub. Configure Security Hub to scan the EKS nodes and the ECR repository.
B.
Activate Amazon Inspector to scan the EKS nodes and the ECR repository.
C.
Launch a new Amazon EC2 instance and install a vulnerability scanning tool from AWS Marketplace. Configure the EC2 instance to scan the EKS nodes. Configure Amazon ECR to perform a basic scan on push.
D.
Install the Amazon CloudWatch agent on the EKS nodes. Configure the CloudWatch agent to scan continuously. Configure Amazon ECR to perform a basic scan on push.
AnswerDiscussion
Correct Answer: B
Activating Amazon Inspector will meet the requirements with the least operational overhead. Amazon Inspector automatically discovers and scans running Amazon EC2 instances (which includes EKS nodes in this context) and container images in Amazon ECR for known software vulnerabilities and unintended network exposure. This solution provides continuous and automated vulnerability management without the need for additional configuration or manual intervention, thus minimizing operational overhead.
Question 479 of 529
A company needs to improve the reliability of its ticketing application. The application runs on an Amazon Elastic Container Service (Amazon ECS) cluster. The company uses Amazon CloudFront to serve the application. A single ECS service of the ECS cluster is the CloudFront distribution’s origin.
The application allows only a specific number of active users to enter a ticket purchasing flow. These users are identified by an encrypted attribute in their JSON Web Token (JWT). All other users are redirected to a waiting room module until there is available capacity for purchasing.
The application is experiencing high loads. The waiting room module is working as designed, but load on the waiting room is disrupting the applications availability.
This disruption is negatively affecting the application's ticket sale transactions.
Which solution will provide the MOST reliability for ticket sale transactions during periods of high load?
A.
Create a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Ensure that the ticketing service uses the JWT information and appropriately forwards requests to the waiting room service.
B.
Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Make the ticketing pod part of a StatefulSet. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod.
C.
Create a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Create a CloudFront function that inspects the JWT information and appropriately forwards requests to the ticketing service or the waiting room service.
D.
Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Use AWS App Mesh by provisioning the App Mesh controller for Kubernetes. Enable mTLS authentication and service-to-service authentication for communication between the ticketing pod and the waiting room pod. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod.
AnswerDiscussion
Correct Answer: C
To improve the reliability of the ticketing application during high load periods, creating a separate service in the ECS cluster for the waiting room with a distinct scaling configuration is key. By utilizing a CloudFront function that inspects the JWT information and appropriately forwards requests to either the ticketing service or the waiting room service, the solution offloads the decision-making process from the ECS services themselves. This reduces the load on the ticketing service, ensuring that it remains available for ticket sale transactions, while the dedicated waiting room service handles the waiting users efficiently.
Question 480 of 529
A solutions architect is creating an AWS CloudFormation template from an existing manually created non-production AWS environment. The CloudFormation template can be destroyed and recreated as needed. The environment contains an Amazon EC2 instance. The EC2 instance has an instance profile that the EC2 instance uses to assume a role in a parent account.
The solutions architect recreates the role in a CloudFormation template and uses the same role name. When the CloudFormation template is launched in the child account, the EC2 instance can no longer assume the role in the parent account because of insufficient permissions
What should the solutions architect do to resolve this issue?
A.
In the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Ensure that the target role ARN in the existing statement that allows the sts:AssumeRole action is correct. Save the trust policy.
B.
In the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Add a statement that allows the sts:AssumeRole action for the root principal of the child account. Save the trust policy.
C.
Update the CloudFormation stack again. Specify only the CAPABILITY_NAMED_IAM capability.
D.
Update the CloudFormation stack again. Specify the CAPABILITY_IAM capability and the CAPABILITY_NAMED_IAM capability.
AnswerDiscussion
Correct Answer: A
The correct answer involves editing the trust policy to ensure that the correct ARN for the role in the child account is specified. When a role is recreated, even if it has the same name, its ARN changes. The EC2 instance needs the correct role ARN to assume the role in the parent account. Allowing the root principal (as in option B) is not recommended due to security risks. Specifying only the CAPABILITY_NAMED_IAM or both IAM capabilities in CloudFormation (options C and D) does not address the trust policy issue. Therefore, to resolve the issue, the trust policy in the parent account must be updated with the correct ARN for the role, ensuring the EC2 instance can assume the role properly.
Question 481 of 529
A company's web application has reliability issues. The application serves customers globally. The application runs on a single Amazon EC2 instance and performs read-intensive operations on an Amazon RDS for MySQL database.
During high load, the application becomes unresponsive and requires a manual restart of the EC2 instance. A solutions architect must improve the application's reliability.
Which solution will meet this requirement with the LEAST development effort?
A.
Create an Amazon CloudFront distribution. Specify the EC2 instance as the distribution’s origin. Configure a Multi-AZ deployment for the RDS for MySQL database. Use the standby DB instance for the read-intensive operations.
B.
Run the application on EC2 instances that are in an Auto Scaling group. Place the EC2 instances behind an Elastic Load Balancing (ELB) load balancer. Replace the database service with Amazon Aurora. Use Aurora Replicas for the read-intensive operations.
C.
Deploy AWS Global Accelerator. Configure a Multi-AZ deployment for the RDS for MySQL database. Use the standby DB instance for the read-intensive operations.
D.
Migrate the application to AWS Lambda functions. Create read replicas for the RDS for MySQL database. Use the read replicas for the read-intensive operations.
AnswerDiscussion
Correct Answer: B
Running the application on EC2 instances that are part of an Auto Scaling group and placing them behind an Elastic Load Balancing (ELB) load balancer addresses the need for automatic restarts and scaling based on load. Replacing the database service with Amazon Aurora, which offers built-in replication capabilities through Aurora Replicas, ensures that read-intensive operations are handled efficiently. This combination improves reliability and scalability with minimal development effort since it leverages managed AWS services designed for such use cases.
Question 482 of 529
A company needs to use an AWS Transfer Family SFTP-enabled server with an Amazon S3 bucket to receive updates from a third-party data supplier. The data is encrypted with Pretty Good Privacy (PGP) encryption. The company needs a solution that will automatically decrypt the data after the company receives the data.
A solutions architect will use a Transfer Family managed workflow. The company has created an IAM service role by using an IAM policy that allows access to AWS Secrets Manager and the S3 bucket. The role’s trust relationship allows the transfer amazonaws.com service to assume the role.
What should the solutions architect do next to complete the solution for automatic decryption?
A.
Store the PGP public key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP encryption parameters in the nominal step. Associate the workflow with the Transfer Family server.
B.
Store the PGP private key in Secrets Manager. Add an exception-handling step in the Transfer Family managed workflow to decrypt files. Configure PGP encryption parameters in the exception handler. Associate the workflow with the SFTP user.
C.
Store the PGP private key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the nominal step. Associate the workflow with the Transfer Family server.
D.
Store the PGP public key in Secrets Manager. Add an exception-handling step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the exception handler. Associate the workflow with the SFTP user.
AnswerDiscussion
Correct Answer: C
To automatically decrypt the data received via the AWS Transfer Family SFTP server, the PGP private key must be stored in AWS Secrets Manager since it is needed for decryption. A nominal step in the Transfer Family managed workflow should be added to handle the decryption process. The decryption parameters should be configured in this step, and then the workflow should be associated with the Transfer Family server to ensure the decryption happens as part of the managed workflow. Storing the public key is incorrect for decryption purposes because the private key is required to decrypt data encrypted with PGP.
Question 483 of 529
A company is migrating infrastructure for its massive multiplayer game to AWS. The game’s application features a leaderboard where players can see rankings in real time. The leaderboard requires microsecond reads and single-digit-millisecond write latencies. The datasets are single-digit terabytes in size and must be available to accept writes in less than a minute if a primary node failure occurs.
The company needs a solution in which data can persist for further analytical processing through a data pipeline.
Which solution will meet these requirements with the LEAST operational overhead?
B.
Create an Amazon ROS database with a read replica. Configure the application to point writes to the writer endpoint. Configure the application to point reads to the reader endpoint.
C.
Create an Amazon MemoryDB for Redis cluster in Muit-AZ mode Configure the application to interact with the primary node.
D.
Create multiple Redis nodes on Amazon EC2 instances that are spread across multiple Availability Zones. Configure backups to Amazon S3.
AnswerDiscussion
Correct Answer: C
MemoryDB for Redis is designed to provide ultra-fast performance with microsecond read and single-digit millisecond write latencies, making it ideal for a real-time leaderboard in a gaming application. It supports Multi-AZ deployments for high availability and can handle primary node failures by promoting a replica quickly. Additionally, it can persist data which makes it suitable for further analytical processing, all with minimal operational overhead.
Question 484 of 529
A company is running several applications in the AWS Cloud. The applications are specific to separate business units in the company. The company is running the components of the applications in several AWS accounts that are in an organization in AWS Organizations.
Every cloud resource in the company’s organization has a tag that is named BusinessUnit. Every tag already has the appropriate value of the business unit name.
The company needs to allocate its cloud costs to different business units. The company also needs to visualize the cloud costs for each business unit.
Which solution will meet these requirements?
A.
In the organization's management account, create a cost allocation tag that is named BusinessUnit. Also in the management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization.
B.
In each member account, create a cost allocation tag that is named BusinessUnit. In the organization’s management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. Create an Amazon CloudWatch dashboard for visualization.
C.
In the organization's management account, create a cost allocation tag that is named BusinessUnit. In each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR. In the management account, create an Amazon CloudWatch dashboard for visualization.
D.
In each member account, create a cost allocation tag that is named BusinessUnit. Also in each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization.
AnswerDiscussion
Correct Answer: A
To meet the requirements of allocating cloud costs to different business units and visualizing the costs, the appropriate solution involves having a central point for collecting and processing the cost data. The correct approach is to create a cost allocation tag named BusinessUnit in the organization's management account, which will help categorize the costs based on business units. Additionally, setting up an AWS Cost and Usage Report (AWS CUR) in the management account with an Amazon S3 bucket as the destination allows centralized collection of cost data. Querying this data using Amazon Athena provides the capability to analyze the costs effectively, and using Amazon QuickSight offers robust visualization tools to present the data for each business unit efficiently. Hence, the best solution is to follow these steps in the management account for comprehensive cost allocation and visualization.
Question 485 of 529
A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function. and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete.
As more smart meters are deployed, the engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many ProvisionedThroughputExceededException errors while performing PUT operations on DynamoDB, and there are also many TooManyRequestsException errors from Lambda.
Which combination of changes will resolve these issues? (Choose two.)
A.
Increase the write capacity units to the DynamoDB table.
B.
Increase the memory available to the Lambda functions.
C.
Increase the payload size from the smart meters to send more data.
D.
Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.
E.
Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message
AnswerDiscussion
Correct Answer: A, D
To resolve the issues faced by the utility company, increasing the write capacity units of the DynamoDB table will help in handling the increased load and avoiding ProvisionedThroughputExceededException errors. Streaming the data into an Amazon Kinesis data stream from API Gateway and processing the data in batches is also beneficial. This approach will reduce the load on the Lambda functions and optimize the data processing workflow, addressing both the TooManyRequestsException errors and the increased duration of Lambda executions.
Question 486 of 529
A company recently completed a successful proof of concept of Amazon WorkSpaces. A solutions architect needs to make the solution highly available across two AWS Regions. Amazon WorkSpaces is deployed in a failover Region, and a hosted zone is deployed in Amazon Route 53.
What should the solutions architect do to configure high availability for the solution?
A.
Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in each Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes.
B.
Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in the primary Region. Create a Route 53 multivalue answer routing policy.
C.
Create a connection alias in the primary Region. Associate the connection alias with a directory in the primary Region. Create a Route 53 weighted routing policy.
D.
Create a connection alias in the primary Region Associate the connection alias with a directory in the failover Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes.
AnswerDiscussion
Correct Answer: A
To ensure high availability for Amazon WorkSpaces across two AWS Regions, the correct approach is to create a connection alias in both the primary and failover Regions and associate each alias with a directory in its respective Region. This setup allows WorkSpaces to be available in both Regions. Using a Route 53 failover routing policy with Evaluate Target Health set to Yes ensures that if the WorkSpaces in the primary Region become unavailable, traffic will be redirected to the WorkSpaces in the failover Region. This configuration aligns with best practices for implementing high availability and failover mechanisms.
Question 487 of 529
A company plans to migrate many VMs from an on-premises environment to AWS. The company requires an initial assessment of the on-premises environment before the migration, a visualization of the dependencies between applications that run on the VMs, and a report that provides an assessment of the on-premises environment.
To get this information, the company has initiated a Migration Evaluator assessment request. The company has the ability to install collector software in its on-premises environment without any constraints
Which solution will provide the company with the required information with the LEAST operational overhead?
A.
Install the AWS Application Discovery Agent on each on-premises VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick insights assessment report from Migration Hub.
B.
Install the Migration Evaluator Collector on each on-premises VM. After the data collection period ends, use Migration Evaluator to view the application dependencies. Download and export the discovered server list from Migration Evaluator. Upload the list to Amazon QuickSight When the QuickSight report is generated, download the Quick Insights assessment report.
C.
Setup the AWS Application Discovery Service Agentless Collector in the on-premises environment. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Export the discovered server list from Application Discovery Service. Upload the list to Migration Evaluator. When the Migration Evaluator report is generated, download the Quick Insights assessment.
D.
Set up the Migration Evaluator Collector in the on-premises environment. Install the AWS Application Discovery Agent on each VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick Insights assessment report from Migration Evaluator.
AnswerDiscussion
Correct Answer: A
To acquire an initial assessment of the on-premises environment, visualize application dependencies, and obtain an assessment report with the least operational overhead, the company should install the AWS Application Discovery Agent on each on-premises VM. After the data collection period ends, AWS Migration Hub will facilitate viewing the application dependencies, and the Quick Insights assessment report can be downloaded from Migration Hub. This approach minimizes the operational complexity by utilizing the AWS Application Discovery Agent and the integrated capabilities of AWS Migration Hub to fulfill all requirements.
Question 488 of 529
A company hosts its primary API on AWS by using an Amazon API Gateway API and AWS Lambda functions that contain the logic for the API methods. The company’s internal applications use the API for core functionality and business logic. The company’s customers use the API to access data from their accounts. Several customers also have access to a legacy API that is running on a single standalone Amazon EC2 instance.
The company wants to increase the security for these APIs to better prevent denial of service (DoS) attacks, check for vulnerabilities, and guard against common exploits.
What should a solutions architect do to meet these requirements?
A.
Use AWS WAF to protect both APIs. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs.
B.
Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze both APIs. Configure Amazon GuardDuty to block malicious attempts to access the APIs.
C.
Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs.
D.
Use AWS WAF to protect the API Gateway AP! Configure Amazon Inspector to protect the legacy API. Configure Amazon GuardDuty to block malicious attempts to access the APIs.
AnswerDiscussion
Correct Answer: C
To increase security, use AWS WAF to protect the Amazon API Gateway API as it helps to prevent denial of service (DoS) attacks and common exploits. Configure Amazon Inspector to analyze the legacy API running on an EC2 instance to check for vulnerabilities. Finally, use Amazon GuardDuty to monitor for malicious attempts to access both the APIs, as GuardDuty is designed to continuously monitor and analyze for potential threats.
Question 489 of 529
A company is running a serverless ecommerce application on AWS. The application uses Amazon API Gateway to invoke AWS Lambda Java functions. The Lambda functions connect to an Amazon RDS for MySQL database to store data.
During a recent sale event, a sudden increase in web traffic resulted in poor API performance and database connection failures. The company needs to implement a solution to minimize the latency for the Lambda functions and to support bursts in traffic.
Which solution will meet these requirements with the LEAST amount of change to the application?
A.
Update the code of the Lambda functions so that the Lambda functions open the database connection outside of the function handler. Increase the provisioned concurrency for the Lambda functions.
B.
Create an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the provisioned concurrency for the Lambda functions.
C.
Create a custom parameter group. Increase the value of the max_connections parameter. Associate the custom parameter group with the RDS DB instance and schedule a reboot. Increase the reserved concurrency for the Lambda functions.
D.
Create an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the reserved concurrency for the Lambda functions.
AnswerDiscussion
Correct Answer: B
Creating an RDS Proxy endpoint for the database will efficiently manage connection pools to handle bursts of traffic. Additionally, storing database secrets in AWS Secrets Manager and setting up the required IAM permissions ensures a secure and seamless connection. Increasing the provisioned concurrency for the Lambda functions will pre-warm instances to handle sudden traffic surges effectively, minimizing latency. This approach requires the least amount of changes to the existing application while addressing the performance issues and connection failures.
Question 490 of 529
A company requires that all internal application connectivity use private IP addresses. To facilitate this policy, a solutions architect has created interface endpoints to connect to AWS Public services. Upon testing, the solutions architect notices that the service names are resolving to public IP addresses, and that internal services cannot connect to the interface endpoints.
Which step should the solutions architect take to resolve this issue?
A.
Update the subnet route table with a route to the interface endpoint.
B.
Enable the private DNS option on the VPC attributes.
C.
Configure the security group on the interface endpoint to allow connectivity to the AWS services.
D.
Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application.
AnswerDiscussion
Correct Answer: B
Enabling the private DNS option on the VPC attributes ensures that the DNS names of AWS services resolve to the private IP addresses provided by the interface endpoints. This allows internal services within the VPC to connect to those AWS services using private IP addresses, aligning with the company's requirement for internal application connectivity.
Question 491 of 529
A company is developing a latency-sensitive application. Part of the application includes several AWS Lambda functions that need to initialize as quickly as possible. The Lambda functions are written in Java and contain initialization code outside the handlers to load libraries, initialize classes, and generate unique IDs.
Which solution will meet the startup performance requirement MOST cost-effectively?
A.
Move all the initialization code to the handlers for each Lambda function. Activate Lambda SnapStart for each Lambda function. Configure SnapStart to reference the $LATEST version of each Lambda function.
B.
Publish a version of each Lambda function. Create an alias for each Lambda function. Configure each alias to point to its corresponding version. Set up a provisioned concurrency configuration for each Lambda function to point to the corresponding alias.
C.
Publish a version of each Lambda function. Set up a provisioned concurrency configuration for each Lambda function to point to the corresponding version. Activate Lambda SnapStar for the published versions of the Lambda functions.
D.
Update the Lambda functions to add a pre-snapshot hook. Move the code that generates unique IDs into the handlers. Publish a version of each Lambda function. Activate Lambda SnapStart for the published versions of the Lambda functions.
AnswerDiscussion
Correct Answer: D
To meet the startup performance requirement most cost-effectively, the best option is to update the Lambda functions by adding a pre-snapshot hook, move the code that generates unique IDs into the handlers, publish a version of each Lambda function, and activate Lambda SnapStart for the published versions of the Lambda functions. This approach leverages Lambda SnapStart to reduce startup time significantly by pre-initializing the function’s execution environment. It avoids the ongoing cost of provisioned concurrency while still providing a mechanism to minimize latency.
Question 492 of 529
A solutions architect is importing a VM from an on-premises environment by using the Amazon EC2 VM Import feature of AWS Import/Export. The solutions architect has created an AMI and has provisioned an Amazon EC2 instance that is based on that AMI. The EC2 instance runs inside a public subnet in a VPC and has a public IP address assigned.
The EC2 instance does not appear as a managed instance in the AWS Systems Manager console.
Which combination of steps should the solutions architect take to troubleshoot this issue? (Choose two.)
A.
Verify that Systems Manager Agent is installed on the instance and is running.
B.
Verify that the instance is assigned an appropriate IAM role for Systems Manager.
C.
Verify the existence of a VPC endpoint on the VPC.
D.
Verity that the AWS Application Discovery Agent is configured.
E.
Verify the correct configuration of service-linked roles for Systems Manager.
AnswerDiscussion
Correct Answer: A, B
To troubleshoot why the EC2 instance does not appear as a managed instance in the AWS Systems Manager console, the solutions architect should verify that the Systems Manager Agent is installed on the instance and is running. This agent is essential for the instance to communicate with AWS Systems Manager. Additionally, the instance must be assigned an appropriate IAM role for Systems Manager that grants the necessary permissions to interact with Systems Manager services. These two steps are fundamental to ensure proper communication and authorization for the instance to be managed by AWS Systems Manager.
Question 493 of 529
A company is using AWS CloudFormation as its deployment tool for all applications. It stages all application binaries and templates within Amazon S3 buckets with versioning enabled. Developers have access to an Amazon EC2 instance that hosts the integrated development environment (IDE). The developers download the application binaries from Amazon S3 to the EC2 instance, make changes, and upload the binaries to an S3 bucket after running the unit tests locally. The developers want to improve the existing deployment mechanism and implement CI/CD using AWS CodePipeline.
The developers have the following requirements:
• Use AWS CodeCommit for source control.
• Automate unit testing and security scanning.
• Alert the developers when unit tests fail.
• Turn application features on and off, and customize deployment dynamically as part of CI/CD.
• Have the lead developer provide approval before deploying an application.
Which solution will meet these requirements?
A.
Use AWS CodeBuild to run unit tests and security scans. Use an Amazon EventBridge rule to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Cloud Development Kit (AWS CDK) constructs for different solution features, and use a manifest file to tum features on and off in the AWS CDK application. Use a manual approval stage in the pipeline to allow the lead developer to approve applications.
B.
Use AWS Lambda to run unit tests and security scans. Use Lambda in a subsequent stage in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Amplify plugins for different solution features and utilize user prompts to tum features on and off. Use Amazon SES in the pipeline to allow the lead developer to approve applications.
C.
Use Jenkins to run unit tests and security scans. Use an Amazon EventBridge rule in the pipeline to send Amazon SES alerts to the developers when unit tests fail Use AWS CloudFormation nested stacks for different solution features and parameters to turn features on and off. Use AWS Lambda in the pipeline to allow the lead developer to approve applications.
D.
Use AWS CodeDeploy to run unit tests and security scans. Use an Amazon CloudWatch alarm in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Use Docker images for different solution features and the AWS CLI to turn features on and off. Use a manual approval stage in the pipeline to allow the lead developer to approve applications.
AnswerDiscussion
Correct Answer: A
The solution leverages AWS CodeBuild for running unit tests and security scans, which is a service specifically designed for such tasks. It uses Amazon EventBridge rules to send Amazon SNS alerts to developers when unit tests fail, addressing the requirement for alerting. AWS Cloud Development Kit (AWS CDK) constructs and manifest files can be used to turn application features on and off dynamically, satisfying the customization requirement. Finally, a manual approval stage in the pipeline ensures the lead developer can review and approve deployments before they proceed, meeting the requirement for lead developer approval.
Question 494 of 529
A global ecommerce company has many data centers around the world. With the growth of its stored data, the company needs to set up a solution to provide scalable storage for legacy on-premises file applications. The company must be able to take point-in-time copies of volumes by using AWS Backup and must retain low-latency access to frequently accessed data. The company also needs to have storage volumes that can be mounted as Internet Small Computer System Interface (iSCSI) devices from the company’s on-premises application servers.
Which solution will meet these requirements?
A.
Provision an AWS Storage Gateway tape gateway. Configure the tape gateway to store data in an Amazon S3 bucket. Deploy AWS Backup to take point-in-time copies of the volumes.
B.
Provision an Amazon FSx File Gateway and an Amazon S3 File Gateway. Deploy AWS Backup to take point-in-time copies of the data.
C.
Provision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup.
D.
Provision an AWS Storage Gateway file gateway in cache mode. Deploy AWS Backup to take point-in-time copies of the volumes.
AnswerDiscussion
Correct Answer: C
To meet the requirements of providing scalable storage for on-premises file applications, retaining low-latency access to frequently accessed data, allowing point-in-time copy capabilities via AWS Backup, and supporting storage volumes that can be mounted as iSCSI devices, provisioning an AWS Storage Gateway volume gateway in cache mode is appropriate. This setup ensures that the frequently accessed data remains on-premises for low latency while still offering scalability and backup capabilities.
Question 495 of 529
A company has an application that uses AWS Key Management Service (AWS KMS) to encrypt and decrypt data. The application stores data in an Amazon S3 bucket in an AWS Region. Company security policies require the data to be encrypted before the data is placed into the S3 bucket. The application must decrypt the data when the application reads files from the S3 bucket.
The company replicates the S3 bucket to other Regions. A solutions architect must design a solution so that the application can encrypt and decrypt data across Regions. The application must use the same key to decrypt the data in each Region.
Which solution will meet these requirements?
A.
Create a KMS multi-Region primary key. Use the KMS multi-Region primary key to create a KMS multi-Region replica key in each additional Region where the application is running. Update the application code to use the specific replica key in each Region.
B.
Create a new customer managed KMS key in each additional Region where the application is running. Update the application code to use the specific KMS key in each Region.
C.
Use AWS Private Certificate Authority to create a new certificate authority (CA) in the primary Region. Issue a new private certificate from the CA for the application’s website URL. Share the CA with the additional Regions by using AWS Resource Access Manager (AWS RAM). Update the application code to use the shared CA certificates in each Region.
D.
Use AWS Systems Manager Parameter Store to create a parameter in each additional Region where the application is running. Export the key material from the KMS key in the primary Region. Store the key material in the parameter in each Region. Update the application code to use the key data from the parameter in each Region.
AnswerDiscussion
Correct Answer: A
To meet the requirement of encrypting and decrypting data across Regions using the same key, the best solution is to use AWS KMS multi-Region keys. By creating a KMS multi-Region primary key and then creating replica keys in each additional Region, the application can use the same key material in every Region. This approach ensures that data encrypted in one Region can be decrypted in any other Region where the replica key is used. This solution satisfies the security policy of encrypting data before storing it in S3 and allows the application to decrypt it in any Region.
Question 496 of 529
A company hosts an application that uses several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). During the initial startup of the EC2 instances, the EC2 instances run user data scripts to download critical content for the application from an Amazon S3 bucket.
The EC2 instances are launching correctly. However, after a period of time, the EC2 instances are terminated with the following error message: “An instance was taken out of service in response to an ELB system health check failure.” EC2 instances continue to launch and be terminated because of Auto Scaling events in an endless loop.
The only recent change to the deployment is that the company added a large amount of critical content to the S3 bucket. The company does not want to alter the user data scripts in production.
What should a solutions architect do so that the production environment can deploy successfully?
A.
Increase the size of the EC2 instances.
B.
Increase the health check timeout for the ALB.
C.
Change the health check path for the ALB.
D.
Increase the health check grace period for the Auto Scaling group.
AnswerDiscussion
Correct Answer: D
To address the issue effectively, increasing the health check grace period for the Auto Scaling group is the best approach. This allows the EC2 instances additional time to complete their startup tasks, including the download of a significant amount of new content from the S3 bucket, before the health checks are performed by the Application Load Balancer. This ensures that the instances are not terminated prematurely due to failing health checks, promoting stability and continuity in the deployment process without altering existing user data scripts.
Question 497 of 529
A company needs to move some on-premises Oracle databases to AWS. The company has chosen to keep some of the databases on premises for business compliance reasons.
The on-premises databases contain spatial data and run cron jobs for maintenance. The company needs to connect to the on-premises systems directly from AWS to query data as a foreign table.
Which solution will meet these requirements?
A.
Create Amazon DynamoDB global tables with auto scaling enabled. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to DynamoDB. Create an AWS Lambda function to move the spatial data to Amazon S3. Query the data by using Amazon Athena. Use Amazon EventBridge to schedule jobs in DynamoDB for maintenance. Use Amazon API Gateway for foreign table support.
B.
Create an Amazon RDS for Microsoft SQL Server DB instance. Use native replication to move the data from on premises to the DB instance. Use the AWS Schema Conversion Tool (AWS SCT) to modify the SQL Server schema as needed after replication. Move the spatial data to Amazon Redshift. Use stored procedures for system maintenance. Create AWS Glue crawlers to connect to the on-premises Oracle databases for foreign table support.
C.
Launch Amazon EC2 instances to host the Oracle databases. Place the EC2 instances in an Auto Scaling group. Use AWS Application Migration Service to move the data from on premises to the EC2 instances and for real-time bidirectional change data capture (CDC) synchronization. Use Oracle native spatial data support. Create an AWS Lambda function to run maintenance jobs as part of an AWS Step Functions workflow. Create an internet gateway for foreign table support.
D.
Create an Amazon RDS for PostgreSQL DB instance. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to the DB instance. Use PostgreSQL native spatial data support. Run cron jobs on the DB instance for maintenance. Use AWS Direct Connect to connect the DB instance to the on-premises environment for foreign table support.
AnswerDiscussion
Correct Answer: D
The company needs a solution to migrate Oracle databases to AWS while maintaining some databases on-premises for compliance. It also requires querying on-premises data from AWS and managing spatial data with cron jobs for maintenance. Creating an Amazon RDS for PostgreSQL DB instance meets these requirements effectively. PostgreSQL has native support for spatial data and can run cron jobs for maintenance. Using the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) enables a smooth migration from on-premises to AWS. AWS Direct Connect ensures a reliable and secure connection between the AWS environment and on-premises systems, allowing querying of foreign tables.
Question 498 of 529
Accompany runs an application on Amazon EC2 and AWS Lambda. The application stores temporary data in Amazon S3. The S3 objects are deleted after 24 hours.
The company deploys new versions of the application by launching AWS CloudFormation stacks. The stacks create the required resources. After validating a new version, the company deletes the old stack. The deletion of an old development stack recently failed. A solutions architect needs to resolve this issue without major architecture changes.
Which solution will meet these requirements?
A.
Create a Lambda function to delete objects from an S3 bucket. Add the Lambda function as a custom resource in the CloudFormation stack with a DependsOn attribute that points to the S3 bucket resource.
B.
Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket.
C.
Update the CloudFormation stack to add a DeletionPolicy attribute with a value of Snapshot for the S3 bucket resource
D.
Update the CloudFormation template to create an Amazon Elastic File System (Amazon EFS) file system to store temporary files instead of Amazon S3. Configure the Lambda functions to run in the same VPC as the EFS file system.
AnswerDiscussion
Correct Answer: A
To resolve the issue of CloudFormation stack deletion failing due to non-empty S3 buckets, the best solution is to create a Lambda function that deletes objects from the S3 bucket. This Lambda function should be added as a custom resource in the CloudFormation stack with a DependsOn attribute pointing to the S3 bucket resource. This ensures that the Lambda function is invoked to empty the bucket before CloudFormation attempts to delete it, thereby preventing any deletion failures.
Question 499 of 529
A company has an application that stores user-uploaded videos in an Amazon S3 bucket that uses S3 Standard storage. Users access the videos frequently in the first 180 days after the videos are uploaded. Access after 180 days is rare. Named users and anonymous users access the videos.
Most of the videos are more than 100 MB in size. Users often have poor internet connectivity when they upload videos, resulting in failed uploads. The company uses multipart uploads for the videos.
A solutions architect needs to optimize the S3 costs of the application.
Which combination of actions will meet these requirements? (Choose two.)
A.
Configure the S3 bucket to be a Requester Pays bucket.
B.
Use S3 Transfer Acceleration to upload the videos to the S3 bucket.
C.
Create an S3 Lifecycle configuration o expire incomplete multipart uploads 7 days after initiation.
D.
Create an S3 Lifecycle configuration to transition objects to S3 Glacier Instant Retrieval after 1 day.
E.
Create an S3 Lifecycle configuration to transition objects to S3 Standard-infrequent Access (S3 Standard- IA) after 180 days.
AnswerDiscussion
Correct Answer: C, E
To optimize the S3 costs, two actions are appropriate. Firstly, creating an S3 Lifecycle configuration to expire incomplete multipart uploads 7 days after initiation will ensure that storage space is not unnecessarily occupied by failed uploads, thereby saving costs. Secondly, transitioning objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days aligns with the access pattern where videos are frequently accessed in the first 180 days and rarely accessed afterward. This will reduce storage costs for data that is infrequently accessed.
Question 500 of 529
A company runs an ecommerce web application on AWS. The web application is hosted as a static website on Amazon S3 with Amazon CloudFront for content delivery. An Amazon API
Gateway API invokes AWS Lambda functions to handle user requests and order processing for the web application The Lambda functions store data in an Amazon ROS for MySQL DB cluster that uses On-Demand instances. The DB cluster usage has been consistent in the past 12 months.
Recently, the website has experienced SQL injection and web exploit attempts. Customers also report that order processing time has increased during periods of peak usage. During these periods, the Lambda functions often have cold starts. As the company grows, the company needs to ensure scalability and low-latency access during traffic peaks. The company also must optimize the database costs and add protection against the SQL injection and web exploit attempts.
Which solution will meet these requirements?
A.
Configure the Lambda functions to have an increased timeout value during peak periods. Use RDS Reserved Instances for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts.
B.
Increase the memory of the Lambda functions, Transition to Amazon Redshift for the database. Integrate Amazon Inspector with CloudFront to protect against the SQL injection and web exploit attempts.
C.
Use Lambda functions with provisioned concurrency for compute during peak periods, Transition to Amazon Aurora Serverless for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts.
D.
Use Lambda functions with provisioned concurrency for compute during peak periods. Use RDS Reserved Instances for the database. Integrate AWS WAF with CloudFront to protect against the SQL injection and web exploit attempts.
AnswerDiscussion
Correct Answer: D
Using AWS Lambda's provisioned concurrency ensures that the functions are pre-warmed and ready to handle requests, thus minimizing cold starts and improving order processing times during peak usage. For the database, Reserved Instances for Amazon RDS provide a cost-effective solution compared to On-Demand instances, given the consistent usage pattern. To protect against SQL injection and web exploit attempts, AWS WAF (Web Application Firewall) integrated with CloudFront is the recommended service as it specifically addresses such threats, rather than AWS Shield Advanced which is more focused on DDoS protection.
Question 501 of 529
A company runs a web application on a single Amazon EC2 instance. End users experience slow application performance during times of peak usage, when CPU utilization is consistently more than 95%.
A user data script installs required custom packages on the EC2 instance. The process of launching the instance takes several minutes.
The company is creating an Auto Scaling group that has mixed instance groups, varied CPUs, and a maximum capacity limit. The Auto Scaling group will use a launch template for various configuration options. The company needs to decrease application latency when new instances are launched during auto scaling.
Which solution will meet these requirements?
A.
Use a predictive scaling policy. Use an instance maintenance policy to run the user data script. Set the default instance warmup time to 0 seconds.
B.
Use a dynamic scaling policy. Use lifecycle hooks to run the user data script. Set the default instance warmup time to 0 seconds.
C.
Use a predictive scaling policy. Enable warm pools for the Auto Scaling group. Use an instance maintenance policy to run the user data script.
D.
Use a dynamic scaling policy. Enable warm pools for the Auto Scaling group. Use lifecycle hooks to run the user data script.
AnswerDiscussion
Correct Answer: D
To meet the requirements of decreasing application latency when new instances are launched during auto scaling, using a dynamic scaling policy is essential for real-time adjustment based on demand. Enabling warm pools allows instances to be pre-initialized and kept in a warmed state, reducing the time it takes to serve traffic once they are needed. Lifecycle hooks allow custom scripts to run during the instance launch, ensuring all necessary packages are installed without delaying the instance's readiness to handle traffic. This combination effectively minimizes application latency during scale-out events.
Question 502 of 529
A company needs to migrate its on-premises database fleet to Amazon RDS. The company is currently using a mixture of Microsoft SQL Server, MySQL, and Oracle databases. Some of the databases have custom schemas and stored procedures.
Which combination of steps should the company take for the migration? (Choose two.)
A.
Use Migration Evaluator Quick Insights to analyze the source databases and to identify the stored procedures that need to be migrated.
B.
Use AWS Application Migration Service to analyze the source databases and to identify the stored procedures that need to be migrated.
C.
Use the AWS Schema Conversion Tool (AWS SCT) to analyze the source databases for changes that are required
D.
Use AWS Database Migration Service (AWS DMS) to migrate the source databases to Amazon RDS.
E.
Use AWS DataSync to migrate the data from the source databases to Amazon RDS.
AnswerDiscussion
Correct Answer: C, D
To migrate a variety of databases with custom schemas and stored procedures to Amazon RDS, the company should use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS). AWS SCT can analyze the source databases and identify necessary changes, including schema and stored procedure modifications. AWS DMS is designed to facilitate the migration of databases to Amazon RDS, ensuring that data is transferred accurately and efficiently. This combination ensures comprehensive analysis and effective migration of the databases.
Question 503 of 529
A company is migrating its blog platform to AWS. The company's on-premises servers connect to AWS through an AWS Site-to-Site VPN connection. The blog content is updated several times a day by multiple authors and is served from a file share on a network-attached storage (NAS) server.
The company needs to migrate the blog platform without delaying the content updates. The company has deployed Amazon EC2 instances across multiple Availability Zones to run the blog platform behind an Application Load Balancer. The company also needs to move 200 TB of archival data from its on-premises servers to Amazon S3 as soon as possible.
Which combination of stops will meet these requirements? (Choose two.)
A.
Create a weekly cron job in Amazon EventBridge. Use the cron job to invoke an AWS Lambda function to update the EC2 instances from the NAS server.
B.
Configure an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume for the EC2 instances to share for content access. Write code to synchronize the EBS volume with the NAS server weekly.
C.
Mount an Amazon Elastic File System (Amazon EFS) file system to the on-premises servers to act as the NAS server. Copy the blog data to the EFS file system. Mount the EFS file system to the C2 instances to serve the content.
D.
Order an AWS Snowball Edge Storage Optimized device. Copy the static data artifacts to the device. Ship the device to AWS.
E.
Order an AWS Snowcons SSD device. Copy the static data artifacts to the device. Ship the device to AWS.
AnswerDiscussion
Correct Answer: C, D
To meet the requirements, the company should use Amazon EFS for the blog content and AWS Snowball Edge for transferring the archival data. Amazon EFS provides a scalable file storage solution that can be mounted to both on-premises servers and EC2 instances, ensuring immediate content updates without synchronization delays. AWS Snowball Edge is designed for large-scale data transfers and can efficiently move 200 TB of archival data to Amazon S3.
Question 504 of 529
A company plans to migrate a legacy on-premises application to AWS. The application is a Java web application that runs on Apache Tomcat with a PostgreSQL database.
The company does not have access to the source code but can deploy the application Java Archive (JAR) files. The application has increased traffic at the end of each month.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Launch Amazon EC2 instances in multiple Availability Zones. Deploy Tomcat and PostgreSQL to all the instances by using Amazon Elastic File System (Amazon EFS) mount points. Use AWS Step Functions to deploy additional EC2 instances to scale for increased traffic.
B.
Provision Amazon Elastic Kubernetes Service (Amazon EKS) in an Auto Scaling group across multiple AWS Regions. Deploy Tomcat and PostgreSQL in the container images. Use a Network Load Balancer to scale for increased traffic.
C.
Refactor the Java application into Python-based containers. Use AWS Lambda functions for the application logic. Store application data in Amazon DynamoDB global tables. Use AWS Storage Gateway and Lambda concurrency to scale for increased traffic.
D.
Use AWS Elastic Beanstalk to deploy the Tomcat servers with auto scaling in multiple Availability Zones. Store application data in an Amazon RDS for PostgreSQL database. Deploy Amazon CloudFront and an Application Load Balancer to scale for increased traffic.
AnswerDiscussion
Correct Answer: D
AWS Elastic Beanstalk provides a managed platform that automatically handles the deployment, capacity provisioning, load balancing, and auto-scaling for the Java web application. This significantly reduces operational overhead. Storing application data in Amazon RDS for PostgreSQL ensures managed database operations, including automated backups and scaling. Additionally, using Amazon CloudFront and an Application Load Balancer helps manage increased traffic and provides low latency. This solution meets the requirements with the least operational overhead given the company's constraints.
Question 505 of 529
A company is migrating its on-premises IoT platform to AWS. The platform consists of the following components:
• A MongoDB cluster as a data store for all collected and processed IoT data.
• An application that uses Message Queuing Telemetry Transport (MQTT) to connect to IoT devices every 5 minutes to collect data.
• An application that runs jobs periodically to generate reports from the IoT data. The jobs take 120-600 seconds to finish running.
• A web application that runs on a web server. End users use the web application to generate reports that are accessible to the general public.
The company needs to migrate the platform to AWS to reduce operational overhead while maintaining performance.
Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)
A.
Create AWS Step Functions state machines with AUS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Configure an Amazon CloudFront distribution that has an S3 origin to serve the reports
B.
Create an AWS Lambda function. Program the Lambda function to connect to the IoT devices. process the data, and write the data to the data store. Configure a Lambda layer to temporarily store messages for processing.
C.
Configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Create an ingress controller on the EKS cluster to serve the reports.
D.
Connect the IoT devices to AWS IoT Core to publish messages. Create an AWS IoT rule that runs when a message is received. Configure the rule to call an AWS Lambda function. Program the Lambda function to parse, transform, and store device message data to the data store.
E.
Migrate the MongoDB cluster to Amazon DocumentDB (with MongoDB compatibility).
F.
Migrate the MongoDB cluster to Amazon EC2 instances.
AnswerDiscussion
Correct Answer: A, D, E
To meet the requirements with the least operational overhead, the following steps are appropriate: Use AWS Step Functions with AWS Lambda to prepare and store reports in Amazon S3 and configure Amazon CloudFront to serve these reports. This combination efficiently handles the periodic job requirements while minimizing management efforts. Connect IoT devices to AWS IoT Core, which facilitates MQTT messaging and can trigger AWS Lambda functions to process and store data, providing an efficient and scalable solution for handling IoT data. Migrate the MongoDB cluster to Amazon DocumentDB (with MongoDB compatibility) to eliminate the operational overhead of managing MongoDB, while maintaining the same functionalities and performance required for the IoT data store.
Question 506 of 529
A company creates an Amazon API Gateway API and shares the API with an external development team. The API uses AWS Lambda functions and is deployed to a stage that is named Production.
The external development team is the sole consumer of the API. The API experiences sudden increases of usage at specific times, leading to concerns about increased costs. The company needs to limit cost and usage without reworking the Lambda functions.
Which solution will meet these requirements MOST cost-effectively?
A.
Configure the API to send requests to Amazon Simple Queue Service (Amazon SQS) queues instead of directly to the Lambda functions. Update the Lambda functions to consume messages from the queues and to process the requests. Set up the queues to invoke the Lambda functions when new messages arrive.
B.
Configure provisioned concurrency for each Lambda function. Use AWS Application Auto Scaling to register the Lambda functions as targets. Set up scaling schedules to increase and decrease capacity to match changes in API usage.
C.
Create an API Gateway API key and an AWS WAF Regional web ACL. Associate the web ACL with the Production stage. Add a rate-based rule to the web ACL. In the rule, specify the rate limit and a custom request aggregation that uses the X-API-Key header. Share the API key with the external development team.
D.
Create an API Gateway API Key and usage plan. Define throttling limits and quotas in the usage plan. Associate the usage plan with the Production stage and the API key. Share the API key with the external development team.
AnswerDiscussion
Correct Answer: D
Creating an API Gateway API Key and usage plan allows you to control and limit the usage of the API. The usage plan lets you define throttling limits (requests per second) and quotas (total requests per day or month), ensuring that the API usage stays within the desired boundaries. This approach helps control costs by limiting the number of requests handled by the Lambda functions without requiring any changes to the existing Lambda functions or the overall architecture.
Question 507 of 529
An entertainment company hosts a ticketing service on a fleet of Linux Amazon EC2 instances that are in an Auto Scaling group. The ticketing service uses a pricing file. The pricing file is stored in an Amazon S3 bucket that has S3 Standard storage. A central pricing solution that is hosted by a third party updates the pricing file.
The pricing file is updated every 1-15 minutes and has several thousand line items. The pricing file is downloaded to each EC2 instance when the instance launches.
The EC2 instances occasionally use outdated pricing information that can result in incorrect charges for customers.
Which solution will resolve this problem MOST cost-effectively?
A.
Create an AWS Lambda function to update an Amazon DynamoDB table with new prices each time the pricing file is updated. Update the ticketing service to use DynramoDB to look up pricing
B.
Create an AWS Lambda function to update an Amazon Elastic File System (Amazon EFS) file share with the pricing file each time the file is updated. Update the ticketing service to use Amazon EFS to access the pricing file.
C.
Load Mountpoint for Amazon S3 onto the AMI of the EC2 instances. Configure Mountpoint for Amazon S3 to mount the S3 bucket that contains the pricing file. Update the ticketing service to point to the mount point and path to access the $3 object,
D.
Create an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS Multi-Attach to attach the volume to every EC2 instance. When a new EC2 instance launches, configure the new instance to update the pricing file on the EBS volume. Update the ticketing service to point to the new local source.
AnswerDiscussion
Correct Answer: C
The most cost-effective solution is to load Mountpoint for Amazon S3 onto the AMI of the EC2 instances and configure it to mount the S3 bucket that contains the pricing file. This approach allows the EC2 instances to directly access the S3 bucket as if it were a local file system, ensuring that the instances always access the latest version of the pricing file without needing to download it each time. This avoids the cost associated with repeatedly downloading the file and minimizes the risk of using outdated pricing information. Additionally, this solution simplifies the architecture by eliminating the need for additional data storage and synchronization mechanisms.
Question 508 of 529
A company has an application that uses Amazon EC2 instances in an Auto Scaling group. The quality assurance (QA) department needs to launch a large number of short-lived environments to test the application. The application environments are currently launched by the manager of the department using an AWS CloudFormation template. To launch the stack, the manager uses a role with permission to use CloudFormation, EC2, and Auto Scaling APIs. The manager wants to allow testers to launch their own environments, but does not want to grant broad permissions to each user.
Which set up would achieve these goals?
A.
Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to assume the manager’s role and add a policy that restricts the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console.
B.
Create an AWS Service Catalog product from the environment template. Add a launch constraint to the product with the existing role. Give users in the QA department permission to use AWS Service Catalog APIs only. Train users to launch the template from the AWS Service Catalog console.
C.
Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to use CloudFormation and S3 APIs, with conditions that restrict the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console.
D.
Create an AWS Elastic Beanstalk application from the environment template. Give users in the QA department permission to use Elastic Beanstalk permissions only. Train users to launch Elastic Beanstalk environments with the Elastic Beanstalk CLI, passing the existing role to the environment as a service role.
AnswerDiscussion
Correct Answer: B
Creating an AWS Service Catalog product from the environment template and adding a launch constraint to the product with the existing role achieves the goal of allowing testers to launch their own environments without granting them broad permissions. By giving users in the QA department permission to use AWS Service Catalog APIs only, the setup ensures that users can only launch the specified environment template, maintaining the needed security and management control.
Question 509 of 529
A company is using a single AWS Region for its ecommerce website. The website includes a web application that runs on several Amazon EC2 instances behind an Application Load Balancer (ALB). The website also includes an Amazon DynamoDB table. A custom domain name in Amazon Route 53 is linked to the ALB. The company created an SSL/TLS certificate in AWS Certificate Manager (ACM) and attached the certificate to the ALB. The company is not using a content delivery network as part of its design.
The company wants to replicate its entire application stack in a second Region to provide disaster recovery, plan for future growth, and provide improved access time to users. A solutions architect needs to implement a solution that achieves these goals and minimizes administrative overhead.
Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)
A.
Create an AWS CloudFormation template for the current infrastructure design. Use parameters for important system values, including Region. Use the CloudFormation template to create the new infrastructure in the second Region.
B.
Use the AWS Management Console to document the existing infrastructure design in the first Region and to create the new infrastructure in the second Region.
C.
Update the Route 53 hosted zone record for the application to use weighted routing. Send 50% of the traffic to the ALB in each Region.
D.
Update the Route 53 hosted zone record for the application to use latency-based routing. Send traffic to the ALB in each Region.
E.
Update the configuration of the existing DynamoDB table by enabling DynamoDB Streams. Add the second Region to create a global table.
F.
Create a new DynamoDB table. Enable DynamoDB Streams for the new table. Add the second Region to create a global table. Copy the data from the existing DynamoDB table to the new table as a one-time operation.
AnswerDiscussion
Correct Answer: A, D, E
To replicate the entire application stack in a second Region with minimal administrative overhead, the solutions architect should take the following steps: Create an AWS CloudFormation template for the current infrastructure design to easily replicate it in other Regions. Update the Route 53 hosted zone record for the application to use latency-based routing which directs traffic based on the lowest latency to users, ensuring improved access time. Enable DynamoDB Streams for the existing DynamoDB table and create a global table to ensure the data is replicated across both Regions smoothly and efficiently.
Question 510 of 529
A company wants to create a single Amazon S3 bucket for its data scientists to store work-related documents. The company uses AWS IAM Identity Center to authenticate all users. A group for the data scientists was created.
The company wants to give the data scientists access to only their own work. The company also wants to create monthly reports that show which documents each user accessed.
Which combination of steps will meet these requirements? (Choose two.)
A.
Create a custom IAM Identity Center permission set to grant the data scientists access to an S3 bucket prefix that matches their username tag. Use a policy to limit access to paths with the ${aws:PrincipalTag/userName}/* condition.
B.
Create an IAM Identity Center role for the data scientists group that has Amazon S3 read access and write access. Add an S3 bucket policy that allows access to the IAM Identity Center role.
C.
Configure AWS CloudTrail to log S3 data events and deliver the logs to an S3 bucket. Use Amazon Athena to run queries on the CloudTrail logs in Amazon S3 and generate reports.
D.
Configure AWS CloudTrail to log S3 management events to CloudWatch. Use Amazon Athena’s CloudWatch connector to query the logs and generate reports.
E.
Enable S3 access logging to EMR File System (EMRFS). Use Amazon S3 Select to query logs and generate reports.
AnswerDiscussion
Correct Answer: A, C
To meet the company's requirements, two steps need to be implemented: controlling access and generating access reports. Creating a custom IAM Identity Center permission set allows data scientists to access only their own work within the S3 bucket by using a policy that limits access based on their username tag. This satisfies the requirement of allowing access to only their own work. Configuring AWS CloudTrail to log S3 data events and delivering the logs to an S3 bucket enables tracking of which documents each user accessed. Using Amazon Athena to query these logs and generate reports satisfies the requirement of creating monthly access reports.
Question 511 of 529
A company hosts a data-processing application on Amazon EC2 instances. The application polls an Amazon Elastic File System (Amazon EFS) file system for newly uploaded files. When a new file is detected, the application extracts data from the file and runs logic to select a Docker container image to process the file. The application starts the appropriate container image and passes the file location as a parameter.
The data processing that the container performs can take up to 2 hours. When the processing is complete, the code that runs inside the container writes the file back to Amazon EFS and exits.
The company needs to refactor the application to eliminate the EC2 instances that are running the containers.
Which solution will meet these requirements?
A.
Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an Amazon EventBridge rule that starts the appropriate Fargate task. Configure the EventBridge rule to run when files are added to the EFS file system.
B.
Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Update and containerize the container selection logic to run as a Fargate service that starts the appropriate Fargate task. Configure an EFS event notification to invoke the Fargate service when files are added to the EFS file system.
C.
Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an AWS Lambda function that starts the appropriate Fargate task. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the Lambda function when objects are created.
D.
Create AWS Lambda container images for the processing. Configure Lambda functions to use the container images. Extract the container selection logic to run as a decision Lambda function that invokes the appropriate Lambda processing function. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the decision Lambda function when objects are created.
AnswerDiscussion
Correct Answer: C
To meet the requirements of eliminating EC2 instances and effectively handling long-running data processing tasks, creating an Amazon Elastic Container Service (ECS) cluster and configuring processing to run as AWS Fargate tasks is appropriate. Migrating the storage of file uploads to Amazon S3 allows you to leverage S3 event notifications, which can invoke an AWS Lambda function for the container selection logic. The Lambda function can then start the appropriate Fargate task. This approach enables automatic triggering upon file upload and ensures that the storage system supports event notifications.
Question 512 of 529
A media company has a 30-T8 repository of digital news videos. These videos are stored on tape in an on-premises tape library and referenced by a Media Asset Management (MAM) system. The company wants to enrich the metadata for these videos in an automated fashion and put them into a searchable catalog by using a MAM feature. The company must be able to search based on information in the video, such as objects, scenery items, or people’s faces. A catalog is available that contains faces of people who have appeared in the videos that include an image of each person. The company would like to migrate these videos to AWS.
The company has a high-speed AWS Direct Connect connection with AWS and would like to move the MAM solution video content directly from its current file system.
How can these requirements be met by using the LEAST amount of ongoing management overhead and causing MINIMAL disruption to the existing system?
A.
Set up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the file gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Rekognition pull the video from the Amazon S3 files backing the file gateway, retrieve the required metadata, and push the metadata into the MAM solution.
B.
Set up an AWS Storage Gateway, tape gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the tape gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video in the tape gateway, retrieve the required metadata, and push the metadata into the MAM solution.
C.
Configure a video ingestion stream by using Amazon Kinesis Video Streams. Use the catalog of faces to build a collection in Amazon Rekognition. Stream the videos from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed videos. Then, use a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Configure the stream to store the videos in Amazon S3.
D.
Set up an Amazon EC2 instance that runs the OpenCV libraries. Copy the videos, images, and face catalog from the on-premises library into an Amazon EBS volume mounted on this EC2 instance. Process the videos to retrieve the required metadata, and push the metadata into the MAM solution, while also copying the video files to an Amazon S3 bucket.
AnswerDiscussion
Correct Answer: A
To meet the company's requirements with the least amount of ongoing management overhead and minimal disruption to the existing system, using an AWS Storage Gateway file gateway appliance on-premises is a suitable approach. This setup allows the MAM solution to extract videos from the current archive and push them into the file gateway. The file gateway will then store the data in Amazon S3, where Amazon Rekognition can access and process the videos to retrieve the required metadata. This solution leverages AWS's managed services, reducing the need for extensive ongoing management while ensuring a seamless integration with the existing system.
Question 513 of 529
A company needs to optimize the cost of an AWS environment that contains multiple accounts in an organization in AWS Organizations. The company conducted cost optimization activities 3 years ago and purchased Amazon EC2 Standard Reserved Instances that recently expired.
The company needs EC2 instances for 3 more years. Additionally, the company has deployed a new serverless workload.
Which strategy will provide the company with the MOST cost savings?
A.
Purchase the same Reserved Instances for an additional 3-year term with All Upfront payment. Purchase a 3-year Compute Savings Plan with All Upfront payment in the management account to cover any additional compute costs
B.
Purchase a 1-year Compute Savings Plan with No Upfront payment in each member account. Use the Savings Plans recommendations in the AWS Cost Management console to choose the Compute Savings Plan.
C.
Purchase a 3-year EC2 Instance Savings Plan with No Upfront payment in the management account to cover EC2 costs in each AWS Region. Purchase a 3-year Compute Savings Plan with No Upfront payment in the management account to cover any additional compute costs.
D.
Purchase a 3-year EC2 Instance Savings Plan with All Upfront payment in each member account. Use the Savings Plans recommendations in the AWS Cost Management console to choose the EC2 Instance Savings Plan.
AnswerDiscussion
Correct Answer: A
To optimize cost for the next 3 years while considering the new serverless workload, the best approach is to purchase Amazon EC2 Standard Reserved Instances for another 3-year term with All Upfront payment to take advantage of the maximum discount. Additionally, purchasing a 3-year Compute Savings Plan with All Upfront payment in the management account will cover any additional compute costs including the new serverless workload, ensuring that the company benefits from the most cost savings.
Question 514 of 529
A company operates a static content distribution platform that serves customers globally. The customers consume content from their own AWS accounts.
The company serves its content from an Amazon S3 bucket. The company uploads the content from its on-premises environment to the S3 bucket by using an S3 File Gateway.
The company wants to improve the platform’s performance and reliability by serving content from the AWS Region that is geographically closest to customers. The company must route the on-premises data to Amazon S3 with minimal latency and without public internet exposure.
Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)
A.
Implement S3 Multi-Region Access Points
B.
Use S3 Cross-Region Replication (CRR) to copy content to different Regions
C.
Create an AWS Lambda function that tracks the routing of clients to Regions
D.
Use an AWS Site-to-Site VPN connection to connect to a Multi-Region Access Point.
E.
Use AWS PrivateLink and AWS Direct Connect to connect to a Multi-Region Access Point.
AnswerDiscussion
Correct Answer: A, E
To meet the requirements, the company should use S3 Multi-Region Access Points to improve performance and reliability by serving content from the AWS Region that is geographically closest to customers. This ensures low latency and high availability. Additionally, AWS PrivateLink and AWS Direct Connect should be used to securely connect the on-premises environment to the S3 Multi-Region Access Point without exposing data to the public internet. This setup minimizes latency and enhances security.
Question 515 of 529
A company is migrating its data center to the AWS Cloud and needs to complete the migration as quickly as possible. The company has many applications that are running on hundreds of VMware VMs in the data center. Each VM is configured with a shared Windows folder that contains common shared files. The file share is larger than 100 GB in size.
The company’s compliance team requires a change request to be fled and approved for every software installation and modification to each VM. The company has an AWS Direct Connect connection with 10 GB of bandwidth between AWS and the data center.
Which set of steps should the company take to complete the migration in the LEAST amount of time?
A.
Use VM ImporvExport to create images of each VM. Use AWS Application Migration Service to manage and view the images. Copy the Windows file share data to an Amazon Elastic File System (Amazon EFS) file system. After migration, remap the file share to the EFS file system.
B.
Deploy the AWS Application Discovery Service agentless appliance to VMware vCenter. Review the portfolio of discovered VMs in AWS Migration Hub.
C.
Deploy the AWS Application Migration Service agentless appliance to VMware vCenter. Copy the Windows file share data to a new Amazon FSx for Windows File Server file system. After migration, remap the file share on each VM to the FSx for Windows File Server file system.
C. Create and review a portfolio in AWS Migration Hub. Order an AWS Snowcone device. Deploy AWS Application Migration Service to VMware vCenter and export all the VMs to the Snowcone device. Copy all Windows file share data to the Snowcone device. Ship the Snowcone device to AWS. Use Application Migration Service to deploy all the migrated instances.
D.
Deploy the AWS Application Discovery Service Agent and the AWS Application Migration Service Agent onto each VMware hypervisor directly. Review the portfolio in AWS Migration Hub. Copy each VM’s file share data to a new Amazon FSx for Windows File Server file system. After migration, remap the file share on each VM to the FSx for Windows File Server file system.
AnswerDiscussion
Correct Answer: C
To migrate the company's VMware VMs to AWS in the least amount of time, they should use an agentless solution to minimize the installation and configuration effort on existing VMs. By deploying the AWS Application Migration Service agentless appliance to VMware vCenter, the company can migrate workloads without needing to modify each VM individually. After this, copying the Windows file share data to a new Amazon FSx for Windows File Server will allow the company to maintain compatibility with existing shared file configurations. This approach leverages the 10 GB bandwidth of AWS Direct Connect for efficient data transfer and satisfies the compliance requirement by avoiding direct modifications to VMs.
Question 516 of 529
A company has multiple AWS accounts that are in an organization in AWS Organizations. The company needs to store AWS account activity and query the data from a central location by using SQL.
Which solution will meet these requirements?
A.
Create an AWS CloudTraii trail in each account. Specify CloudTrail management events for the trail. Configure CloudTrail to send the events to Amazon CloudWatch Logs. Configure CloudWatch cross-account observability. Query the data in CloudWatch Logs Insights.
B.
Use a delegated administrator account to create an AWS CloudTrail Lake data store. Specify CloudTrail management events for the data store. Enable the data store for all accounts in the organization. Query the data in CloudTrail Lake.
C.
Use a delegated administrator account to create an AWS CloudTral trail. Specify CloudTrail management events for the trail. Enable the trail for all accounts in the organization. Keep all other settings as default. Query the CloudTrail data from the CloudTrail event history page.
D.
Use AWS CloudFormation StackSets to deploy AWS CloudTrail Lake data stores in each account. Specify CloudTrail management events for the data stores. Keep all other settings as default, Query the data in CloudTrail Lake.
AnswerDiscussion
Correct Answer: B
To meet the requirements of storing AWS account activity and querying the data from a central location using SQL, the best solution is to use a delegated administrator account to create an AWS CloudTrail Lake data store. In CloudTrail Lake, you can specify CloudTrail management events for the data store and enable it for all accounts in the organization. This allows centralized collection and storage of account activity, and provides the ability to query this data using SQL-based queries, which aligns perfectly with the stated needs.
Question 517 of 529
A company is using AWS to develop and manage its production web application. The application includes an Amazon API Gateway HTTP API that invokes an AWS Lambda function. The Lambda function processes and then stores data in a database.
The company wants to implement user authorization for the web application in an integrated way. The company already uses a third-party identity provider that issues OAuth tokens for the company’s other applications.
Which solution will meet these requirements?
A.
Integrate the company’s third-party identity provider with API Gateway. Configure an API Gateway Lambda authorizer to validate tokens from the identity provider. Require the Lambda authorizer on all API routes. Update the web application to get tokens from the identity provider and include the tokens in the Authorization header when calling the API Gateway HTTP API.
B.
Integrate the company's third-party identity provider with AWS Directory Service. Configure Directory Service as an API Gateway authorizer to validate tokens from the identity provider. Require the Directory Service authorizer on all API routes. Configure AWS IAM Identity Center as a SAML 2.0 identity Provider. Configure the web application as a custom SAML 2.0 application.
C.
Integrate the company’s third-party identity provider with AWS IAM Identity Center. Configure API Gateway to use IAM Identity Center for zero-configuration authentication and authorization. Update the web application to retrieve AWS Security Token Service (AWS STS) tokens from IAM Identity Center and include the tokens in the Authorization header when calling the API Gateway HTTP API.
D.
Integrate the company’s third-party identity provider with AWS IAM Identity Center. Configure IAM users with permissions to call the API Gateway HTTP API. Update the web application to extract request parameters from the IAM users and include the parameters in the Authorization header when calling the API Gateway HTTP API.
AnswerDiscussion
Correct Answer: A
Integrating the company's third-party identity provider with API Gateway and configuring an API Gateway Lambda authorizer to validate tokens from the identity provider is the best approach. This allows for seamless integration of the existing identity provider, which issues OAuth tokens, with the API Gateway. The Lambda authorizer can validate these tokens, ensuring secure user authorization. Requiring the Lambda authorizer on all API routes and updating the web application to include tokens in the Authorization header ensures that every API request is properly authenticated.
Question 518 of 529
A company has deployed applications to thousands of Amazon EC2 instances in an AWS account. A security audit discovers that several unencrypted Amazon Elastic Block Store (Amazon EBS) volumes are attached to the EC2 instances. The company’s security policy requires the EBS volumes to be encrypted.
The company needs to implement an automated solution to encrypt the EBS volumes. The solution also must prevent development teams from creating unencrypted EBS volumes.
Which solution will meet these requirements?
A.
Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an AWS Key Management Service (AWS KMS) customer managed key. In the key policy, include a statement to deny the creation of unencrypted EBS volumes.
B.
Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes, Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an SCP to deny the creation of unencrypted EBS volumes.
C.
Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes. Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes.
D.
Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes.
AnswerDiscussion
Correct Answer: D
The correct approach involves using the AWS Config managed rule to identify unencrypted EBS volumes, as it continually monitors and assesses compliance with desired configurations. Enabling automatic remediation and associating an AWS Systems Manager Automation runbook ensures that any unencrypted volumes found are promptly encrypted. By modifying the AWS account setting for EBS encryption to default to encryption, it prevents future creation of unencrypted EBS volumes. This solution meets both the identification and prevention requirements.
Question 519 of 529
A company is running a large containerized workload in the AWS Cloud. The workload consists of approximately 100 different services. The company uses Amazon Elastic Container Service (Amazon ECS) to orchestrate the workload.
Recently the company’s development team started using AWS Fargate instead of Amazon EC2 instances in the ECS cluster. In the past, the workload has come close to running the maximum number of EC2 instances that are available in the account.
The company is worried that the workload could reach the maximum number of ECS tasks that are allowed. A solutions architect must implement a solution that will notify the development team when Fargate reaches 80% of the maximum number of tasks.
What should the solutions architect do to meet this requirement?
A.
Use Amazon CloudWatch to monitor the Sample Count statistic for each service in the ECS cluster. Set an alarm for when the math expression sample count/SERVICE_QUOTA(service)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).
B.
Use Amazon CloudWatch to monitor service quotas that are published under the AWS/Usage metric namespace. Set an alarm for when the math expression metric/SERVICE_QUOTA(metric)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).
C.
Create an AWS Lambda function to poll detailed metrics from the ECS cluster. When the number of running Fargate tasks is greater than 80, invoke Amazon Simple Email Service (Amazon SES) to notify the development team.
D.
Create an AWS Config rule to evaluate whether the Fargate SERVICE_QUOTA is greater than 80. Use Amazon Simple Email Service (Amazon SES) to notify the development team when the AWS Config rule is not compliant.
AnswerDiscussion
Correct Answer: B
In this scenario, the best approach to monitor and alert the development team is to use Amazon CloudWatch to monitor service quotas published under the AWS/Usage metric namespace. This method allows setting up an alarm for when the usage metric exceeds 80% of the service quota. Using CloudWatch metrics is a reliable and efficient way to track this type of data, and by employing a math expression that compares the metric with the SERVICE_QUOTA, you can get the required percentage. Moreover, integrating this with Amazon Simple Notification Service (SNS) ensures that the development team is promptly notified. This solution is both comprehensive and straightforward to implement, meeting the company's requirements effectively.
Question 520 of 529
A company has several AWS Lambda functions written in Python. The functions are deployed with the .zip package deployment type. The functions use a Lambda layer that contains common libraries and packages in a .zip file. The Lambda .zip packages and Lambda layer .zip file are stored in an Amazon S3 bucket.
The company must implement automatic scanning of the Lambda functions and the Lambda layer to identify CVEs. A subset of the Lambda functions must receive automated code scans to detect potential data leaks and other vulnerabilities. The code scans must occur only for selected Lambda functions, not all the Lambda functions.
Which combination of actions will meet these requirements? (Choose three.)
A.
Activate Amazon Inspector. Start automated CVE scans.
B.
Activate Lambda standard scanning and Lambda code scanning in Amazon Inspector.
C.
Enable Amazon GuardDuty. Enable the Lambda Protection feature in GuardDuty.
D.
Enable scanning in the Monitor settings of the Lambda functions that need code scans.
E.
Tag Lambda functions that do not need code scans. In the tag, include a key of InspectorCodeExclusion and a value of LambdaCodeScanning.
F.
Use Amazon Inspector to scan the 3 bucket that contains the Lambda .zip packages and the Lambda layer .zip file for code scans.
AnswerDiscussion
Correct Answer: A, B, D
To meet the requirements, first, activate Amazon Inspector to start automated CVE scans. Second, activate Lambda standard scanning and Lambda code scanning within Amazon Inspector to specifically target Lambda functions for scanning. Third, enable scanning in the Monitor settings of the Lambda functions that need code scans, ensuring that only the selected functions receive automated code scans to detect potential data leaks and other vulnerabilities.
Question 521 of 529
A company is changing the way that it handles patching of Amazon EC2 instances in its application account. The company currently patches instances over the internet by using a NAT gateway in a VPC in the application account.
The company has EC2 instances set up as a patch source repository in a dedicated private VPC in a core account. The company wants to use AWS Systems Manager Patch Manager and the patch source repository in the core account to patch the EC2 instances in the application account. The company must prevent all EC2 instances in the application account from accessing the internet.
The EC2 instances in the application account need to access Amazon S3, where the application data is stored. These EC2 instances need connectivity to Systems Manager and to the patch source repository in the private VPC in the core account.
Which solution will meet these requirements?
A.
Create a network ACL that blocks outbound traffic on port 80. Associate the network ACL with all subnets in the application account. In the application account and the core account, deploy one EC2 instance that runs a custom VPN server. Create a VPN tunnel to access the private VPC. Update the route table in the application account.
B.
Create private VIFs for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route table in the core account.
C.
Create VPC endpoints for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a VPC peering connection to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts.
D.
Create a network ACL that blocks inbound traffic on port 80. Associate the network ACL with all subnets in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts.
AnswerDiscussion
Correct Answer: C
The correct solution involves creating VPC endpoints for both AWS Systems Manager and Amazon S3, which will allow the EC2 instances in the application account to access these services without requiring internet access. Deleting the NAT gateway ensures that the instances do not have internet access. Additionally, creating a VPC peering connection between the application account and the core account will enable the EC2 instances in the application account to access the patch source repository in the private VPC of the core account. Updating the route tables in both accounts will ensure that the traffic is properly routed. This configuration meets all the requirements of preventing internet access, maintaining connectivity to required AWS services, and accessing the patch repository securely.
Question 522 of 529
A company in the United States (US) has acquired a company in Europe. Both companies use the AWS Cloud. The US company has built a new application with a microservices architecture. The US company is hosting the application across five VPCs in the us-east-2 Region. The application must be able to access resources in one VPC in the eu-west-1 Region.
However, the application must not be able to access any other VPCs.
The VPCs in both Regions have no overlapping CIDR ranges. All accounts are already consolidated in one organization in AWS Organizations.
Which solution will meet these requirements MOST cost-effectively?
A.
Create one transit gateway in eu-west-1. Attach the VPCs in us-east-2 and the VPC in eu-west-1 to the transit gateway. Create the necessary route entries in each VPC so that the traffic is routed through the transit gateway.
B.
Create one transit gateway in each Region. Attach the involved subnets to the regional transit gateway. Create the necessary route entries in the associated route tables for each subnet so that the traffic is routed through the regional transit gateway. Peer the two transit gateways.
C.
Create a full mesh VPC peering connection configuration between all the VPCs. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection.
D.
Create one VPC peering connection for each VPC in us-east-2 to the VPC in eu-west-1. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection.
AnswerDiscussion
Correct Answer: D
The most cost-effective solution is to create a VPC peering connection for each VPC in us-east-2 to the VPC in eu-west-1. This approach allows the application to access resources in the specified VPC in eu-west-1 without needing a complex and costly setup like transit gateways, which are more expensive and may introduce unnecessary complexity for this use case. Given that the VPCs in both regions have no overlapping CIDR blocks and the requirement is to restrict access to only one VPC, VPC peering is both a simple and cost-effective method to meet these requirements.
Question 523 of 529
A travel company built a web application that uses Amazon Simple Email Service (Amazon SES) to send email notifications to users. The company needs to enable logging to help troubleshoot email delivery issues. The company also needs the ability to do searches that are based on recipient, subject, and time sent.
Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)
A.
Create an Amazon SES configuration set with Amazon Data Firehose as the destination. Choose to send logs to an Amazon S3 bucket.
B.
Enable AWS CloudTrail logging. Specify an Amazon S3 bucket as the destination for the logs.
C.
Use Amazon Athena to query the logs in the Amazon S3 bucket for recipient, subject, and time sent.
D.
Create an Amazon CloudWatch log group. Configure Amazon SES to send logs to the log group.
E.
Use Amazon Athena to query the logs in Amazon CloudWatch for recipient, subject, and time sent.
AnswerDiscussion
Correct Answer: A, C
To meet the requirements of logging email delivery issues and performing searches based on recipient, subject, and time sent, the solutions architect should use an Amazon SES configuration set with Amazon Kinesis Data Firehose as the destination to send logs to an Amazon S3 bucket. Additionally, Amazon Athena can be used to query these logs in the S3 bucket for the necessary details such as recipient, subject, and time sent. This combination ensures that the logs are stored in a searchable format and that an effective querying method is in place.
Question 524 of 529
A company migrated to AWS and uses AWS Business Support. The company wants to monitor the cost-effectiveness of Amazon EC2 instances across AWS accounts. The EC2 instances have tags for department, business unit, and environment. Development EC2 instances have high cost but low utilization.
The company needs to detect and stop any underutilized development EC2 instances. Instances are underutilized if they had 10% or less average daily CPU utilization and 5 MB or less network I/O for at least 4 of the past 14 days.
Which solution will meet these requirements with the LEAST operational overhead?
A.
Configure Amazon CloudWatch dashboards to monitor EC2 instance utilization based on tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances.
B.
Configure AWS Systems Manager to track EC2 instance utilization and report underutilized instances to Amazon CloudWatch. Filter the CloudWatch data by tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances.
C.
Create an Amazon EventBridge rule to detect low utilization of EC2 instances reported by AWS Trusted Advisor. Configure the rule to invoke an AWS Lambda function that filters the data by tags for department, business unit, and environment and stops underutilized development EC2 instances.
D.
Create an AWS Lambda function to run daily to retrieve utilization data for all EC2 instances. Save the data to an Amazon DynamoDB table. Create an Amazon QuickSight dashboard that uses the DynamoDB table as a data source to identify and stop underutilized development EC2 instances.
AnswerDiscussion
Correct Answer: C
To detect and stop underutilized development EC2 instances with the least operational overhead, leveraging AWS Trusted Advisor's low utilization reports is effective. Trusted Advisor already provides recommendations based on specific utilization metrics. By creating an Amazon EventBridge rule to trigger an AWS Lambda function for filtering instances by department, business unit, and environment tags and stopping the underutilized instances, this solution minimizes the need for continuous monitoring and management, thus reducing overall operational overhead.
Question 525 of 529
A company is hosting an application on AWS for a project that will run for the next 3 years. The application consists of 20 Amazon EC2 On-Demand Instances that are registered in a target group for a Network Load Balancer (NLB). The instances are spread across two Availability Zones. The application is stateless and runs 24 hours a day, 7 days a week.
The company receives reports from users who are experiencing slow responses from the application. Performance metrics show that the instances are at 10% CPU utilization during normal application use. However, the CPU utilization increases to 100% at busy times, which typically last for a few hours.
The company needs a new architecture to resolve the problem of slow responses from the application.
Which solution will meet these requirements MOST cost-effectively?
A.
Create an Auto Scaling group. Attach the Auto Scaling group to the target group of the NLB. Set the minimum capacity to 20 and the desired capacity to 28. Purchase Reserved Instances for 20 instances.
B.
Create a Spot Fleet that has a request type of request. Set the TotalTargetCapacity parameter to 20. Set the DefaultTargetCapacityType parameter to On-Demand. Specify the NLB when creating the Spot Fleet.
C.
Create a Spot Fleet that has a request type of maintain. Set the TotalTargetCapacity parameter to 20. Set the DefaultTargetCapacityType parameter to Spot. Replace the NLB with an Application Load Balancer.
D.
Create an Auto Scaling group. Attach the Auto Scaling group to the target group of the NLB. Set the minimum capacity to 4 and the maximum capacity to 28. Purchase Reserved Instances for four instances.
AnswerDiscussion
Correct Answer: D
To address the problem of slow responses during high utilization periods, the most cost-effective solution is to create an Auto Scaling group with a minimum capacity of 4 and a maximum capacity of 28. This allows the application to scale up during busy times to handle the increased load and scale down during normal periods, saving costs. By purchasing Reserved Instances for the minimum number of instances (4), the company can further reduce costs as Reserved Instances are cheaper than On-Demand Instances. This ensures that the application is scalable and cost-efficient.
Question 526 of 529
Accompany is building an application to collect and transmit sensor data from a factory. The application will use AWS IoT Core to send data from hundreds of devices to an Amazon S3 data lake. The company must enrich the data before loading the data into Amazon S3.
The application will transmit the sensor data every 5 seconds. New sensor data must be available in Amazon S3 less than 30 minutes after the application collects the data. No other applications are processing the sensor data from AWS IoT Core.
Which solution will meet these requirements MOST cost-effectively?
A.
Create a topic in AWS IoT Core to ingest the sensor data. Create an AWS Lambda function to enrich the data and to write the data to Amazon S3. Configure an AWS IoT rule action to invoke the Lambda function.
B.
Use AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose. Set the Kinesis Data Firehose buffering interval to 900 seconds. Use Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data, Configure Kinesis Data Firehose to deliver the data to Amazon S3.
C.
Create a topic in AWS IoT Core to ingest the sensor data. Configure an AWS IoT rule action to send the data to an Amazon Timestream table. Create an AWS Lambda, function to read the data from Timestream. Configure the Lambda function to enrich the data and to write the data to Amazon S3.
D.
Use AWS loT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Streams. Create a consumer AWS Lambda function to process the data from Kinesis Data Streams and to enrich the data. Call the S3 PutObject API operation from the Lambda function to write the data to Amazon S3.
AnswerDiscussion
Correct Answer: A
The most cost-effective solution involves the minimum necessary services that can achieve the requirements. By creating a topic in AWS IoT Core to ingest the sensor data, you leverage a robust platform designed to handle incoming IoT data. Using an AWS Lambda function to enrich and write the data to Amazon S3 is both straightforward and efficient. Configuring an AWS IoT rule action to invoke the Lambda function ensures that the data is processed and stored in S3 within the required timeframe, without the need for additional services like Kinesis, which would increase the complexity and cost of the solution.
Question 527 of 529
A company is collecting data from a large set of IoT devices. The data is stored in an Amazon S3 data lake. Data scientists perform analytics on Amazon EC2 instances that run in two public subnets in a VPC in a separate AWS account.
The data scientists need access to the data lake from the EC2 instances. The EC2 instances already have an assigned role with permissions to access Amazon S3.
According to company policies, only authorized networks are allowed to have access to the IoT data.
Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)
A.
Create a gateway VPC endpoint for Amazon S3 in the data scientists’ VPC.
B.
Create an S3 access point in the data scientists' AWS account for the data lake.
C.
Update the EC2 instance role. Add a policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN.
D.
Update the VPC route table to route S3 traffic to an S3 access point.
E.
Add an S3 bucket policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN.
AnswerDiscussion
Correct Answer: A, E
To ensure secure access to the Amazon S3 data lake from EC2 instances without traversing the public internet, a gateway VPC endpoint for Amazon S3 should be created. This enables direct, secure connectivity between the EC2 instances and S3, adhering to company policies that only authorized networks can access IoT data. Additionally, an S3 bucket policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN must be implemented. This enforces access control based on specific conditions, ensuring that only authorized networks have access to the data.
Question 528 of 529
A company wants to migrate its website to AWS. The website uses containers that are deployed in an on-premises, self-managed Kubernetes cluster. All data for the website is stored in an on-premises PostgreSQL database.
The company has decided to migrate the on-premises Kubernetes cluster to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster will use EKS managed node groups with a static number of nodes. The company will also migrate the on-premises database to an Amazon RDS for PostgreSQL database.
A solutions architect needs to estimate the total cost of ownership (TCO) for this workload before the migration.
Which solution will provide the required TCO information?
A.
Request access to Migration Evaluator. Run the Migration Evaluator Collector and import the data. Configure a scenario. Export a Quick Insights report from Migration Evaluator.
B.
Launch AWS Database Migration Service (AWS DMS) for the on-premises database. Generate an assessment report. Create an estimate in AWS Pricing Calculator for the costs of the EKS migration.
C.
Initialize AWS Application Migration Service. Add the on-premises servers as source servers. Launch a test instance. Output a TCO report from Application Migration Service.
D.
Access the AWS Cloud Economics Center webpage to assess the AWS Cloud Value Framework. Create an AWS Cost and Usage report from the Cloud Value Framework.
AnswerDiscussion
Correct Answer: A
To estimate the total cost of ownership (TCO) for migrating the workload to AWS, the most appropriate solution is to use Migration Evaluator. Migration Evaluator provides tools to analyze the current environment and generate reports that include the estimated costs for running the workload on AWS. By running the Migration Evaluator Collector, configuring a scenario, and exporting a Quick Insights report, you can obtain a comprehensive TCO analysis, encompassing both the Amazon EKS migration and the Amazon RDS for PostgreSQL database migration. Other options, such as using AWS DMS or AWS Application Migration Service, do not offer the same level of comprehensive TCO analysis.
Question 529 of 529
An events company runs a ticketing platform on AWS. The company’s customers configure and schedule their events on the platform. The events result in large increases of traffic to the platform. The company knows the date and time of each customer’s events.
The company runs the platform on an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster consists of Amazon EC2 On-Demand Instances that are in an Auto Scaling group. The Auto Scaling group uses a predictive scaling policy.
The ECS cluster makes frequent requests to an Amazon S3 bucket to download ticket assets. The ECS cluster and the S3 bucket are in the same AWS Region and the same AWS account. Traffic between the ECS cluster and the S3 bucket flows across a NAT gateway.
The company needs to optimize the cost of the platform without decreasing the platform's availability.
Which combination of steps will meet these requirements? (Choose two.)
A.
Create a gateway VPC endpoint for the S3 bucket.
B.
Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances. Configure the new capacity provider strategy to have the same weight as the existing capacity provider strategy.
C.
Create On-Demand Capacity Reservations for the applicable instance type for the time period of the scheduled scaling policies.
D.
Enable S3 Transfer Acceleration on the S3 bucket.
E.
Replace the predictive scaling policy with scheduled scaling policies for the scheduled events.
AnswerDiscussion
Correct Answer: A, E
Creating a gateway VPC endpoint for the S3 bucket will reduce data transfer costs by eliminating the need for traffic to go through the NAT gateway, which incurs charges. Replacing the predictive scaling policy with scheduled scaling policies will ensure that scaling operations are based on the known dates and times of customer events, optimizing resource utilization and reducing costs associated with over-provisioning. Together, these steps will optimize costs without compromising availability.